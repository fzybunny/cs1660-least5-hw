

the project gutenberg etext of loc workshop on electronic texts




                      workshop on electronic texts

                               proceedings



                          edited by james daly







                              june 


                           library of congress
                            washington dc



    supported by a grant from the david and lucile packard foundation


                                 


                            table of contents


acknowledgements

introduction

proceedings
   welcome
      prosser gifford and carl fleischhauer

   session i  content in a new form  who will use it and what will they do
      james daly moderator
      avra michelson overview
      susan h veccia user evaluation
      joanne freeman beyond the scholar
         discussion

   session ii  show and tell
      jacqueline hess moderator
      elli mylonas perseus project
         discussion
      eric m calaluca patrologia latina database
      carl fleischhauer and ricky erway american memory
         discussion
      dorothy twohig the papers of george washington
         discussion
      maria l lebron the online journal of current clinical trials
         discussion
      lynne k personius cornell mathematics books
         discussion

   session iii  distribution networks and networking  
                 options for dissemination
      robert g zich moderator
      clifford a lynch
         discussion
      howard besser
         discussion
      ronald l larsen
      edwin b brownrigg
         discussion

   session iv  image capture text capture overview of text and
                image storage formats
         william l hooton moderator
      a principal methods for image capture of text  
            direct scanning use of microform
         anne r kenney
         pamela qj andre
         judith a zidar
         donald j waters
            discussion
      b special problems  bound volumes conservation
                            reproducing printed halftones
         george thoma
         carl fleischhauer
            discussion
      c image standards and implications for preservation
         jean baronas
         patricia battin
            discussion
      d text conversion  ocr vs rekeying standards of accuracy
                           and use of imperfect texts service bureaus
         michael lesk
         ricky erway
         judith a zidar
            discussion

   session v  approaches to preparing electronic texts
      susan hockey moderator
      stuart weibel
         discussion
      cm sperbergmcqueen
         discussion
      eric m calaluca
         discussion

   session vi  copyright issues
      marybeth peters

   session vii  conclusion
      prosser gifford moderator
      general discussion

appendix i  program

appendix ii  abstracts

appendix iii  directory of participants


                                 


                            acknowledgements

i would like to thank carl fleischhauer and prosser gifford for the
opportunity to learn about areas of human activity unknown to me a scant
ten months ago and the david and lucile packard foundation for
supporting that opportunity  the help given by others is acknowledged on
a separate page

                                                           october 


                                 


                              introduction

the workshop on electronic texts  drew together representatives of
various projects and interest groups to compare ideas beliefs
experiences and in particular methods of placing and presenting
historical textual materials in computerized form  most attendees gained
much in insight and outlook from the event  but the assembly did not
form a new nation or to put it another way the diversity of projects
and interests was too great to draw the representatives into a cohesive
actionoriented body

everyone attending the workshop shared an interest in preserving and
providing access to historical texts  but within this broad field the
attendees represented a variety of formal informal figurative and
literal groups with many individuals belonging to more than one  these
groups may be defined roughly according to the following topics or
activities

 imaging
 searchable coded texts
 national and international computer networks
 cdrom production and dissemination
 methods and technology for converting older paper materials into
electronic form
 study of the use of digital materials by scholars and others

this summary is arranged thematically and does not follow the actual
sequence of presentations

notes
       in this document the phrase electronic text is used to mean
     any computerized reproduction or version of a document book
     article or manuscript including images and not merely a machine
     readable or machinesearchable text

       the workshop was held at the library of congress on  june
      with funding from the david and lucile packard foundation 
     the document that follows represents a summary of the presentations
     made at the workshop and was compiled by james daly  this
     introduction was written by daly and carl fleischhauer


preservation and imaging

preservation as that term is used by archivists was most explicitly
discussed in the context of imaging  anne kenney and lynne personius
explained how the concept of a faithful copy and the userfriendliness of
the traditional book have guided their project at cornell university 
although interested in computerized dissemination participants in the
cornell project are creating digital image sets of older books in the
public domain as a source for a fresh paper facsimile or in a future
phase microfilm  the books returned to the library shelves are
highquality and useful replacements on acidfree paper that should last
a long time  to date the cornell project has placed little or no
emphasis on creating searchable texts one would not be surprised to find
that the project participants view such texts as new editions and thus
not as faithful reproductions 

in her talk on preservation patricia battin struck an ecumenical and
flexible note as she endorsed the creation and dissemination of a variety
of types of digital copies  do not be too narrow in defining what counts
as a preservation element battin counseled for the present at least
digital copies made with preservation in mind cannot be as narrowly
standardized as say microfilm copies with the same objective  setting
standards precipitously can inhibit creativity but delay can result in
chaos she advised

in part battins position reflected the unsettled nature of imageformat
standards and attendees could hear echoes of this unsettledness in the
comments of various speakers  for example jean baronas reviewed the
status of several formal standards moving through committees of experts
and clifford lynch encouraged the use of a new guideline for transmitting
document images on internet  testimony from participants in the national
agricultural librarys nal text digitization program and lcs american
memory project highlighted some of the challenges to the actual creation
or interchange of images including difficulties in converting
preservation microfilm to digital form  donald waters reported on the
progress of a master plan for a project at yale university to convert
books on microfilm to digital image sets project open book pob

the workshop offered rather less of an imaging practicum than planned
but howto hints emerge at various points for example throughout
kenneys presentation and in the discussion of arcana such as
thresholding and dithering offered by george thoma and fleischhauer

notes
       although there is a sense in which any reproductions of
     historical materials preserve the human record specialists in the
     field have developed particular guidelines for the creation of
     acceptable preservation copies

       titles and affiliations of presenters are given at the
     beginning of their respective talks and in the directory of
     participants appendix iii


the machinereadable text  markup and use

the sections of the workshop that dealt with machinereadable text tended
to be more concerned with access and use than with preservation at least
in the narrow technical sense  michael sperbergmcqueen made a forceful
presentation on the text encoding initiatives tei implementation of
the standard generalized markup language sgml  his ideas were echoed
by susan hockey elli mylonas and stuart weibel  while the
presentations made by the tei advocates contained no practicum their
discussion focused on the value of the finished product what the
european community calls reusability but what may also be termed
durability  they argued that marking upthat is codinga text in a
wellconceived way will permit it to be moved from one computer
environment to another as well as to be used by various users  two
kinds of markup were distinguished   procedural markup which
describes the features of a text eg dots on a page and 
descriptive markup which describes the structure or elements of a
document eg chapters paragraphs and front matter

the tei proponents emphasized the importance of texts to scholarship 
they explained how heavily coded and thus analyzed and annotated texts
can underlie research play a role in scholarly communication and
facilitate classroom teaching  sperbergmcqueen reminded listeners that
a written or printed item eg a particular edition of a book is
merely a representation of the abstraction we call a text  to concern
ourselves with faithfully reproducing a printed instance of the text
sperbergmcqueen argued is to concern ourselves with the representation
of a representation images as simulacra for the text  the tei proponents
interest in images tends to focus on corollary materials for use in teaching
for example photographs of the acropolis to accompany a greek text

by the end of the workshop sperbergmcqueen confessed to having been
converted to a limited extent to the view that electronic images
constitute a promising alternative to microfilming indeed an
alternative probably superior to microfilming  but he was not convinced
that electronic images constitute a serious attempt to represent text in
electronic form  hockey and mylonas also conceded that their experience
at the pierce symposium the previous week at georgetown university and
the present conference at the library of congress had compelled them to
reevaluate their perspective on the usefulness of text as images 
attendees could see that the text and image advocates were in
constructive tension so to say

three nontei presentations described approaches to preparing
machinereadable text that are less rigorous and thus less expensive  in
the case of the papers of george washington dorothy twohig explained
that the digital version will provide a notquiteperfect rendering of
the transcribed textsome  documents available for research
during the decades while the perfect or print version is completed 
members of the american memory team and the staff of nals text
digitization program see below also outlined a middle ground concerning
searchable texts  in the case of american memory contractors produce
texts with about percent accuracy that serve as browse or
reference versions of written or printed originals  end users who need
faithful copies or perfect renditions must refer to accompanying sets of
digital facsimile images or consult copies of the originals in a nearby
library or archive  american memory staff argued that the high cost of
producing percent accurate copies would prevent lc from offering
access to large parts of its collections


the machinereadable text  methods of conversion

although the workshop did not include a systematic examination of the
methods for converting texts from paper or from facsimile images into
machinereadable form nevertheless various speakers touched upon this
matter  for example weibel reported that oclc has experimented with a
merging of multiple optical character recognition systems that will
reduce errors from an unacceptable rate of  characters out of every
l to an unacceptable rate of  characters out of every l

pamela andre presented an overview of nals text digitization program and
judith zidar discussed the technical details  zidar explained how nal
purchased hardware and software capable of performing optical character
recognition ocr and text conversion and used its own staff to convert
texts  the process zidar said required extensive editing and project
staff found themselves considering alternatives including rekeying
andor creating abstracts or summaries of texts  nal reckoned costs at
 per page  by way of contrast ricky erway explained that american
memory had decided from the start to contract out conversion to external
service bureaus  the criteria used to select these contractors were cost
and quality of results as opposed to methods of conversion  erway noted
that historical documents or books often do not lend themselves to ocr 
bound materials represent a special problem  in her experience quality
controlinspecting incoming materials counting errors in samplesposed
the most timeconsuming aspect of contracting out conversion  erway
reckoned american memorys costs at  per page but cautioned that fewer
costelements had been included than in nals figure


options for dissemination

the topic of dissemination proper emerged at various points during the
workshop  at the session devoted to national and international computer
networks lynch howard besser ronald larsen and edwin brownrigg
highlighted the virtues of internet today and of the network that will
evolve from internet  listeners could discern in these narratives a
vision of an information democracy in which millions of citizens freely
find and use what they need  lynch noted that a lack of standards
inhibits disseminating multimedia on the network a topic also discussed
by besser  larsen addressed the issues of network scalability and
modularity and commented upon the difficulty of anticipating the effects
of growth in orders of magnitude  brownrigg talked about the ability of
packet radio to provide certain links in a network without the need for
wiring  however the presenters also called attention to the
shortcomings and incongruities of presentday computer networks  for
example   network use is growing dramatically but much network
traffic consists of personal communication email   large bodies of
information are available but a users ability to search across their
entirety is limited   there are significant resources for science and
technology but few network sources provide content in the humanities 
 machinereadable texts are commonplace but the capability of the
system to deal with images let alone other media formats lags behind 
a glimpse of a multimedia future for networks however was provided by
maria lebron in her overview of the online journal of current clinical
trials ojcct and the process of scholarly publishing online   

the contrasting form of the cdrom disk was never systematically
analyzed but attendees could glean an impression from several of the
showandtell presentations  the perseus and american memory examples
demonstrated recently published disks while the descriptions of the
ibycus version of the papers of george washington and chadwyckhealeys
patrologia latina database pld told of disks to come  according to
eric calaluca plds principal focus has been on converting jacquespaul
mignes definitive collection of latin texts to machinereadable form 
although everyone could share the network advocates enthusiasm for an
online future the possibility of rolling up ones sleeves for a session
with a cdrom containing both textual materials and a powerful retrieval
engine made the disk seem an appealing vessel indeed  the overall
discussion suggested that the transition from cdrom to online networked
access may prove far slower and more difficult than has been anticipated


who are the users and what do they do

although concerned with the technicalities of production the workshop
never lost sight of the purposes and uses of electronic versions of
textual materials  as noted above those interested in imaging discussed
the problematical matter of digital preservation while the tei proponents
described how machinereadable texts can be used in research  this latter
topic received thorough treatment in the paper read by avra michelson
she placed the phenomenon of electronic texts within the context of
broader trends in information technology and scholarly communication

among other things michelson described online conferences that
represent a vigorous and important intellectual forum for certain
disciplines  internet now carries more than  conferences with about
 percent of these devoted to topics in the social sciences and the
humanities  other scholars use online networks for distance learning 
meanwhile there has been a tremendous growth in enduser computing
professors today are less likely than their predecessors to ask the
campus computer center to process their data  electronic texts are one
key to these sophisticated applications michelson reported and more and
more scholars in the humanities now work in an online environment 
toward the end of the workshop michael lesk presented a corollary to
michelsons talk reporting the results of an experiment that compared
the work of one group of chemistry students using traditional printed
texts and two groups using electronic sources  the experiment
demonstrated that in the event one does not know what to read one needs
the electronic systems the electronic systems hold no advantage at the
moment if one knows what to read but neither do they impose a penalty

daly provided an anecdotal account of the revolutionizing impact of the
new technology on his previous methods of research in the field of classics
his account by extrapolation served to illustrate in part the arguments
made by michelson concerning the positive effects of the sudden and radical
transformation being wrought in the ways scholars work

susan veccia and joanne freeman delineated the use of electronic
materials outside the university  the most interesting aspect of their
use freeman said could be seen as a paradox  teachers in elementary
and secondary schools requested access to primary source materials but
at the same time found that primariness itself made these materials
difficult for their students to use


other topics

marybeth peters reviewed copyright law in the united states and offered
advice during a lively discussion of this subject  but uncertainty
remains concerning the price of copyright in a digital medium because a
solution remains to be worked out concerning management and synthesis of
copyrighted and outofcopyright pieces of a database

as moderator of the final session of the workshop prosser gifford directed
discussion to future courses of action and the potential role of lc in
advancing them  among the recommendations that emerged were the following

      workshop participants should  begin to think about working
     with image material but structure and digitize it in such a
     way that at a later stage it can be interpreted into text and
      find a common way to build text and images together so that
     they can be used jointly at some stage in the future with
     appropriate network support because that is how users will want
     to access these materials  the library might encourage attempts
     to bring together people who are working on texts and images

      a network version of american memory should be developed or
     consideration should be given to making the data in it
     available to people interested in doing network multimedia 
     given the current dearth of digital data that is appealing and
     unencumbered by extremely complex rights problems developing a
     network version of american memory could do much to help make
     network multimedia a reality

      concerning the thorny issue of electronic deposit lc should
     initiate a catalytic process in terms of distributed
     responsibility that is bring together the distributed
     organizations and set up a study group to look at all the
     issues related to electronic deposit and see where we as a
     nation should move  for example lc might attempt to persuade
     one major library in each state to deal with its state
     equivalent publisher which might produce a cooperative project
     that would be equitably distributed around the country and one
     in which lc would be dealing with a minimal number of publishers
     and minimal copyright problems  lc must also deal with the
     concept of online publishing determining among other things
     how serials such as ojcct might be deposited for copyright

      since a number of projects are planning to carry out
     preservation by creating digital images that will end up in
     online or nearline storage at some institution lc might play
     a helpful role at least in the near term by accelerating how
     to catalog that information into the research library information
     network rlin and then into oclc so that it would be accessible
     this would reduce the possibility of multiple institutions digitizing
     the same work 


conclusion

the workshop was valuable because it brought together partisans from
various groups and provided an occasion to compare goals and methods 
the more committed partisans frequently communicate with others in their
groups but less often across group boundaries  the workshop was also
valuable to attendeesincluding those involved with american memorywho
came less committed to particular approaches or concepts  these
attendees learned a great deal and plan to select and employ elements of
imaging textcoding and networked distribution that suit their
respective projects and purposes

still reality rears its ugly head  no breakthrough has been achieved 
on the imaging side one confronts a proliferation of competing
datainterchange standards and a lack of consensus on the role of digital
facsimiles in preservation  in the realm of machinereadable texts one
encounters a reasonably mature standard but methodological difficulties
and high costs  these latter problems of course represent a special
impediment to the desire as it is sometimes expressed in the popular
press to put the contents of the library of congress on line  in
the words of one participant there was no solution to the economic
problemsthe projects that are out there are surviving but it is going
to be a lot of work to transform the information industry and so far the
investment to do that is not forthcoming lesk per litteras


                                 


                               proceedings


welcome


gifford  origin of workshop in current librarians desire to make lcs
collections more widely available  desiderata arising from the prospect
of greater interconnectedness 


after welcoming participants on behalf of the library of congress
american memory am and the national demonstration lab prosser
gifford director for scholarly programs library of congress located
the origin of the workshop on electronic texts in a conversation he had
had considerably more than a year ago with carl fleischhauer concerning
some of the issues faced by am  on the assumption that numerous other
people were asking the same questions the decision was made to bring
together as many of these people as possible to ask the same questions
together  in a deeper sense gifford said the origin of the workshop
lay in the desire of the current librarian of congress james h 
billington to make the collections of the library especially those
offering unique or unusual testimony on aspects of the american
experience available to a much wider circle of users than those few
people who can come to washington to use them  this meant that the
emphasis of am from the outset has been on archival collections of the
basic material and on making these collections themselves available
rather than selected or heavily edited products

from ams emphasis followed the questions with which the workshop began 
who will use these materials and in what form will they wish to use
them  but an even larger issue deserving mention in giffords view was
the phenomenal growth in internet connectivity  he expressed the hope
that the prospect of greater interconnectedness than ever before would
lead to   much more cooperative and mutually supportive endeavors 
development of systems of shared and distributed responsibilities to
avoid duplication and to ensure accuracy and preservation of unique
materials and  agreement on the necessary standards and development of
the appropriate directories and indices to make navigation
straightforward among the varied resources that are and increasingly
will be available  in this connection gifford requested that
participants reflect from the outset upon the sorts of outcomes they
thought the workshop might have  did those present constitute a group
with sufficient common interests to propose a next step or next steps
and if so what might those be  they would return to these questions the
following afternoon

                                 


fleischhauer  core of workshop concerns preparation and production of
materials  special challenge in conversion of textual materials 
quality versus quantity  do the several groups represented share common
interests 


carl fleischhauer coordinator american memory library of congress
emphasized that he would attempt to represent the people who perform some
of the work of converting or preparing  materials and that the core of
the workshop had to do with preparation and production  fleischhauer
then drew a distinction between the long term when many things would be
available and connected in the ways that gifford described and the short
term in which am not only has wrestled with the issue of what is the
best course to pursue but also has faced a variety of technical
challenges

fleischhauer remarked ams endeavors to deal with a wide range of library
formats such as motion picture collections soundrecording collections
and pictorial collections of various sorts especially collections of
photographs  in the course of these efforts am kept coming back to
textual materialsmanuscripts or rare printed matter bound materials
etc  text posed the greatest conversion challenge of all  thus the
genesis of the workshop which reflects the problems faced by am  these
problems include physical problems  for example those in the library
and archive business deal with collections made up of fragile and rare
manuscript items bound materials especially the notoriously brittle
bound materials of the late nineteenth century  these are precious
cultural artifacts however as well as interesting sources of
information and lc desires to retain and conserve them  am needs to
handle things without damaging them  guillotining a book to run it
through a sheet feeder must be avoided at all costs

beyond physical problems issues pertaining to quality arose  for
example the desire to provide users with a searchable text is affected
by the question of acceptable level of accuracy  one hundred percent
accuracy is tremendously expensive  on the other hand the output of
optical character recognition ocr can be tremendously inaccurate 
although am has attempted to find a middle ground uncertainty persists
as to whether or not it has discovered the right solution

questions of quality arose concerning images as well  fleischhauer
contrasted the extremely high level of quality of the digital images in
the cornell xerox project with ams efforts to provide a browsequality
or accessquality image as opposed to an archival or preservation image 
fleischhauer therefore welcomed the opportunity to compare notes

fleischhauer observed in passing that conversations he had had about
networks have begun to signal that for various forms of media a
determination may be made that there is a browsequality item or a
distributionandaccessquality item that may coexist in some systems
with a higher quality archival item that would be inconvenient to send
through the network because of its size  fleischhauer referred of
course to images more than to searchable text

as am considered those questions several conceptual issues arose  ought
am occasionally to reproduce materials entirely through an image set at
other times entirely through a text set and in some cases a mix 
there probably would be times when the historical authenticity of an
artifact would require that its image be used  an image might be
desirable as a recourse for users if one could not provide percent
accurate text  again am wondered as a practical matter if a
distinction could be drawn between rare printed matter that might exist
in multiple collectionsthat is in ten or fifteen libraries  in such
cases the need for perfect reproduction would be less than for unique
items  implicit in his remarks fleischhauer conceded was the admission
that am has been tilting strongly towards quantity and drawing back a
little from perfect quality  that is it seemed to am that society would
be better served if more things were distributed by lceven if they were
not quite perfectthan if fewer things perfectly represented were
distributed  this was stated as a proposition to be tested with
responses to be gathered from users

in thinking about issues related to reproduction of materials and seeing
other people engaged in parallel activities am deemed it useful to
convene a conference  hence the workshop  fleischhauer thereupon
surveyed the several groups represented   the world of images image
users and image makers  the world of text and scholarship and within
this group those concerned with languagefleischhauer confessed to finding
delightful irony in the fact that some of the most advanced thinkers on
computerized texts are those dealing with ancient greek and roman materials
 the network world and  the general world of library science which
includes people interested in preservation and cataloging

fleischhauer concluded his remarks with special thanks to the david and
lucile packard foundation for its support of the meeting the american
memory group the office for scholarly programs the national
demonstration lab and the office of special events  he expressed the
hope that david woodley packard might be able to attend noting that
packards work and the work of the foundation had sponsored a number of
projects in the text area

                                 

session i  content in a new form   who will use it and what will they do


daly  acknowledgements  a new latin authors disk   effects of the new
technology on previous methods of research        


serving as moderator james daly acknowledged the generosity of all the
presenters for giving of their time counsel and patience in planning
the workshop as well as of members of the american memory project and
other library of congress staff and the david and lucile packard
foundation and its executive director colburn s wilbur

daly then recounted his visit in march to the center for electronic texts
in the humanities ceth and the department of classics at rutgers
university where an old friend lowell edmunds introduced him to the
departments ibycus scholarly personal computer and in particular the
new latin cdrom containing among other things almost all classical
latin literary texts through ad   packard humanities institute
phi los altos california released this disk late in  with a
nominal triennial licensing fee

playing with the disk for an hour or so at rutgers brought home to daly
at once the revolutionizing impact of the new technology on his previous
methods of research  had this disk been available two or three years
earlier daly contended when he was engaged in preparing a commentary on
book  of virgils aeneid for cambridge university press he would not
have required a fortyeightsquarefoot table on which to spread the
numerous most frequently consulted items including some ten or twelve
concordances to key latin authors an almost equal number of lexica to
authors who lacked concordances and where either lexica or concordances
were lacking numerous editions of authors antedating and postdating virgil

nor when checking each of the average six to seven words contained in
the virgilian hexameter for its usage elsewhere in virgils works or
other latin authors would daly have had to maintain the laborious
mechanical process of flipping through these concordances lexica and
editions each time  nor would he have had to frequent as often the
milton s eisenhower library at the johns hopkins university to consult
the thesaurus linguae latinae  instead of devoting countless hours or
the bulk of his research time to gathering data concerning virgils use
of words dalynow freed by phis latin authors disk from the
tyrannical yet in some ways paradoxically happy scholarly drudgery
would have been able to devote that same bulk of time to analyzing and
interpreting virgilian verbal usage

citing theodore brunner gregory crane elli mylonas and avra michelson
daly argued that this reversal in his style of work made possible by the
new technology would perhaps have resulted in better more productive
research  indeed even in the course of his browsing the latin authors
disk at rutgers its powerful search retrieval and highlighting
capabilities suggested to him several new avenues of research into
virgils use of sound effects  this anecdotal account daly maintained
may serve to illustrate in part the sudden and radical transformation
being wrought in the ways scholars work

                                 


michelson  elements related to scholarship and technology  electronic
texts within the context of broader trends within information technology
and scholarly communication  evaluation of the prospects for the use of
electronic texts  relationship of electronic texts to processes of
scholarly communication in humanities research  new exchange formats
created by scholars  projects initiated to increase scholarly access to
converted text  trend toward making electronic resources available
through research and education networks  changes taking place in
scholarly communication among humanities scholars  networkmediated
scholarship transforming traditional scholarly practices  key
information technology trends affecting the conduct of scholarly
communication over the next decade  the trend toward enduser computing
 the trend toward greater connectivity  effects of these trends  key
transformations taking place  summary of principal arguments 


avra michelson archival research and evaluation staff national archives
and records administration nara argued that establishing who will use
electronic texts and what they will use them for involves a consideration
of both information technology and scholarship trends  this
consideration includes several elements related to scholarship and
technology   the key trends in information technology that are most
relevant to scholarship  the key trends in the use of currently
available technology by scholars in the nonscientific community and 
the relationship between these two very distinct but interrelated trends 
the investment in understanding this relationship being made by
information providers technologists and public policy developers as
well as by scholars themselves seems to be pervasive and growing
michelson contended  she drew on collaborative work with jeff rothenberg
on the scholarly use of technology

michelson sought to place the phenomenon of electronic texts within the
context of broader trends within information technology and scholarly
communication  she argued that electronic texts are of most use to
researchers to the extent that the researchers working context ie
their relevant bibliographic sources collegial feedback analytic tools
notes drafts etc along with their fields primary and secondary
sources also is accessible in electronic form and can be integrated in
ways that are unique to the online environment

evaluation of the prospects for the use of electronic texts includes two
elements   an examination of the ways in which researchers currently
are using electronic texts along with other electronic resources and 
an analysis of key information technology trends that are affecting the
longterm conduct of scholarly communication  michelson limited her
discussion of the use of electronic texts to the practices of humanists
and noted that the scientific community was outside the panels overview

michelson examined the nature of the current relationship of electronic
texts in particular and electronic resources in general to what she
maintained were essentially five processes of scholarly communication
in humanities research  researchers  identify sources  communicate
with their colleagues  interpret and analyze data  disseminate
their research findings and  prepare curricula to instruct the next
generation of scholars and students  this examination would produce a
clearer understanding of the synergy among these five processes that
fuels the tendency of the use of electronic resources for one process to
stimulate its use for other processes of scholarly communication

for the first process of scholarly communication the identification of
sources michelson remarked the opportunity scholars now enjoy to
supplement traditional wordofmouth searches for sources among their
colleagues with new forms of electronic searching  so for example
instead of having to visit the library researchers are able to explore
descriptions of holdings in their offices  furthermore if their own
institutions holdings prove insufficient scholars can access more than
 major american library catalogues over internet including the
universities of california michigan pennsylvania and wisconsin 
direct access to the bibliographic databases offers intellectual
empowerment to scholars by presenting a comprehensive means of browsing
through libraries from their homes and offices at their convenience

the second process of communication involves communication among
scholars  beyond the most common methods of communication scholars are
using email and a variety of new electronic communications formats
derived from it for further academic interchange  email exchanges are
growing at an astonishing rate reportedly  percent a month  they
currently constitute approximately half the traffic on research and
education networks  moreover the global spread of email has been so
rapid that it is now possible for american scholars to use it to
communicate with colleagues in close to  other countries

other new exchange formats created by scholars and operating on internet
include more than  conferences with about  percent of these devoted
to topics in the social sciences and humanities  the rate of growth of
these scholarly electronic conferences also is astonishing  from l to
l  new conferences were identified on internet  from october 
to june  an additional  conferences in the social sciences and
humanities were added to this directory of listings  scholars have
established conferences in virtually every field within every different
discipline  for example there are currently close to  active social
science and humanities  conferences on topics such as art and
architecture ethnomusicology folklore japanese culture medical
education and gifted and talented education  the appeal to scholars of
communicating through these conferences is that unlike any other medium
electronic conferences today provide a forum for global communication
with peers at the front end of the research process

interpretation and analysis of sources constitutes the third process of
scholarly communication that michelson discussed in terms of texts and
textual resources  the methods used to analyze sources fall somewhere on
a continuum from quantitative analysis to qualitative analysis 
typically evidence is culled and evaluated using methods drawn from both
ends of this continuum  at one end quantitative analysis involves the
use of mathematical processes such as a count of frequencies and
distributions of occurrences or on a higher level regression analysis 
at the other end of the continuum qualitative analysis typically
involves nonmathematical processes oriented toward language
interpretation or the building of theory  aspects of this work involve
the processingeither manual or computationalof large and sometimes
massive amounts of textual sources although the use of nontextual
sources as evidence such as photographs sound recordings film footage
and artifacts is significant as well

scholars have discovered that many of the methods of interpretation and
analysis that are related to both quantitative and qualitative methods
are processes that can be performed by computers  for example computers
can count  they can count brush strokes used in a rembrandt painting or
perform regression analysis for understanding cause and effect  by means
of advanced technologies computers can recognize patterns analyze text
and model concepts  furthermore computers can complete these processes
faster with more sources and with greater precision than scholars who
must rely on manual interpretation of data  but if scholars are to use
computers for these processes source materials must be in a form
amenable to computerassisted analysis  for this reason many scholars
once they have identified the sources that are key to their research are
converting them to machinereadable form  thus a representative example
of the numerous textual conversion projects organized by scholars around
the world in recent years to support computational text analysis is the
tlg the thesaurus linguae graecae  this project is devoted to
converting the extant ancient texts of classical greece  editors note 
according to the tlg newsletter of may l tlg was in use in thirtytwo
different countries  this figure updates michelsons previous count by one

the scholars performing these conversions have been asked to recognize
that the electronic sources they are converting for one use possess value
for other research purposes as well  as a result during the past few
years humanities scholars have initiated a number of projects to
increase scholarly access to converted text  so for example the text
encoding initiative tei about which more is said later in the program
was established as an effort by scholars to determine standard elements
and methods for encoding machinereadable text for electronic exchange 
in a second effort to facilitate the sharing of converted text scholars
have created a new institution the center for electronic texts in the
humanities ceth  the center estimates that there are  series of
source texts in the humanities that have been converted to
machinereadable form worldwide  ceth is undertaking an international
search for converted text in the humanities compiling it into an
electronic library and preparing bibliographic descriptions of the
sources for the research libraries information networks rlin
machinereadable data file  the library profession has begun to initiate
large conversion projects as well such as american memory

while scholars have been making converted text available to one another
typically on disk or on cdrom the clear trend is toward making these
resources available through research and education networks  thus the
american and french research on the treasury of the french language
artfl and the dante project are already available on internet 
michelson summarized this section on interpretation and analysis by
noting that   increasing numbers of humanities scholars in the library
community are recognizing the importance to the advancement of
scholarship of retrospective conversion of source materials in the arts
and humanities and  there is a growing realization that making the
sources available on research and education networks maximizes their
usefulness for the analysis performed by humanities scholars

the fourth process of scholarly communication is dissemination of
research findings that is publication  scholars are using existing
research and education networks to engineer a new type of publication 
scholarlycontrolled journals that are electronically produced and
disseminated  although such journals are still emerging as a
communication format their number has grown from approximately twelve
to thirtysix during the past year july  to june   most of
these electronic scholarly journals are devoted to topics in the
humanities  as with network conferences scholarly enthusiasm for these
electronic journals stems from the mediums unique ability to advance
scholarship in a way that no other medium can do by supporting global
feedback and interchange practically in real time early in the research
process  beyond scholarly journals michelson remarked the delivery of
commercial fulltext products such as articles in professional journals
newsletters magazines wire services and reference sources  these are
being delivered via online local library catalogues especially through
cdroms  furthermore according to michelson there is general optimism
that the copyright and fees issues impeding the delivery of full text on
existing research and education networks soon will be resolved

the final process of scholarly communication is curriculum development
and instruction and this involves the use of computer information
technologies in two areas  the first is the development of
computeroriented instructional tools which includes simulations
multimedia applications and computer tools that are used to assist in
the analysis of sources in the classroom etc  the perseus project a
database that provides a multimedia curriculum on classical greek
civilization is a good example of the way in which entire curricula are
being recast using information technologies  it is anticipated that the
current difficulty in exchanging electronically computerbased
instructional software which in turn makes it difficult for one scholar
to build upon the work of others will be resolved before too long 
standalone curricular applications that involve electronic text will be
sharable through networks reinforcing their significance as intellectual
products as well as instructional tools

the second aspect of electronic learning involves the use of research and
education networks for distance education programs  such programs
interactively link teachers with students in geographically scattered
locations and rely on the availability of electronic instructional
resources  distance education programs are gaining wide appeal among
state departments of education because of their demonstrated capacity to
bring advanced specialized course work and an array of experts to many
classrooms  a recent report found that at least  states operated at
least one statewide network for education in  with networks under
development in many of the remaining states

michelson summarized this section by noting two striking changes taking
place in scholarly communication among humanities scholars  first is the
extent to which electronic text in particular and electronic resources
in general are being infused into each of the five processes described
above  as mentioned earlier there is a certain synergy at work here 
the use of electronic resources for one process tends to stimulate its
use for other processes because the chief course of movement is toward a
comprehensive online working context for humanities scholars that
includes online availability of key bibliographies scholarly feedback
sources analytical tools and publications  michelson noted further
that the movement toward a comprehensive online working context for
humanities scholars is not new  in fact it has been underway for more
than forty years in the humanities since father roberto busa began
developing an electronic concordance of the works of saint thomas aquinas
in   what we are witnessing today michelson contended is not the
beginning of this online transition but for at least some humanities
scholars the turning point in the transition from a print to an
electronic working context  coinciding with the online transition the
second striking change is the extent to which research and education
networks are becoming the new medium of scholarly communication  the
existing internet and the pending national education and research network
nren represent the new meeting ground where scholars are going for
bibliographic information scholarly dialogue and feedback the most
current publications in their field and highlevel educational
offerings  traditional scholarly practices are undergoing tremendous
transformations as a result of the emergence and growing prominence of
what is called networkmediated scholarship

michelson next turned to the second element of the framework she proposed
at the outset of her talk for evaluating the prospects for electronic
text namely the key information technology trends affecting the conduct
of scholarly communication over the next decade   enduser computing
and  connectivity

enduser computing means that the person touching the keyboard or
performing computations is the same as the person who initiates or
consumes the computation  the emergence of personal computers along
with a host of other forces such as ubiquitous computing advances in
interface design and the online transition is prompting the consumers
of computation to do their own computing and is thus rendering obsolete
the traditional distinction between end users and ultimate users

the trend toward enduser computing is significant to consideration of
the prospects for electronic texts because it means that researchers are
becoming more adept at doing their own computations and thus more
competent in the use of electronic media  by avoiding programmer
intermediaries computation is becoming central to the researchers
thought process  this direct involvement in computing is changing the
researchers perspective on the nature of research itself that is the
kinds of questions that can be posed the analytical methodologies that
can be used the types and amount of sources that are appropriate for
analyses and the form in which findings are presented  the trend toward
enduser computing means that increasingly electronic media and
computation are being infused into all processes of humanities
scholarship inspiring remarkable transformations in scholarly
communication

the trend toward greater connectivity suggests that researchers are using
computation increasingly in network environments  connectivity is
important to scholarship because it erases the distance that separates
students from teachers and scholars from their colleagues while allowing
users to access remote databases share information in many different
media connect to their working context wherever they are and
collaborate in all phases of research

the combination of the trend toward enduser computing and the trend
toward connectivity suggests that the scholarly use of electronic
resources already evident among some researchers will soon become an
established feature of scholarship  the effects of these trends along
with ongoing changes in scholarly practices point to a future in which
humanities researchers will use computation and electronic communication
to help them formulate ideas access sources perform research
collaborate with colleagues seek peer review publish and disseminate
results and engage in many other professional and educational activities

in summary michelson emphasized four points   a portion of humanities
scholars already consider electronic texts the preferred format for
analysis and dissemination   scholars are using these electronic
texts in conjunction with other electronic resources in all the
processes of scholarly communication   the humanities scholars
working context is in the process of changing from print technology to
electronic technology in many ways mirroring transformations that have
occurred or are occurring within the scientific community   these
changes are occurring in conjunction with the development of a new
communication medium  research and education networks that are
characterized by their capacity to advance scholarship in a wholly unique
way

michelson also reiterated her three principal arguments  l electronic
texts are best understood in terms of the relationship to other
electronic resources and the growing prominence of networkmediated
scholarship   the prospects for electronic texts lie in their capacity
to be integrated into the online network of electronic resources that
comprise the new working context for scholars   retrospective conversion
of portions of the scholarly record should be a key strategy as information
providers respond to changes in scholarly communication practices

                                 


veccia  ams evaluation project and public users of electronic resources
 am and its design  site selection and evaluating the macintosh
implementation of am  characteristics of the six public libraries
selected  characteristics of ams users in these libraries  principal
ways am is being used 


susan veccia team leader and joanne freeman associate coordinator
american memory library of congress gave a joint presentation  first
by way of introduction veccia explained her and freemans roles in
american memory am  serving principally as an observer veccia has
assisted with the evaluation project of am placing am collections in a
variety of different sites around the country and helping to organize and
implement that project  freeman has been an associate coordinator of am
and has been involved principally with the interpretative materials
preparing some of the electronic exhibits and printed historical
information that accompanies am and that is requested by users  veccia
and freeman shared anecdotal observations concerning am with public users
of electronic resources  notwithstanding a fairly structured evaluation
in progress both veccia and freeman chose not to report on specifics in
terms of numbers etc because they felt it was too early in the
evaluation project to do so

am is an electronic archive of primary source materials from the library
of congress selected collections representing a variety of formats
photographs graphic arts recorded sound motion pictures broadsides
and soon pamphlets and books  in terms of the design of this system
the interpretative exhibits have been kept separate from the primary
resources with good reason  accompanying this collection are printed
documentation and user guides as well as guides that freeman prepared for
teachers so that they may begin using the content of the system at once

veccia described the evaluation project before talking about the public
users of am limiting her remarks to public libraries because freeman
would talk more specifically about schools from kindergarten to twelfth
grade k   having started in spring  the evaluation currently
involves testing of the macintosh implementation of am  since the
primary goal of this evaluation is to determine the most appropriate
audience or audiences for am very different sites were selected  this
makes evaluation difficult because of the varying degrees of technology
literacy among the sites  am is situated in fortyfour locations of
which six are public libraries and sixteen are schools  represented
among the schools are elementary junior high and high schools
district offices also are involved in the evaluation which will
conclude in summer 

veccia focused the remainder of her talk on the six public libraries one
of which doubles as a state library  they represent a range of
geographic areas and a range of demographic characteristics  for
example three are located in urban settings two in rural settings and
one in a suburban setting  a range of technical expertise is to be found
among these facilities as well  for example one is an apple library of
the future while two others are rural oneroom librariesin one am
sits at the front desk next to a tractor manual

all public libraries have been extremely enthusiastic supportive and
appreciative of the work that am has been doing  veccia characterized
various users  most users in public libraries describe themselves as
general readers of the students who use am in the public libraries
those in fourth grade and above seem most interested  public libraries
in rural sites tend to attract retired people who have been highly
receptive to am  users tend to fall into two additional categories 
people interested in the content and historical connotations of these
primary resources and those fascinated by the technology  the format
receiving the most comments has been motion pictures  the adult users in
public libraries are more comfortable with ibm computers whereas young
people seem comfortable with either ibm or macintosh although most of
them seem to come from a macintosh background  this same tendency is
found in the schools

what kinds of things do users do with am  in a public library there are
two main goals or ways that am is being used  as an individual learning
tool and as a leisure activity  adult learning was one area that veccia
would highlight as a possible application for a tool such as am  she
described a patron of a rural public library who comes in every day on
his lunch hour and literally reads am methodically going through the
collection image by image  at the end of his hour he makes an electronic
bookmark puts it in his pocket and returns to work  the next day he
comes in and resumes where he left off  interestingly this man had
never been in the library before he used am  in another small rural
library the coordinator reports that am is a popular activity for some
of the older retired people in the community who ordinarily would not
use those thingscomputers  another example of adult learning in
public libraries is book groups one of which in particular is using am
as part of its reading on industrialization integration and urbanization
in the early s

one library reports that a family is using am to help educate their
children  in another instance individuals from a local museum came in
to use am to prepare an exhibit on toys of the past  these two examples
emphasize the mission of the public library as a cultural institution
reaching out to people who do not have the same resources available to
those who live in a metropolitan area or have access to a major library 
one rural library reports that junior high school students in large
numbers came in one afternoon to use am for entertainment  a number of
public libraries reported great interest among postcard collectors in the
detroit collection which was essentially a collection of images used on
postcards around the turn of the century  train buffs are similarly
interested because that was a time of great interest in railroading 
people it was found relate to things that they know of firsthand  for
example in both rural public libraries where am was made available
observers reported that the older people with personal remembrances of
the turn of the century were gravitating to the detroit collection 
these examples served to underscore michelsons observation re the
integration of electronic tools and ideasthat people learn best when
the material relates to something they know

veccia made the final point that in many cases am serves as a
publicrelations tool for the public libraries that are testing it  in
one case am is being used as a vehicle to secure additional funding for
the library  in another case am has served as an inspiration to the
staff of a major local public library in the south to think about ways to
make its own collection of photographs more accessible to the public

                                  


freeman  am and archival electronic resources in a school environment 
questions concerning context  questions concerning the electronic format
itself  computer anxiety  access and availability of the system 
hardware  strengths gained through the use of archival resources in
schools 


reiterating an observation made by veccia that am is an archival
resource made up of primary materials with very little interpretation
freeman stated that the project has attempted to bridge the gap between
these bare primary materials and a school environment and in that cause
has created guided introductions to am collections  loud demand from the
educational community  chiefly from teachers working with the upper
grades of elementary school through high school greeted the announcement
that am would be tested around the country

freeman reported not only on what was learned about am in a school
environment but also on several universal questions that were raised
concerning archival electronic resources in schools  she discussed
several strengths of this type of material in a school environment as
opposed to a highly structured resource that offers a limited number of
paths to follow

freeman first raised several questions about using am in a school
environment  there is often some difficulty in developing a sense of
what the system contains  many students sit down at a computer resource
and assume that because am comes from the library of congress all of
american history is now at their fingertips  as a result of that sort of
mistaken judgment some students are known to conclude that am contains
nothing of use to them when they look for one or two things and do not
find them  it is difficult to discover that middle ground where one has
a sense of what the system contains  some students grope toward the idea
of an archive a new idea to them since they have not previously
experienced what it means to have access to a vast body of somewhat
random information

other questions raised by freeman concerned the electronic format itself 
for instance in a school environment it is often difficult both for
teachers and students to gain a sense of what it is they are viewing 
they understand that it is a visual image but they do not necessarily
know that it is a postcard from the turn of the century a panoramic
photograph or even machinereadable text of an eighteenthcentury
broadside a twentiethcentury printed book or a nineteenthcentury
diary  that distinction is often difficult for people in a school
environment to grasp  because of that it occasionally becomes difficult
to draw conclusions from what one is viewing

freeman also noted the obvious fear of the computer which constitutes a
difficulty in using an electronic resource  though students in general
did not suffer from this anxiety several older students feared that they
were computerilliterate an assumption that became selffulfilling when
they searched for something but failed to find it  freeman said she
believed that some teachers also fear computer resources because they
believe they lack complete control  freeman related the example of
teachers shooing away students because it was not their time to use the
system  this was a case in which the situation had to be extremely
structured so that the teachers would not feel that they had lost their
grasp on what the system contained

a final question raised by freeman concerned access and availability of
the system  she noted the occasional existence of a gap in communication
between school librarians and teachers  often am sits in a school
library and the librarian is the person responsible for monitoring the
system  teachers do not always take into their world new library
resources about which the librarian is excited  indeed at the sites
where am had been used most effectively within a library the librarian
was required to go to specific teachers and instruct them in its use  as
a result several am sites will have inservice sessions over a summer
in the hope that perhaps with a more individualized link teachers will
be more likely to use the resource

a related issue in the school context concerned the number of
workstations available at any one location  centralization of equipment
at the district level with teachers invited to download things and walk
away with them proved unsuccessful because the hours these offices were
open were also school hours

another issue was hardware  as veccia observed a range of sites exists
some technologically advanced and others essentially acquiring their
first computer for the primary purpose of using it in conjunction with
ams testing  users at technologically sophisticated sites want even
more sophisticated hardware so that they can perform even more
sophisticated tasks with the materials in am  but once they acquire a
newer piece of hardware they must learn how to use that also at an
unsophisticated site it takes an extremely long time simply to become
accustomed to the computer not to mention the program offered with the
computer  all of these small issues raise one large question namely
are systems like am truly rewarding in a school environment or do they
simply act as innovative toys that do little more than spark interest

freeman contended that the evaluation project has revealed several strengths
that were gained through the use of archival resources in schools including

      psychic rewards from using am as a vast rich database with
     teachers assigning various projects to studentsoral presentations
     written reports a documentary a turnofthecentury newspaper
     projects that start with the materials in am but are completed using
     other resources am thus is used as a research tool in conjunction
     with other electronic resources as well as with books and items in
     the library where the system is set up

      students are acquiring computer literacy in a humanities context

      this sort of system is overcoming the isolation between disciplines
     that often exists in schools  for example many english teachers are
     requiring their students to write papers on historical topics
     represented in am  numerous teachers have reported that their
     students are learning critical thinking skills using the system

      on a broader level am is introducing primary materials not only
     to students but also to teachers in an environment where often
     simply none existan exciting thing for the students because it
     helps them learn to conduct research to interpret and to draw
     their own conclusions  in learning to conduct research and what it
     means students are motivated to seek knowledge  that relates to
     another positive outcomea high level of personal involvement of
     students with the materials in this system and greater motivation to
     conduct their own research and draw their own conclusions

      perhaps the most ironic strength of these kinds of archival
     electronic resources is that many of the teachers am interviewed
     were desperate it is no exaggeration to say not only for primary
     materials but for unstructured primary materials  these would they
     thought foster personally motivated research exploration and
     excitement in their students  indeed these materials have done
     just that  ironically however this lack of structure produces
     some of the confusion to which the newness of these kinds of
     resources may also contribute  the key to effective use of archival
     products in a school environment is a clear effective introduction
     to the system and to what it contains 

                                 


discussion  nothing known quantitatively about the number of
humanities scholars who must see the original versus those who would
settle for an edited transcript or about the ways in which humanities
scholars are using information technology  firm conclusions concerning
the manner and extent of the use of supporting materials in print
provided by am to await completion of evaluative study  a listeners
reflections on additional applications of electronic texts  role of
electronic resources in teaching elementary research skills to students 


during the discussion that followed the presentations by michelson
veccia and freeman additional points emerged

lesk asked if michelson could give any quantitative estimate of the
number of humanities scholars who must see or want to see the original
or the best possible version of the material versus those who typically
would settle for an edited transcript  while unable to provide a figure
she offered her impressions as an archivist who has done some reference
work and has discussed this issue with other archivists who perform
reference that those who use archives and those who use primary sources
for what would be considered very highlevel scholarly research as
opposed to say undergraduate papers were few in number especially
given the public interest in using primary sources to conduct
genealogical or avocational research and the kind of professional
research done by people in private industry or the federal government 
more important in michelsons view was that quantitatively nothing is
known about the ways in which for example humanities scholars are using
information technology  no studies exist to offer guidance in creating
strategies  the most recent study was conducted in  by the american
council of learned societies acls and what it showed was that 
percent of humanities scholars at that time were using computers  that
constitutes the extent of our knowledge

concerning ams strategy for orienting people toward the scope of
electronic resources freeman could offer no hard conclusions at this
point because she and her colleagues were still waiting to see
particularly in the schools what has been made of their efforts  within
the system however am has provided what are called electronic exhibits
such as introductions to time periods and materialsand these are
intended to offer a student user a sense of what a broadside is  and what
it might tell her or him  but freeman conceded that the project staff
would have to talk with students next year after teachers have had a
summer to use the materials and attempt to discover what the students
were learning from the materials  in addition freeman described
supporting materials in print provided by am at the request of local
teachers during a meeting held at lc  these included time lines
bibliographies and other materials that could be reproduced on a
photocopier in a classroom  teachers could walk away with and use these
and in this way gain a better understanding of the contents  but again
reaching firm conclusions concerning the manner and extent of their use
would have to wait until next year

as to the changes she saw occurring at the national archives and records
administration nara as a result of the increasing emphasis on
technology in scholarly research michelson stated that nara at this
point was absorbing the report by her and jeff rothenberg addressing
strategies for the archival profession in general although not for the
national archives specifically  nara is just beginning to establish its
role and what it can do  in terms of changes and initiatives that nara
can take no clear response could be given at this time

greenfield remarked two trends mentioned in the session  reflecting on
dalys opening comments on how he could have used a latin collection of
text in an electronic form he said that at first he thought most scholars
would be unwilling to do that  but as he thought of that in terms of the
original meaning of researchthat is having already mastered these texts
researching them for critical and comparative purposesfor the first time
the electronic format made a lot of sense  greenfield could envision
growing numbers of scholars learning the new technologies for that very
aspect of their scholarship and for conveniences sake

listening to veccia and freeman greenfield thought of an additional
application of electronic texts  he realized that am could be used as a
guide to lead someone to original sources  students cannot be expected
to have mastered these sources things they have never known about
before  thus am is leading them in theory to a vast body of
information and giving them a superficial overview of it enabling them
to select parts of it  greenfield asked if any evidence exists that this
resource will indeed teach the new user the k students how to do
research  scholars already know how to do research and are applying
these new tools  but he wondered why students would go beyond picking
out things that were most exciting to them

freeman conceded the correctness of greenfields observation as applied
to a school environment  the risk is that a student would sit down at a
system play with it find some things of interest and then walk away 
but in the relatively controlled situation of a school library much will
depend on the instructions a teacher or a librarian gives a student  she
viewed the situation not as one of finetuning research skills but of
involving students at a personal level in understanding and researching
things  given the guidance one can receive at school it then becomes
possible to teach elementary research skills to students which in fact
one particular librarian said she was teaching her fifth graders 
freeman concluded that introducing the idea of following ones own path
of inquiry which is essentially what research entails involves more
than teaching specific skills  to these comments veccia added the
observation that the individual teacher and the use of a creative
resource rather than am itself seemed to make the key difference
some schools and some teachers are making excellent use of the nature
of critical thinking and teaching skills she said

concurring with these remarks daly closed the session with the thought that
the more that producers produced for teachers and for scholars to use with
their students the more successful their electronic products would prove

                                 

session ii  show and tell

jacqueline hess director national demonstration laboratory served as
moderator of the showandtell session  she noted that a
questionandanswer period would follow each presentation


mylonas  overview and content of perseus  perseus primary materials
exist in a systemindependent archival form  a concession  textual
aspects of perseus  tools to use with the greek text  prepared indices
and fulltext searches in perseus  englishgreek word search leads to
close study of words and concepts  navigating perseus by tracing down
indices  using the iconography to perform research 


elli mylonas managing editor perseus project harvard university first
gave an overview of perseus a large collaborative effort based at
harvard university but with contributors and collaborators located at
numerous universities and colleges in the united states eg bowdoin
maryland pomona chicago virginia  funded primarily by the
annenbergcpb project with additional funding from apple harvard and
the packard humanities institute among others perseus is a multimedia
hypertextual database for teaching and research on classical greek
civilization which was released in february  in version  and
distributed by yale university press

consisting entirely of primary materials perseus includes ancient greek
texts and translations of those texts catalog entriesthat is museum
catalog entries not library catalog entrieson vases sites coins
sculpture and archaeological objects maps and a dictionary among
other sources  the number of objects and the objects for which catalog
entries exist are accompanied by thousands of color images which
constitute a major feature of the database  perseus contains
approximately  megabytes of text an amount that will double in
subsequent versions  in addition to these primary materials the perseus
project has been building tools for using them making access and
navigation easier the goal being to build part of the electronic
environment discussed earlier in the morning in which students or
scholars can work with their sources

the demonstration of perseus will show only a fraction of the real work
that has gone into it because the project had to face the dilemma of
what to enter when putting something into machinereadable form  should
one aim for very high quality or make concessions in order to get the
material in  since perseus decided to opt for very high quality all of
its primary materials exist in a systemindependentinsofar as it is
possible to be systemindependentarchival form  deciding what that
archival form would be and attaining it required much work and thought 
for example all the texts are marked up in sgml which will be made
compatible with the guidelines of the text encoding initiative tei when
they are issued

drawings are postscript files not meeting international standards but
at least designed to go across platforms  images or rather the real
archival forms consist of the best available slides which are being
digitized  much of the catalog material exists in database forma form
that the average user could use manipulate and display on a personal
computer but only at great cost  thus this is where the concession
comes in  all of this rich wellmarkedup information is stripped of
much of its content the images are converted into bitmaps and the text
into small formatted chunks  all this information can then be imported
into hypercard and run on a midrange macintosh which is what perseus
users have  this fact has made it possible for perseus to attain wide
use fairly rapidly  without those archival forms the hypercard version
being demonstrated could not be made easily and the project could not
have the potential to move to other forms and machines and software as
they appear none of which information is in perseus on the cd

of the numerous multimedia aspects of perseus mylonas focused on the
textual  part of what makes perseus such a pleasure to use mylonas
said is this effort at seamless integration and the ability to move
around both visual and textual material  perseus also made the decision
not to attempt to interpret its material any more than one interprets by
selecting  but mylonas emphasized perseus is not courseware  no
syllabus exists  there is no effort to define how one teaches a topic
using perseus although the project may eventually collect papers by
people who have used it to teach  rather perseus aims to provide
primary material in a kind of electronic library an electronic sandbox
so to say in which students and scholars who are working on this
material can explore by themselves  with that mylonas demonstrated
perseus beginning with the perseus gateway the first thing one sees
upon opening perseusan effort in part to solve the contextualizing
problemwhich tells the user what the system contains

mylonas demonstrated only a very small portion beginning with primary
texts and running off the cdrom  having selected aeschylus prometheus
bound which was viewable in greek and english pretty much in the same
segments together mylonas demonstrated tools to use with the greek text
something not possible with a book  looking up the dictionary entry form
of an unfamiliar word in greek after subjecting it to perseus
morphological analysis for all the texts  after finding out about a
word a user may then decide to see if it is used anywhere else in greek 
because vast amounts of indexing support all of the primary material one
can find out where else all forms of a particular greek word appear
often not a trivial matter because greek is highly inflected  further
since the story of prometheus has to do with the origins of sacrifice a
user may wish to study and explore sacrifice in greek literature by
typing sacrifice into a small window a user goes to the englishgreek
word listsomething one cannot do without the computer perseus has
indexed the definitions of its dictionarythe string sacrifice appears
in the definitions of these sixtyfive words  one may then find out
where any of those words is used in the works of a particular author 
the english definitions are not lemmatized

all of the indices driving this kind of usage were originally devised for
speed mylonas observed in other words all that kind of information
all forms of all words where they exist the dictionary form they belong
towere collected into databases which will expedite searching  then
it was discovered that one can do things searching in these databases
that could not be done searching in the full texts  thus although there
are fulltext searches in perseus much of the work is done behind the
scenes using prepared indices  re the indexing that is done behind the
scenes mylonas pointed out that without the sgml forms of the text it
could not be done effectively  much of this indexing is based on the
structures that are made explicit by the sgml tagging

it was found that one of the things many of perseus nongreekreading
users do is start from the dictionary and then move into the close study
of words and concepts via this kind of englishgreek word search by which
means they might select a concept  this exercise has been assigned to
students in core courses at harvardto study a concept by looking for the
english word in the dictionary finding the greek words and then finding
the words in the greek but of course reading across in the english
that tells them a great deal about what a translation means as well

should one also wish to see images that have to do with sacrifice that
person would go to the object key word search which allows one to
perform a similar kind of index retrieval on the database of
archaeological objects  without words pictures are useless perseus has
not reached the point where it can do much with images that are not
cataloged  thus although it is possible in perseus with text and images
to navigate by knowing where one wants to end upfor example a
redfigure vase from the boston museum of fine artsone can perform this
kind of navigation very easily by tracing down indices  mylonas
illustrated several generic scenes of sacrifice on vases  the features
demonstrated derived from perseus  version  will implement even
better means of retrieval

mylonas closed by looking at one of the pictures and noting again that
one can do a great deal of research using the iconography as well as the
texts  for instance students in a core course at harvard this year were
highly interested in greek concepts of foreigners and representations of
nongreeks  so they performed a great deal of research both with texts
eg herodotus and with iconography on vases and coins on how the
greeks portrayed nongreeks  at the same time art historians who study
iconography were also interested and were able to use this material

                                 


discussion  indexing and searchability of all english words in perseus 
several features of perseus   several levels of customization
possible  perseus used for general education  perseus effects on
education  contextual information in perseus  main challenge and
emphasis of perseus 


several points emerged in the discussion that followed mylonass presentation

although mylonas had not demonstrated perseus ability to crosssearch
documents she confirmed that all english words in perseus are indexed
and can be searched  so for example sacrifice could have been searched
in all texts the historical essay and all the catalogue entries with
their descriptionsin short in all of perseus

boolean logic is not in perseus  but will be added to the next
version although an effort is being made not to restrict perseus to a
database in which one just performs searching boolean or otherwise  it
is possible to move laterally through the documents by selecting a word
one is interested in and selecting an area of information one is
interested in and trying to look that word up in that area

since perseus was developed in hypercard several levels of customization
are possible  simple authoring tools exist that allow one to create
annotated paths through the information which are useful for notetaking
and for guided tours for teaching purposes and for expository writing 
with a little more ingenuity it is possible to begin to add or substitute
material in perseus

perseus has not been used so much for classics education as for general
education where it seemed to have an impact on the students in the core
course at harvard a general required course that students must take in
certain areas  students were able to use primary material much more

the perseus project has an evaluation team at the university of maryland
that has been documenting perseus effects on education  perseus is very
popular and anecdotal evidence indicates that it is having an effect at
places other than harvard for example test sites at ball state
university drury college and numerous small places where opportunities
to use vast amounts of primary data may not exist  one documented effect
is that archaeological anthropological and philological research is
being done by the same person instead of by three different people

the contextual information in perseus includes an overview essay a
fairly linear historical essay on the fifth century bc that provides
links into the primary material eg herodotus thucydides and
plutarch via small gray underscoring on the screen of linked
passages  these are handmade links into other material

to different extents most of the production work was done at harvard
where the people and the equipment are located  much of the
collaborative activity involved data collection and structuring because
the main challenge and the emphasis of perseus is the gathering of
primary material that is building a useful environment for studying
classical greece collecting data and making it useful 
systemsbuilding is definitely not the main concern  thus much of the
work has involved writing essays collecting information rewriting it
and tagging it  that can be done off site  the creative link for the
overview essay as well as for both systems and data was collaborative
and was forged via email and paper mail with professors at pomona and
bowdoin

                                 


calaluca  plds principal focus and contribution to scholarship 
various questions preparatory to beginning the project  basis for
project  basic rule in converting pld  concerning the images in pld 
running pld under a variety of retrieval softwares  encoding the
database a hardfought issue  various features demonstrated  importance
of user documentation  limitations of the cdrom version    


eric calaluca vice president chadwyckhealey inc demonstrated a
software interpretation of the patrologia latina database pld  plds
principal focus from the beginning of the project about threeandahalf
years ago was on converting mignes latin series and in the end
calaluca suggested conversion of the text will be the major contribution
to scholarship  calaluca stressed that as possibly the only private
publishing organization at the workshop chadwyckhealey had sought no
federal funds or national foundation support before embarking upon the
project but instead had relied upon a great deal of homework and
marketing to accomplish the task of conversion

ever since the possibilities of computersearching have emerged scholars
in the field of late ancient and early medieval studies philosophers
theologians classicists and those studying the history of natural law
and the history of the legal development of western civilization have
been longing for a fully searchable version of western literature for
example all the texts of augustine and bernard of clairvaux and
boethius not to mention all the secondary and tertiary authors

various questions arose calaluca said  should one convert migne 
should the database be encoded  is it necessary to do that  how should
it be delivered  what about cdrom  since this is a transitional
medium why even bother to create software to run on a cdrom  since
everybody knows people will be networking information why go to the
troublewhich is far greater with cdrom than with the production of
magnetic data  finally how does one make the data available  can many
of the hurdles to using electronic information that some publishers have
imposed upon databases be eliminated

the pld project was based on the principle that computersearching of
texts is most effective when it is done with a large database  because
pld represented a collection that serves so many disciplines across so
many periods it was irresistible

the basic rule in converting pld was to do no harm to avoid the sins of
intrusion in such a database  no introduction of newer editions no
onthespot changes no eradicating of all possible falsehoods from an
edition  thus pld is not the final act in electronic publishing for
this discipline but simply the beginning  the conversion of pld has
evoked numerous unanticipated questions  how will information be used 
what about networking  can the rights of a database be protected 
should one protect the rights of a database  how can it be made
available

those converting pld also tried to avoid the sins of omission that is
excluding portions of the collections or whole sections  what about the
images  pld is full of images some are extremely pious
nineteenthcentury representations of the fathers while others contain
highly interesting elements  the goal was to cover all the text of migne
including notes in greek and in hebrew the latter of which in
particular causes problems in creating a search structure all the
indices and even the images which are being scanned in separately
searchable files

several north american institutions that have placed acquisition requests
for the pld database have requested it in magnetic form without software
which means they are already running it without software without
anything demonstrated at the workshop

what cannot practically be done is go back and reconvert and reencode
data a timeconsuming and extremely costly enterprise  calaluca sees
pld as a database that can and should be run under a variety of
retrieval softwares  this will permit the widest possible searches 
consequently the need to produce a cdrom of pld as well as to develop
software that could handle some  gigabyte of heavily encoded text
developed out of conversations with collection development and reference
librarians who wanted software both compassionate enough for the
pedestrian but also capable of incorporating the most detailed
lexicographical studies that a user desires to conduct  in the end the
encoding and conversion of the data will prove the most enduring
testament to the value of the project

the encoding of the database was also a hardfought issue  did the
database need to be encoded were there normative structures for encoding
humanist texts  should it be sgml  what about the teiwill it last
will it prove useful  calaluca expressed some minor doubts as to whether
a data bank can be fully teiconformant  every effort can be made but
in the end to be teiconformant means to accept the need to make some
firm encoding decisions that can indeed be disputed  the tei points
the publisher in a proper direction but does not presume to make all the
decisions for him or her  essentially the goal of encoding was to
eliminate as much as possible the hindrances to informationnetworking
so that if an institution acquires a database everybody associated with
the institution can have access to it

calaluca demonstrated a portion of volume  because it had the most
anomalies in it  the software was created by electronic book
technologies of providence ri and is called dynatext  the software
works only with sgmlcoded data

viewing a table of contents on the screen the audience saw how dynatext
treats each element as a book and attempts to simplify movement through a
volume  familiarity with the patrologia in print ie the text its
source and the editions will make the machinereadable versions highly
useful  software with a windows application was sought for pld
calaluca said because this was the main trend for scholarly use

calaluca also demonstrated how a user can perform a variety of searches
and quickly move to any part of a volume the lookup screen provides
some basic simple wordsearching 

calaluca argued that one of the major difficulties is not the software 
rather in creating a product that will be used by scholars representing
a broad spectrum of computer sophistication  user documentation proves
to be the most important service one can provide

calaluca next illustrated a truncated search under mysterium within ten
words of virtus and how one would be able to find its contents throughout
the entire database  he said that the exciting thing about pld is that
many of the applications in the retrieval software being written for it
will exceed the capabilities of the software employed now for the cdrom
version  the cdrom faces genuine limitations in terms of speed and
comprehensiveness in the creation of a retrieval software to run it 
calaluca said he hoped that individual scholars will download the data
if they wish to their personal computers and have ready access to
important texts on a constant basis which they will be able to use in
their research and from which they might even be able to publish

calaluca explained that the blue numbers represented mignes column numbers
which are the standard scholarly references  pulling up a note he stated
that these texts were heavily edited and the image files would appear simply
as a note as well so that one could quickly access an image

                                 


fleischhauererway  several problems with which am is still wrestling 
various search and retrieval capabilities  illustration of automatic
stemming and a truncated search  ams attempt to find ways to connect
cataloging to the texts  ams gravitation towards sgml  striking a
balance between quantity and quality  how am furnishes users recourse to
images  conducting a search in a fulltext environment  macintosh and
ibm prototypes of am  multimedia aspects of am 


a demonstration of american memory by its coordinator carl fleischhauer
and ricky erway associate coordinator library of congress concluded
the morning session  beginning with a collection of broadsides from the
continental congress and the constitutional convention the only text
collection in a presentable form at the time of the workshop fleischhauer
highlighted several of the problems with which am is still wrestling
in its final form the disk will contain two collections not only the
broadsides but also the full text with illustrations of a set of
approximately  africanamerican pamphlets from the period  to 

as freeman had explained earlier am has attempted to use a small amount
of interpretation to introduce collections  in the present case the
contractor a company named quick source in silver spring md used
software called toolbook and put together a modestly interactive
introduction to the collection  like the two preceding speakers
fleischhauer argued that the real asset was the underlying collection

fleischhauer proceeded to describe various search and retrieval
capabilities while erway worked the computer  in this particular package
the go to pulldown allowed the user in effect to jump out of toolbook
where the interactive program was located and enter the thirdparty
software used by am for this text collection which is called personal
librarian  this was the windows version of personal librarian a
software application put together by a company in rockville md

since the broadsides came from the revolutionary war period a search was
conducted using the words british or war with the default operator reset
as or  fleischhauer demonstrated both automatic stemming which finds
other forms of the same root and a truncated search  one of personal
librarians strongest features the relevance ranking was represented by
a chart that indicated how often words being sought appeared in
documents with the one receiving the most hits obtaining the highest
score  the hit list that is supplied takes the relevance ranking into
account making the first hit in effect the one the software has
selected as the most relevant example

while in the text of one of the broadside documents fleischhauer
remarked ams attempt to find ways to connect cataloging to the texts
which it does in different ways in different manifestations  in the case
shown the cataloging was pasted on  am took marc records that were
written as online records right into one of the librarys mainframe
retrieval programs pulled them out and handed them off to the contractor
who massaged them somewhat to display them in the manner shown  one of
ams questions is does the cataloguing normally performed in the mainframe
work in this context or had am ought to think through adjustments

fleischhauer made the additional point that as far as the text goes am
has gravitated towards sgml he pointed to the boldface in the upper part
of the screen  although extremely limited in its ability to translate
or interpret sgml personal librarian will furnish both bold and italics
on screen a fairly easy thing to do but it is one of the ways in which
sgml is useful

striking a balance between quantity and quality has been a major concern
of am with accuracy being one of the places where project staff have
felt that less than percent accuracy was not unacceptable 
fleischhauer cited the example of the standard of the rekeying industry
namely  percent as one service bureau informed him to go from
 to  percent would double the cost

fleischhauer next demonstrated how am furnishes users recourse to images
and at the same time recalled lesks pointed question concerning the
number of people who would look at those images and the number who would
work only with the text  if the implication of lesks question was
sound fleischhauer said it raised the stakes for text accuracy and
reduced the value of the strategy for images

contending that preservation is always a bugaboo fleischhauer
demonstrated several images derived from a scan of a preservation
microfilm that am had made  he awarded a grade of c at best perhaps a
c minus or a c plus for how well it worked out  indeed the matter of
learning if other people had better ideas about scanning in general and
in particular scanning from microfilm was one of the factors that drove
am to attempt to think through the agenda for the workshop  skew for
example was one of the issues that am in its ignorance had not reckoned
would prove so difficult

further the handling of images of the sort shown in a desktop computer
environment involved a considerable amount of zooming and scrolling 
ultimately am staff feel that perhaps the paper copy that is printed out
might be the most useful one but they remain uncertain as to how much
onscreen reading users will do

returning to the text fleischhauer asked viewers to imagine a person who
might be conducting a search in a fulltext environment  with this
scenario he proceeded to illustrate other features of personal librarian
that he considered helpful for example it provides the ability to
notice words as one reads  clicking the include button on the bottom
of the search window pops the words that have been highlighted into the
search  thus a user can refine the search as he or she reads
reexecuting the search and continuing to find things in the quest for
materials  this software not only contains relevance ranking boolean
operators and truncation it also permits one to perform word algebra
so to say where one puts two or three words in parentheses and links
them with one boolean operator and then a couple of words in another set
of parentheses and asks for things within so many words of others

until they became acquainted recently with some of the work being done in
classics the am staff had not realized that a large number of the
projects that involve electronic texts were being done by people with a
profound interest in language and linguistics  their search strategies
and thinking are oriented to those fields as is shown in particular by
the perseus example  as amateur historians the am staff were thinking
more of searching for concepts and ideas than for particular words 
obviously fleischhauer conceded searching for concepts and ideas and
searching for words may be two rather closely related things

while displaying several images fleischhauer observed that the macintosh
prototype built by am contains a greater diversity of formats  echoing a
previous speaker he said that it was easier to stitch things together in
the macintosh though it tended to be a little more anemic in search and
retrieval  am therefore increasingly has been investigating
sophisticated retrieval engines in the ibm format

fleischhauer demonstrated several additional examples of the prototype
interfaces  one was ams metaphor for the network future in which a
kind of readingroom graphic suggests how one would be able to go around
to different materials  am contains a large number of photographs in
analog video form worked up from a videodisc which enable users to make
copies to print or incorporate in digital documents  a framegrabber is
built into the system making it possible to bring an image into a window
and digitize or print it out

fleischhauer next demonstrated sound recording which included texts 
recycled from a previous project the collection included sixty rpm
phonograph records of political speeches that were made during and
immediately after world war i  these constituted approximately three
hours of audio as am has digitized it which occupy  megabytes on a
cd  thus they are considerably compressed  from the catalogue card
fleischhauer proceeded to a transcript of a speech with the audio
available and with highlighted text following it as it played
a photograph has been added and a transcription made

considerable value has been added beyond what the library of congress
normally would do in cataloguing a sound recording which raises several
questions for am concerning where to draw lines about how much value it can
afford to add and at what point perhaps this becomes more than am could
reasonably do or reasonably wish to do  fleischhauer also demonstrated
a motion picture  as freeman had reported earlier the motion picture
materials have proved the most popular not surprisingly  this says more
about the medium he thought than about ams presentation of it

because ams goal was to bring together things that could be used by
historians or by people who were curious about history
turnofthecentury footage seemed to represent the most appropriate
collections from the library of congress in motion pictures these were
the very first films made by thomas edisons company and some others at
that time  the particular example illustrated was a biograph film
brought in with a framegrabber into a window  a single videodisc
contains about fifty titles and pieces of film from that period all of
new york city  taken together am believes they provide an interesting
documentary resource

                                 


discussion  using the framegrabber in am  volume of material processed
and to be processed  purpose of am within lc  cataloguing and the
nature of ams material  sgml coding and the question of quality versus
quantity 


during the questionandanswer period that followed fleischhauers
presentation several clarifications were made

am is bringing in motion pictures from a videodisc  the framegrabber
devices create a window on a computer screen which permits users to
digitize a single frame of the movie or one of the photographs  it
produces a crude roughandready image that high school students can
incorporate into papers and that has worked very nicely in this way

commenting on fleischhauers assertion that am was looking more at
searching ideas than words mylonas argued that without words an idea
does not exist  fleischhauer conceded that he ought to have articulated
his point more clearly  mylonas stated that they were in fact both
talking about the same thing  by searching for words and by forcing
people to focus on the word the perseus project felt that they would get
them to the idea  the way one reviews results is tailored more to one
kind of user than another

concerning the total volume of material that has been processed in this
way am at this point has in retrievable form seven or eight collections
all of them photographic  in the macintosh environment for example
there probably are  photographs  the sound recordings
number sixty items  the broadsides number about  items  there are
 political cartoons in the form of drawings  the motion pictures as
individual items number sixty to seventy

am also has a manuscript collection the life history portion of one of
the federal project series which will contain  individual
documents all firstperson narratives  am has in process about 
africanamerican pamphlets or about  printed pages for the period
  also in the works are some  panoramic photographs  am
has recycled a fair amount of the work done by lcs prints and
photographs division during the librarys optical disk pilot project in
the s  for example a special division of lc has tooled up and
thought through all the ramifications of electronic presentation of
photographs  indeed they are wheeling them out in great barrel loads 
the purpose of am within the library it is hoped is to catalyze several
of the other special collection divisions which have no particular
experience with in some cases mixed feelings about an activity such as
am  moreover in many cases the divisions may be characterized as not
only lacking experience in electronifying things but also in automated
cataloguing  marc cataloguing as practiced in the united states is
heavily weighted toward the description of monograph and serial
materials but is much thinner when one enters the world of manuscripts
and things that are held in the librarys music collection and other
units  in response to a comment by lesk that ams material is very
heavily photographic and is so primarily because individual records have
been made for each photograph fleischhauer observed that an itemlevel
catalog record exists for example for each photograph in the detroit
publishing collection of  pictures  in the case of the federal
writers project for which nearly  documents exist representing
information from twentysix different states am with the assistance of
karen stuart of the manuscript division will attempt to find some way not
only to have a collectionlevel record but perhaps a marc record for each
state which will then serve as an umbrella for the  documents
that come under it  but that drama remains to be enacted  the am staff
is conservative and clings to cataloguing though of course visitors tout
artificial intelligence and neural networks in a manner that suggests that
perhaps one need not have cataloguing or that much of it could be put aside

the matter of sgml coding fleischhauer conceded returned the discussion
to the earlier treated question of quality versus quantity in the library
of congress  of course text conversion can be done with percent
accuracy but it means that when ones holdings are as vast as lcs only
a tiny amount will be exposed whereas permitting lower levels of
accuracy can lead to exposing or sharing larger amounts but with the
quality correspondingly impaired

                                 


twohig  a contrary experience concerning electronic options  volume of
material in the washington papers and a suggestion of david packard 
implications of packards suggestion  transcribing the documents for the
cdrom  accuracy of transcriptions  the cdrom edition of the founding
fathers documents 


finding encouragement in a comment of michelsons from the morning
sessionthat numerous people in the humanities were choosing electronic
options to do their workdorothy twohig editor the papers of george
washington opened her illustrated talk by noting that her experience
with literary scholars and numerous people in editing was contrary to
michelsons  twohig emphasized literary scholars complete ignorance of
the technological options available to them or their reluctance or in
some cases their downright hostility toward these options

after providing an overview of the five founding fathers projects
jefferson at princeton franklin at yale john adams at the
massachusetts historical society and madison down the hall from her at
the university of virginia twohig observed that the washington papers
like all of the projects include both sides of the washington
correspondence and deal with some  documents to be published with
extensive annotation in eighty to eightyfive volumes a project that
will not be completed until well into the next century  thus it was
with considerable enthusiasm several years ago that the washington papers
project wpp greeted david packards suggestion that the papers of the
founding fathers could be published easily and inexpensively and to the
great benefit of american scholarship via cdrom

in pragmatic terms funding from the packard foundation would expedite
the transcription of thousands of documents waiting to be put on disk in
the wpp offices  further since the costs of collecting editing and
converting the founding fathers documents into letterpress editions were
running into the millions of dollars and the considerable staffs
involved in all of these projects were devoting their careers to
producing the work the packard foundations suggestion had a
revolutionary aspect  transcriptions of the entire corpus of the
founding fathers papers would be available on cdrom to public and
college libraries even high schools at a fraction of the cost
 for the annual license feeto produce a limited university
press run of  of each volume of the published papers at  per
printed volume  given the current budget crunch in educational systems
and the corresponding constraints on librarians in smaller institutions
who wish to add these volumes to their collections producing the
documents on cdrom would likely open a greatly expanded audience for the
papers  twohig stressed however that development of the founding
fathers cdrom is still in its infancy  serious software problems remain
to be resolved before the material can be put into readable form  

funding from the packard foundation resulted in a major push to
transcribe the  or so documents of the washington papers remaining
to be transcribed onto computer disks  slides illustrated several of the
problems encountered for example the present inability of cdrom to
indicate the crossouts deleted material in eighteenth century
documents  twohig next described documents from various periods in the
eighteenth century that have been transcribed in chronological order and
delivered to the packard offices in california where they are converted
to the cdrom a process that is expected to consume five years to
complete that is reckoning from david packards suggestion made several
years ago until about july   twohig found an encouraging
indication of the projects benefits in the ongoing use made by scholars
of the search functions of the cdrom particularly in reducing the time
spent in manually turning the pages of the washington papers

twohig next furnished details concerning the accuracy of transcriptions 
for instance the insertion of thousands of documents on the cdrom
currently does not permit each document to be verified against the
original manuscript several times as in the case of documents that appear
in the published edition  however the transcriptions receive a cursory
check for obvious typos the misspellings of proper names and other
errors from the wpp cdrom editor  eventually all documents that appear
in the electronic version will be checked by project editors  although
this process has met with opposition from some of the editors on the
grounds that imperfect work may leave their offices the advantages in
making this material available as a research tool outweigh  fears about the
misspelling of proper names and other relatively minor editorial matters

completion of all five founding fathers projects ie retrievability
and searchability of all of the documents by proper names alternate
spellings or varieties of subjects will provide one of the richest
sources of this size for the history of the united states in the latter
part of the eighteenth century  further publication on cdrom will
allow editors to include even minutiae such as laundry lists not
included in the printed volumes

it seems possible that the extensive annotation provided in the printed
volumes eventually will be added to the cdrom edition pending
negotiations with the publishers of the papers  at the moment the
founding fathers cdrom is accessible only on the ibycus a computer
developed out of the thesaurus linguae graecae project and designed for
the use of classical scholars  there are perhaps  ibycus computers in
the country most of which are in university classics departments 
ultimately it is anticipated that the cdrom edition of the founding
fathers documents will run on any ibmcompatible or macintosh computer
with a cdrom drive  numerous changes in the software will also occur
before the project is completed  editors note an ibycus was
unavailable to demonstrate the cdrom

                                 


discussion  several additional features of wpp clarified 


discussion following twohigs presentation served to clarify several
additional features including  that the projects primary
intellectual product consists in the electronic transcription of the
material  that the text transmitted to the cdrom people is not
marked up  that cataloging and subjectindexing of the material
remain to be worked out though at this point material can be retrieved
by name and  that because all the searching is done in the hardware
the ibycus is designed to read a cdrom which contains only sequential
text files  technically it then becomes very easy to read the material
off and put it on another device

                                 


lebron  overview of the history of the joint project between aaas and
oclc  several practices the online environment shares with traditional
publishing on hard copy  several technical and behavioral barriers to
electronic publishing  how aaas and oclc arrived at the subject of
clinical trials  advantages of the electronic format and other features
of ojcct  an illustrated tour of the journal 


maria lebron managing editor the online journal of current clinical
trials ojcct presented an illustrated overview of the history of the
joint project between the american association for the advancement of
science aaas and the online computer library center inc oclc  the
joint venture between aaas and oclc owes its beginning to a
reorganization launched by the new chief executive officer at oclc about
three years ago and combines the strengths of these two disparate
organizations  in short ojcct represents the process of scholarly
publishing on line

lebron next discussed several practices the online environment shares
with traditional publishing on hard copyfor example peer review of
manuscriptsthat are highly important in the academic world  lebron
noted in particular the implications of citation counts for tenure
committees and grants committees  in the traditional hardcopy
environment citation counts are readily demonstrable whereas the
online environment represents an ethereal medium to most academics

lebron remarked several technical and behavioral barriers to electronic
publishing for instance the problems in transmission created by special
characters or by complex graphics and halftones  in addition she noted
economic limitations such as the storage costs of maintaining back issues
and market or audience education

manuscripts cannot be uploaded to ojcct lebron explained because it is
not a bulletin board or email forms of electronic transmission of
information that have created an ambience clouding peoples understanding
of what the journal is attempting to do  ojcct which publishes
peerreviewed medical articles dealing with the subject of clinical
trials includes text tabular material and graphics although at this
time it can transmit only line illustrations

next lebron described how aaas and oclc arrived at the subject of
clinical trials  it is  a highly statistical discipline that  does
not require halftones but can satisfy the needs of its audience with line
illustrations and graphic material and  there is a need for the speedy
dissemination of highquality research results  clinical trials are
research activities that involve the administration of a test treatment
to some experimental unit in order to test its usefulness before it is
made available to the general population  lebron proceeded to give
additional information on ojcct concerning its editorinchief editorial
board editorial content and the types of articles it publishes
including peerreviewed research reports and reviews as well as
features shared by other traditional hardcopy journals

among the advantages of the electronic format are faster dissemination of
information including raw data and the absence of space constraints
because pages do not exist  this latter fact creates an interesting
situation when it comes to citations  nor are there any issues  aaass
capacity to download materials directly from the journal to a
subscribers printer hard drive or floppy disk helps ensure highly
accurate transcription  other features of ojcct include onscreen alerts
that allow linkage of subsequently published documents to the original
documents online searching by subject author title etc indexing of
every single word that appears in an article viewing access to an
article by component abstract full text or graphs numbered
paragraphs to replace page counts publication in science every thirty
days of indexing of all articles published in the journal
typesetquality screens and hypertext links that enable subscribers to
bring up medline abstracts directly without leaving the journal

after detailing the two primary ways to gain access to the journal
through the oclc network and compuserv if one desires graphics or through
the internet if just an ascii file is desired lebron illustrated the
speedy editorial process and the coding of the document using sgml tags
after it has been accepted for publication  she also gave an illustrated
tour of the journal its searchandretrieval capabilities in particular
but also including problems associated with scanning in illustrations
and the importance of onscreen alerts to the medical profession re
retractions or corrections or more frequently editorials letters to
the editors or followup reports  she closed by inviting the audience
to join aaas on  july when ojcct was scheduled to go online

                                 


discussion  additional features of ojcct 


in the lengthy discussion that followed lebrons presentation these
points emerged

      the sgml text can be tailored as users wish

      all these articles have a fairly simple document definition

      documenttype definitions dtds were developed and given to ojcct
     for coding

      no articles will be removed from the journal  because there are
     no back issues there are no lost issues either  once a subscriber
     logs onto the journal he or she has access not only to the currently
     published materials but retrospectively to everything that has been
     published in it  thus the table of contents grows bigger  the date
     of publication serves to distinguish between currently published
     materials and older materials

      the pricing system for the journal resembles that for most medical
     journals  for   for a year plus telecommunications charges
     there are no connect time charges    for   for the
     entire year for single users though the journal can be put on a
     local area network lan  however only one person can access the
     journal at a time  site licenses may come in the future

      aaas is working closely with colleagues at oclc to display
     mathematical equations on screen

      without compromising any steps in the editorial process the
     technology has reduced the time lag between when a manuscript is
     originally submitted and the time it is accepted the review process
     does not differ greatly from the standard sixtoeight weeks
     employed by many of the hardcopy journals  the process still
     depends on people

      as far as a preservation copy is concerned articles will be
     maintained on the computer permanently and subscribers as part of
     their subscription will receive a microfichequality archival copy
     of everything published during that year in addition reprints can
     be purchased in much the same way as in a hardcopy environment 
     hard copies are prepared but are not the primary medium for the
     dissemination of the information

      because ojcct is not yet on line it is difficult to know how many
     people would simply browse through the journal on the screen as
     opposed to downloading the whole thing and printing it out a mix of
     both types of users likely will result

                                 


personius  developments in technology over the past decade  the class
project  advantages for technology and for the class project 
developing a network application an underlying assumption of the project
 details of the scanning process  printondemand copies of books 
future plans include development of a browsing tool 


lynne personius assistant director cornell information technologies for
scholarly information services cornell university first commented on
the tremendous impact that developments in technology over the past ten
yearsnetworking in particularhave had on the way information is
handled and how in her own case these developments have counterbalanced
cornells relative geographical isolation  other significant technologies
include scanners which are much more sophisticated than they were ten years
ago mass storage and the dramatic savings that result from it in terms of
both space and money relative to twenty or thirty years ago new and
improved printing technologies which have greatly affected the distribution
of information and of course digital technologies whose applicability to
library preservation remains at issue

given that context personius described the college library access and
storage system class project a library preservation project
primarily and what has been accomplished  directly funded by the
commission on preservation and access and by the xerox corporation which
has provided a significant amount of hardware the class project has been
working with a development team at xerox to develop a software
application tailored to library preservation requirements  within
cornell participants in the project have been working jointly with both
library and information technologies  the focus of the project has been
on reformatting and saving books that are in brittle condition 
personius showed workshop participants a brittle book and described how
such books were the result of developments in papermaking around the
beginning of the industrial revolution  the papermaking process was
changed so that a significant amount of acid was introduced into the
actual paper itself which deteriorates as it sits on library shelves

one of the advantages for technology and for the class project is that
the information in brittle books is mostly out of copyright and thus
offers an opportunity to work with material that requires library
preservation and to create and work on an infrastructure to save the
material  acknowledging the familiarity of those working in preservation
with this information personius noted that several things are being
done  the primary preservation technology used today is photocopying of
brittle material  saving the intellectual content of the material is the
main goal  with microfilm copy the intellectual content is preserved on
the assumption that in the future the image can be reformatted in any
other way that then exists

an underlying assumption of the class project from the beginning was
that it would develop a network application  project staff scan books
at a workstation located in the library near the brittle material
an imageserver filing system is located at a distance from that
workstation and a printer is located in another building  all of the
materials digitized and stored on the imagefiling system are cataloged
in the online catalogue  in fact a record for each of these electronic
books is stored in the rlin database so that a record exists of what is
in the digital library throughout standard catalogue procedures  in the
future researchers working from their own workstations in their offices
or their networks will have accesswherever they might bethrough a
request server being built into the new digital library  a second
assumption is that the preferred means of finding the material will be by
looking through a catalogue  personius described the scanning process
which uses a prototype scanner being developed by xerox and which scans a
very high resolution image at great speed  another significant feature
because this is a preservation application is the placing of the pages
that fall apart one for one on the platen  ordinarily a scanner could
be used with some sort of a document feeder but because of this
application that is not feasible  further because class is a
preservation application after the paper replacement is made there a
very careful quality control check is performed  an original book is
compared to the printed copy and verification is made before proceeding
that all of the image all of the information has been captured  then
a new library book is produced  the printed images are rebound by a
commercial binder and a new book is returned to the shelf 
significantly the books returned to the library shelves are beautiful
and useful replacements on acidfree paper that should last a long time
in effect the equivalent of preservation photocopies  thus the project
has a library of digital books  in essence class is scanning and
storing books as  dotperinch bitmapped images compressed using
group  ccitt ie the french acronym for international consultative
committee for telegraph and telephone compression  they are stored as
tiff files on an optical filing system that is composed of a database
used for searching and locating the books and an optical jukebox that
stores  twelveinch platters  a veryhighresolution printed copy of
these books at  dots per inch is created using a xerox docutech
printer to make the paper replacements on acidfree paper

personius maintained that the class project presents an opportunity to
introduce people to books as digital images by using a paper medium 
books are returned to the shelves while people are also given the ability
to print on demandto make their own copies of books  personius
distributed copies of an engineering journal published by engineering
students at cornell around  as an example of what a printondemand
copy of material might be like  this very cheap copy would be available
to people to use for their own research purposes and would bridge the gap
between an electronic work and the paper that readers like to have 
personius then attempted to illustrate a very early prototype of
networked access to this digital library  xerox corporation has
developed a prototype of a view station that can send images across the
network to be viewed

the particular library brought down for demonstration contained two
mathematics books  class is developing and will spend the next year
developing an application that allows people at workstations to browse
the books  thus class is developing a browsing tool on the assumption
that users do not want to read an entire book from a workstation but
would prefer to be able to look through and decide if they would like to
have a printed copy of it

                                 


discussion  re retrieval software  digital file copyright  scanning
rate during production  autosegmentation  criteria employed in
selecting books for scanning  compression and decompression of images 
ocr not precluded 


during the questionandanswer period that followed her presentation
personius made these additional points

      re retrieval software cornell is developing a unixbased server
     as well as clients for the server that support multiple platforms
     macintosh ibm and sun workstations in the hope that people from
     any of those platforms will retrieve books a further operating
     assumption is that standard interfaces will be used as much as
     possible where standards can be put in place because class
     considers this retrieval software a library application and would
     like to be able to look at material not only at cornell but at other
     institutions

      the phrase digital file copyright by cornell university was
     added at the advice of cornells legal staff with the caveat that it
     probably would not hold up in court  cornell does not want people
     to copy its books and sell them but would like to keep them
     available for use in a library environment for library purposes

      in production the scanner can scan about  pages per hour
     capturing  dots per inch

      the xerox software has filters to scan halftone material and avoid
     the moire patterns that occur when halftone material is scanned 
     xerox has been working on hardware and software that would enable
     the scanner itself to recognize this situation and deal with it
     appropriatelya kind of autosegmentation that would enable the
     scanner to handle halftone material as well as text on a single page

      the books subjected to the elaborate process described above were
     selected because class is a preservation project with the first 
     books selected coming from cornells mathematics collection because
     they were still being heavily used and because although they were
     in need of preservation the mathematics library and the mathematics
     faculty were uncomfortable having them microfilmed  they wanted a
     printed copy  thus these books became a logical choice for this
     project  other books were chosen by the projects selection committees
     for experiments with the technology as well as to meet a demand or need

      images will be decompressed before they are sent over the line at
     this time they are compressed and sent to the image filing system
     and then sent to the printer as compressed images they are returned
     to the workstation as compressed dpi images and the workstation
     decompresses and scales them for displayan inefficient way to
     access the material though it works quite well for printing and
     other purposes

      class is also decompressing on macintosh and ibm a slow process
     right now  eventually compression and decompression will take
     place on an image conversion server  tradeoffs will be made based
     on future performance testing concerning where the file is
     compressed and what resolution image is sent

      ocr has not been precluded images are being stored that have been
     scanned at a high resolution which presumably would suit them well
     to an ocr process  because the material being scanned is about 
     years old and was printed with lessthanideal technologies very
     early and preliminary tests have not produced good results  but the
     project is capturing an image that is of sufficient resolution to be
     subjected to ocr in the future  moreover the system architecture
     and the system plan have a logical place to store an ocr image if it
     has been captured  but that is not being done now

                                 

session iii  distribution networks and networking  options for
dissemination


zich  issues pertaining to cdroms  options for publishing in cdrom 


robert zich special assistant to the associate librarian for special
projects library of congress and moderator of this session first noted
the blessed but somewhat awkward circumstance of having four very
distinguished people representing networks and networking or at least
leaning in that direction while lacking anyone to speak from the
strongest possible background in cdroms  zich expressed the hope that
members of the audience would join the discussion  he stressed the
subtitle of this particular session options for dissemination and
concerning cdroms the importance of determining when it would be wise
to consider dissemination in cdrom versus networks  a shopping list of
issues pertaining to cdroms included  the grounds for selecting
commercial publishers and inhouse publication where possible versus
nonprofit or government publication  a similar list for networks
included  determining when one should consider dissemination through a
network identifying the mechanisms or entities that exist to place items
on networks identifying the pool of existing networks determining how a
producer  would choose between networks and identifying the elements of
a business arrangement in a network

options for publishing in cdrom  an outside publisher versus
selfpublication  if an outside publisher is used it can be nonprofit
such as the government printing office gpo or the national technical
information service ntis in the case of government  the pros and cons
associated with employing an outside publisher are obvious  among the
pros there is no trouble getting accepted  one pays the bill and in
effect goes ones way  among the cons when one pays an outside
publisher to perform the work that publisher will perform the work it is
obliged to do but perhaps without the production expertise and skill in
marketing and dissemination that some would seek  there is the body of
commercial publishers that do possess that kind of expertise in
distribution and marketing but that obviously are selective  in
selfpublication one exercises full control but then one must handle
matters such as distribution and marketing  such are some of the options
for publishing in the case of cdrom

in the case of technical and design issues which are also important
there are many matters which many at the workshop already knew a good
deal about  retrieval system requirements and costs what to do about
images the various capabilities and platforms the tradeoffs between
cost and performance concerns about localarea networkability
interoperability etc

                                 


lynch  creating networked information is different from using networks
as an access or dissemination vehicle  networked multimedia on a large
scale does not yet work  typical cdrom publication model a twoedged
sword  publishing information on a cdrom in the present world of
immature standards  contrast between cdrom and network pricing 
examples demonstrated earlier in the day as a set of insular information
gems  paramount need to link databases  layering to become increasingly
necessary  project needs and the issues of information reuse and active
versus passive use  xwindows as a way of differentiating between
network access and networked information  barriers to the distribution
of networked multimedia information  need for good realtime delivery
protocols  the question of presentation integrity in clientserver
computing in the academic world  recommendations for producing multimedia


clifford lynch director library automation university of california
opened his talk with the general observation that networked information
constituted a difficult and elusive topic because it is something just
starting to develop and not yet fully understood  lynch contended that
creating genuinely networked information was different from using
networks as an access or dissemination vehicle and was more sophisticated
and more subtle  he invited the members of the audience to extrapolate
from what they heard about the preceding demonstration projects to what
sort of a world of electronics informationscholarly archival
cultural etcthey wished to end up with ten or fifteen years from now 
lynch suggested that to extrapolate directly from these projects would
produce unpleasant results

putting the issue of cdrom in perspective before getting into
generalities on networked information lynch observed that those engaged
in multimedia today who wish to ship a product so to say probably do
not have much choice except to use cdrom  networked multimedia on a
large scale basically does not yet work because the technology does not
exist  for example anybody who has tried moving images around over the
internet knows that this is an exciting touchandgo process a
fascinating and fertile area for experimentation research and
development but not something that one can become deeply enthusiastic
about committing to production systems at this time

this situation will change lynch said  he differentiated cdrom from
the practices that have been followed up to now in distributing data on
cdrom  for lynch the problem with cdrom is not its portability or its
slowness but the twoedged sword of having the retrieval application and
the user interface inextricably bound up with the data which is the
typical cdrom publication model  it is not a case of publishing data
but of distributing a typically standalone typically closed system
allsoftware user interface and dataon a little disk  hence all
the betweendisk navigational issues as well as the impossibility in most
cases of integrating data on one disk with that on another  most cdrom
retrieval software does not network very gracefully at present  however
in the present world of immature standards and lack of understanding of
what network information is or what the ground rules are for creating or
using it publishing information on a cdrom does add value in a very
real sense

lynch drew a contrast between cdrom and network pricing and in doing so
highlighted something bizarre in information pricing  a large
institution such as the university of california has vendors who will
offer to sell information on cdrom for a price per year in four digits
but for the same data eg an abstracting and indexing database on
magnetic tape regardless of how many people may use it concurrently
will quote a price in six digits

what is packaged with the cdrom in one sense adds valuea complete
access system not just raw unrefined informationalthough it is not
generally perceived that way  this is because the access software
although it adds value is viewed by some people particularly in the
university environment where there is a very heavy commitment to
networking as being developed in the wrong direction

given that context lynch described the examples demonstrated as a set of
insular information gemsperseus for example offers nicely linked
information but would be very difficult to integrate with other
databases that is to link together seamlessly with other source files
from other sources  it resembles an island and in this respect is
similar to numerous standalone projects that are based on videodiscs
that is on the singleworkstation concept

as scholarship evolves in a network environment the paramount need will
be to link databases  we must link personal databases to public
databases to group databases in fairly seamless wayswhich is
extremely difficult in the environments under discussion with copies of
databases proliferating all over the place

the notion of layering also struck lynch as lurking in several of the
projects demonstrated  several databases in a sense constitute
information archives without a significant amount of navigation built in 
educators critics and others will want a layered structureone that
defines or links paths through the layers to allow users to reach
specific points  in lynchs view layering will become increasingly
necessary and not just within a single resource but across resources
eg tracing mythology and cultural themes across several classics
databases as well as a database of renaissance culture  this ability to
organize resources to build things out of multiple other things on the
network or select pieces of it represented for lynch one of the key
aspects of network information

contending that information reuse constituted another significant issue
lynch commended to the audiences attention project needs ie national
engineering education delivery system  this projects objective is to
produce a database of engineering courseware as well as the components
that can be used to develop new courseware  in a number of the existing
applications lynch said the issue of reuse how much one can take apart
and reuse in other applications was not being well considered  he also
raised the issue of active versus passive use one aspect of which  is
how much information will be manipulated locally by users  most people
he argued may do a little browsing and then will wish to print  lynch
was uncertain how these resources would be used by the vast majority of
users in the network environment

lynch next said a few words about xwindows as a way of differentiating
between network access and networked information  a number of the
applications demonstrated at the workshop could be rewritten to use x
across the network so that one could run them from any xcapable device
a workstation an x terminaland transact with a database across the
network  although this opens up access a little assuming one has enough
network to handle it it does not provide an interface to develop a
program that conveniently integrates information from multiple databases 
x is a viewing technology that has limits  in a real sense it is just a
graphical version of remote login across the network  xtype applications
represent only one step in the progression towards real access

lynch next discussed barriers to the distribution of networked multimedia
information  the heart of the problem is a lack of standards to provide
the ability for computers to talk to each other retrieve information
and shuffle it around fairly casually  at the moment little progress is
being made on standards for networked information for example present
standards do not cover images digital voice and digital video  a
useful tool kit of exchange formats for basic texts is only now being
assembled  the synchronization of content streams ie synchronizing a
voice track to a video track establishing temporal relations between
different components in a multimedia object constitutes another issue
for networked multimedia that is just beginning to receive attention

underlying network protocols also need some work good realtime
delivery protocols on the internet do not yet exist  in lynchs view
highly important in this context is the notion of networked digital
object ids the ability of one object on the network to point to another
object or component thereof on the network  serious bandwidth issues
also exist  lynch was uncertain if billionbitpersecond networks would
prove sufficient if numerous people ran video in parallel

lynch concluded by offering an issue for database creators to consider
as well as several comments about what might constitute good trial
multimedia experiments  in a networked information world the database
builder or service builder publisher does not exercise the same
extensive control over the integrity of the presentation strange
programs munge with ones data before the user sees it  serious
thought must be given to what guarantees integrity of presentation  part
of that is related to where one draws the boundaries around a networked
information service  this question of presentation integrity in
clientserver computing has not been stressed enough in the academic
world lynch argued though commercial service providers deal with it
regularly

concerning multimedia lynch observed that good multimedia at the moment
is hideously expensive to produce  he recommended producing multimedia
with either very high sale value or multimedia with a very long life
span or multimedia that will have a very broad usage base and whose
costs therefore can be amortized among large numbers of users  in this
connection historical and humanistically oriented material may be a good
place to start because it tends to have a longer life span than much of
the scientific material as well as a wider user base  lynch noted for
example that american memory fits many of the criteria outlined  he
remarked the extensive discussion about bringing the internet or the
national research and education network nren into the k environment
as a way of helping the american educational system

lynch closed by noting that the kinds of applications demonstrated struck
him as excellent justifications of broadscale networking for k but
that at this time no killer application exists to mobilize the k
community to obtain connectivity

                                 


discussion  dearth of genuinely interesting applications on the network
a slowchanging situation  the issue of the integrity of presentation in
a networked environment  several reasons why cdrom software does not
network 


during the discussion period that followed lynchs presentation several
additional points were made

lynch reiterated even more strongly his contention that historically
once one goes outside highend science and the group of those who need
access to supercomputers there is a great dearth of genuinely
interesting applications on the network  he saw this situation changing
slowly with some of the scientific databases and scholarly discussion
groups and electronic journals coming on as well as with the availability
of wide area information servers wais and some of the databases that
are being mounted there  however many of those things do not seem to
have piqued great popular interest  for instance most high school
students of lynchs acquaintance would not qualify as devotees of serious
molecular biology

concerning the issue of the integrity of presentation lynch believed
that a couple of information providers have laid down the law at least on
certain things  for example his recollection was that the national
library of medicine feels strongly that one needs to employ the
identifier field if he or she is to mount a database commercially  the
problem with a real networked environment is that one does not know who
is reformatting and reprocessing ones data when one enters a client
server mode  it becomes anybodys guess for example if the network
uses a z server or what clients are doing with ones data  a data
provider can say that his contract will only permit clients to have
access to his data after he vets them and their presentation and makes
certain it suits him  but lynch held out little expectation that the
network marketplace would evolve in that way because it required too
much prior negotiation

cdrom software does not network for a variety of reasons lynch said 
he speculated that cdrom publishers are not eager to have their products
really hook into wide area networks because they fear it will make their
data suppliers nervous  moreover until relatively recently one had to
be rather adroit to run a full tcpip stack plus applications on a
pcsize machine whereas nowadays it is becoming easier as pcs grow
bigger and faster  lynch also speculated that software providers had not
heard from their customers until the last year or so or had not heard
from enough of their customers

                                 


besser  implications of disseminating images on the network planning
the distribution of multimedia documents poses two critical
implementation problems  layered approach represents the way to deal
with users capabilities  problems in platform design file size and its
implications for networking  transmission of megabyte size images
impractical  compression and decompression at the users end  promising
trends for compression  a disadvantage of using xwindows  a project at
the smithsonian that mounts images on several networks   


howard besser school of library and information science university of
pittsburgh spoke primarily about multimedia focusing on images and the
broad implications of disseminating them on the network  he argued that
planning the distribution of multimedia documents posed two critical
implementation problems which he framed in the form of two questions 
 what platform will one use and what hardware and software will users
have for viewing of the material  and  how can one deliver a
sufficiently robust set of information in an accessible format in a
reasonable amount of time  depending on whether network or cdrom is the
medium used this question raises different issues of storage
compression and transmission

concerning the design of platforms eg sound gray scale simple
color etc and the various capabilities users may have besser
maintained that a layered approach was the way to deal with users
capabilities  a result would be that users with less powerful
workstations would simply have less functionality  he urged members of
the audience to advocate standards and accompanying software that handle
layered functionality across a wide variety of platforms

besser also addressed problems in platform design namely deciding how
large a machine to design for situations when the largest number of users
have the lowest level of the machine and one desires higher
functionality  besser then proceeded to the question of file size and
its implications for networking  he discussed still images in the main 
for example a digital color image that fills the screen of a standard
megapel workstation sun or next will require one megabyte of storage
for an eightbit image or three megabytes of storage for a true color or
twentyfourbit image  lossless compression algorithms that is
computational procedures in which no data is lost in the process of
compressing and decompressing an imagethe exact bitrepresentation is
maintained might bring storage down to a third of a megabyte per image
but not much further than that  the question of size makes it difficult
to fit an appropriately sized set of these images on a single disk or to
transmit them quickly enough on a network

with these full screen megapel images that constitute a third of a
megabyte one gets  fullscreen images on a onegigabyte disk
a standard cdrom represents approximately  percent of that  storing
images the size of a pc screen just  bit color increases storage
capacity to  images per gigabyte  percent of that gives
one the size of a cdrom which in turn creates a major problem  one
cannot have fullscreen fullcolor images with lossless compression one
must compress them or use a lower resolution  for megabytesize images
anything slower than a t speed is impractical  for example on a
fiftysixkilobaud line it takes three minutes to transfer a
onemegabyte file if it is not compressed and this speed assumes ideal
circumstances no other user contending for network bandwidth  thus
questions of disk access remote display and current telephone
connection speed make transmission of megabytesize images impractical

besser then discussed ways to deal with these large images for example
compression and decompression at the users end  in this connection the
issues of how much one is willing to lose in the compression process and
what image quality one needs in the first place are unknown  but what is
known is that compression entails some loss of data  besser urged that
more studies be conducted on image quality in different situations for
example what kind of images are needed for what kind of disciplines and
what kind of image quality is needed for a browsing tool an intermediate
viewing tool and archiving

besser remarked two promising trends for compression  from a technical
perspective algorithms that use what is called subjective redundancy
employ principles from visual psychophysics to identify and remove
information from the image that the human eye cannot perceive from an
interchange and interoperability perspective the jpeg ie joint
photographic experts group an iso standard compression algorithms also
offer promise  these issues of compression and decompression besser
argued resembled those raised earlier concerning the design of different
platforms  gauging the capabilities of potential users constitutes a
primary goal  besser advocated layering or separating the images from
the applications that retrieve and display them to avoid tying them to
particular software

besser detailed several lessons learned from his work at berkeley with
imagequery especially the advantages and disadvantages of using
xwindows  in the latter category for example retrieval is tied
directly to ones data an intolerable situation in the long run on a
networked system  finally besser described a project of jim wallace at
the smithsonian institution who is mounting images in a extremely
rudimentary way on the compuserv and genie networks and is preparing to
mount them on america on line  although the average user takes over
thirty minutes to download these images assuming a fairly fast modem
nevertheless images have been downloaded  times

besser concluded his talk with several comments on the business
arrangement between the smithsonian and compuserv  he contended that not
enough is known concerning the value of images

                                 


discussion  creating digitized photographic collections nearly
impossible except with large organizations like museums  need for study
to determine quality of images users will tolerate 


during the brief exchange between lesk and besser that followed several
clarifications emerged

lesk argued that the photographers were far ahead of besser  it is
almost impossible to create such digitized photographic collections
except with large organizations like museums because all the
photographic agencies have been going crazy about this and will not sign
licensing agreements on any sort of reasonable terms  lesk had heard
that national geographic for example had tried to buy the right to use
some image in some kind of educational production for  per image but
the photographers will not touch it  they want accounting and payment
for each use which cannot be accomplished within the system  besser
responded that a consortium of photographers headed by a former national
geographic photographer had started assembling its own collection of
electronic reproductions of images with the money going back to the
cooperative

lesk contended that besser was unnecessarily pessimistic about multimedia
images because people are accustomed to lowquality images particularly
from video  besser urged the launching of a study to determine what
users would tolerate what they would feel comfortable with and what
absolutely is the highest quality they would ever need  conceding that
he had adopted a dire tone in order to arouse people about the issue
besser closed on a sanguine note by saying that he would not be in this
business if he did not think that things could be accomplished

                                 


larsen  issues of scalability and modularity  geometric growth of the
internet and the role played by layering  basic functions sustaining
this growth  a librarys roles and functions in a network environment 
effects of implementation of the z protocol for information
retrieval on the library system  the tradeoff between volumes of data
and its potential usage  a snapshot of current trends 


ronald larsen associate director for information technology university
of maryland at college park first addressed the issues of scalability
and modularity  he noted the difficulty of anticipating the effects of
ordersofmagnitude growth reflecting on the twenty years of experience
with the arpanet and internet  recalling the days demonstrations of
cdrom and optical disk material he went on to ask if the field has yet
learned how to scale new systems to enable delivery and dissemination
across largescale networks

larsen focused on the geometric growth of the internet from its inception
circa  to the present and the adjustments required to respond to
that rapid growth  to illustrate the issue of scalability larsen
considered computer networks as including three generic components 
computers network communication nodes and communication media  each
component scales eg computers range from pcs to supercomputers
network nodes scale from interface cards in a pc through sophisticated
routers and gateways and communication media range from baud
dialup facilities through mbps backbone links and eventually to
multigigabitpersecond communication lines and architecturally the
components are organized to scale hierarchically from local area networks
to internationalscale networks  such growth is made possible by
building layers of communication protocols as besser pointed out
by layering both physically and logically a sense of scalability is
maintained from local area networks in offices across campuses through
bridges routers campus backbones fiberoptic links etc up into
regional networks and ultimately into national and international
networks

larsen then illustrated the geometric growth over a twoyear period
through september of the number of networks that comprise the
internet  this growth has been sustained largely by the availability of
three basic functions  electronic mail file transfer ftp and remote
logon telnet  larsen also reviewed the growth in the kind of traffic
that occurs on the network  network traffic reflects the joint contributions
of a larger population of users and increasing use per user  today one sees
serious applications involving moving images across the networka rarity
ten years ago  larsen recalled and concurred with bessers main point
that the interesting problems occur at the application level

larsen then illustrated a model of a librarys roles and functions in a
network environment  he noted in particular the placement of online
catalogues onto the network and patrons obtaining access to the library
increasingly through local networks campus networks and the internet 
larsen supported lynchs earlier suggestion that we need to address
fundamental questions of networked information in order to build
environments that scale in the information sense as well as in the
physical sense

larsen supported the role of the library system as the access point into
the nations electronic collections  implementation of the z
protocol for information retrieval would make such access practical and
feasible  for example this would enable patrons in maryland to search
california libraries or other libraries around the world that are
conformant with z in a manner that is familiar to university of
maryland patrons  this clientserver model also supports moving beyond
secondary content into primary content  the notion of how one links
from secondary content to primary content larsen said represents a
fundamental problem that requires rigorous thought  after noting
numerous network experiments in accessing fulltext materials including
projects supporting the ordering of materials across the network larsen
revisited the issue of transmitting highdensity highresolution color
images across the network and the large amounts of bandwidth they
require  he went on to address the bandwidth and synchronization
problems inherent in sending fullmotion video across the network

larsen illustrated the tradeoff between volumes of data in bytes or
orders of magnitude and the potential usage of that data  he discussed
transmission rates particularly the time it takes to move various forms
of information and what one could do with a network supporting
multigigabitpersecond transmission  at the moment the network
environment includes a composite of datatransmission requirements
volumes and forms going from steady to bursty highvolume and from
very slow to very fast  this aggregate must be considered in the design
construction and operation of multigigabyte networks

larsens objective is to use the networks and library systems now being
constructed to increase access to resources wherever they exist and
thus to evolve toward an online electronic virtual library

larsen concluded by offering a snapshot of current trends  continuing
geometric growth in network capacity and number of users slower
development of applications and glacial development and adoption of
standards  the challenge is to design and develop each new application
system with network access and scalability in mind

                                 


brownrigg  access to the internet cannot be taken for granted  packet
radio and the development of melvyl in  in the division of library
automation at the university of california    design criteria for packet
radio  a demonstration project in san diego and future plans  spread
spectrum  frequencies at which the radios will run and plans to
reimplement the wais server software in the public domain  need for an
infrastructure of radios that do not move around  


edwin brownrigg executive director memex research institute first
polled the audience in order to seek out regular users of the internet as
well as those planning to use it some time in the future  with nearly
everybody in the room falling into one category or the other brownrigg
made a point re access namely that numerous individuals especially those
who use the internet every day take for granted their access to it the
speeds with which they are connected and how well it all works 
however as brownrigg discovered between  and  in australia
if one wants access to the internet but cannot afford it or has some
physical boundary that prevents her or him from gaining access it can
be extremely frustrating  he suggested that because of economics and
physical barriers we were beginning to create a world of haves and havenots
in the process of scholarly communication even in the united states

brownrigg detailed the development of melvyl in academic year  in
the division of library automation at the university of california in
order to underscore the issue of access to the system which at the
outset was extremely limited  in short the project needed to build a
network which at that time entailed use of satellite technology that is
putting earth stations on campus and also acquiring some terrestrial links
from the state of californias microwave system  the installation of
satellite links however did not solve the problem which actually
formed part of a larger problem involving politics and financial resources
for while the project team could get a signal onto a campus it had no means
of distributing the signal throughout the campus  the solution involved
adopting a recent development in wireless communication called packet radio
which combined the basic notion of packetswitching with radio  the project
used this technology to get the signal from a point on campus where it
came down an earth station for example into the libraries because it
found that wiring the libraries especially the older marble buildings
would cost  per terminal

brownrigg noted that ten years ago the project had neither the public
policy nor the technology that would have allowed it to use packet radio
in any meaningful way  since then much had changed  he proceeded to
detail research and development of the technology how it is being
deployed in california and what direction he thought it would take
the design criteria are to produce a highspeed onetime lowcost
highquality secure licensefree device packet radio that one can
plug in and play today forget about it and have access to the internet 
by high speed brownrigg meant  megabyte and  megabytes  those units
have been built he continued and are in the process of being
typecertified by an independent underwriting laboratory so that they can
be typelicensed by the federal communications commission  as is the
case with citizens band one will be able to purchase a unit and not have
to worry about applying for a license

the basic idea brownrigg elaborated is to take highspeed radio data
transmission and create a backbone network that at certain strategic
points in the network will gateway into a mediumspeed packet radio
ie one that runs at  kilobytes so that perhaps by 
people like those in the audience for the price of a vcr could purchase
a mediumspeed radio for the office or home have full network connectivity
to the internet and partake of all its services with no need for an fcc
license and no regular bill from the local common carrier  brownrigg
presented several details of a demonstration project currently taking
place in san diego and described plans pending funding to install a
fullbore network in the san francisco area  this network will have 
nodes running at backbone speeds and  of these nodes will be libraries
which in turn will be the gateway ports to the  kilobyte radios that
will give coverage for the neighborhoods surrounding the libraries

brownrigg next explained part  a new rule within title  of the
code of federal regulations enacted by the fcc in   this rule
challenged the industry which has only now risen to the occasion to
build a radio that would run at no more than one watt of output power and
use a fairly exotic method of modulating the radio wave called spread
spectrum  spread spectrum in fact permits the building of networks so
that numerous data communications can occur simultaneously without
interfering with each other within the same wide radio channel

brownrigg explained that the frequencies at which the radios would run
are very short wave signals  they are well above standard microwave and
radar  with a radio wave that small one watt becomes a tremendous punch
per bit and thus makes transmission at reasonable speed possible  in
order to minimize the potential for congestion the project is
undertaking to reimplement software which has been available in the
networking business and is taken for granted now for example tcpip
routing algorithms bridges and gateways  in addition the project
plans to take the wais server software in the public domain and
reimplement it so that one can have a wais server on a mac instead of a
unix machine  the memex research institute believes that libraries in
particular will want to use the wais servers with packet radio  this
project which has a team of about twelve people will run through 
and will include the  libraries already mentioned as well as other
professionals such as those in the medical profession engineering and
law  thus the need is to create an infrastructure of radios that do not
move around which brownrigg hopes will solve a problem not only for
libraries but for individuals who by and large today do not have access
to the internet from their homes and offices

                                 


discussion  project operating frequencies 


during a brief discussion period which also concluded the days
proceedings brownrigg stated that the project was operating in four
frequencies  the slow speed is operating at  megahertz and it would
later go up to  megahertz  with the highspeed frequency the
onemegabyte radios will run at  gigabits and  will run at  
at  rain can be a factor but it would have to be tropical rain
unlike what falls in most parts of the united states

                                 

session iv  image capture text capture overview of text and
             image storage formats

william hooton vice president of operations inet moderated this session


kenney  factors influencing development of cxp  advantages of using
digital technology versus photocopy and microfilm  a primary goal of
cxp publishing challenges  characteristics of copies printed  quality
of samples achieved in image capture  several factors to be considered
in choosing scanning  emphasis of cxp on timely and costeffective
production of blackandwhite printed facsimiles  results of producing
microfilm from digital files  advantages of creating microfilm  details
concerning production  costs  role of digital technology in library
preservation 


anne kenney associate director department of preservation and
conservation cornell university opened her talk by observing that the
cornell xerox project cxp has been guided by the assumption that the
ability to produce printed facsimiles or to replace paper with paper
would be important at least for the present generation of users and
equipment  she described three factors that influenced development of
the project   because the project has emphasized the preservation of
deteriorating brittle books the quality of what was produced had to be
sufficiently high to return a paper replacement to the shelf  cxp was
only interested in using   a system that was costeffective which
meant that it had to be costcompetitive with the processes currently
available principally photocopy and microfilm and  new or currently
available product hardware and software

kenney described the advantages that using digital technology offers over
both photocopy and microfilm   the potential exists to create a higher
quality reproduction of a deteriorating original than conventional
lightlens technology   because a digital image is an encoded
representation it can be reproduced again and again with no resulting
loss of quality as opposed to the situation with lightlens processes
in which there is discernible difference between a second and a
subsequent generation of an image   a digital image can be manipulated
in a number of ways to improve image capture for example xerox has
developed a windowing application that enables one to capture a page
containing both text and illustrations in a manner that optimizes the
reproduction of both  with lightlens technology one must choose which
to optimize text or the illustration in preservation microfilming the
current practice is to shoot an illustrated page twice once to highlight
the text and the second time to provide the best capture for the
illustration   a digital image can also be edited density levels
adjusted to remove underlining and stains and to increase legibility for
faint documents   onscreen inspection can take place at the time of
initial setup and adjustments made prior to scanning factors that
substantially reduce the number of retakes required in quality control

a primary goal of cxp has been to evaluate the paper output printed on
the xerox docutech a highspeed printer that produces dpi pages from
scanned images at a rate of  pages a minute  kenney recounted several
publishing challenges to represent faithful and legible reproductions of
the originals that the dpi copy for the most part successfully
captured  for example many of the deteriorating volumes in the project
were heavily illustrated with fine line drawings or halftones or came in
languages such as japanese in which the buildup of characters comprised
of varying strokes is difficult to reproduce at lower resolutions a
surprising number of them came with annotations and mathematical
formulas which it was critical to be able to duplicate exactly

kenney noted that  the copies are being printed on paper that meets the
ansi standards for performance  the docutech printer meets the machine
and toner requirements for proper adhesion of print to page as described
by the national archives and thus  paper product is considered to be
the archival equivalent of preservation photocopy

kenney then discussed several samples of the quality achieved in the
project that had been distributed in a handout for example a copy of a
printondemand version of the  reed lecture on the steam turbine
which contains halftones line drawings and illustrations embedded in
text the first four loose pages in the volume compared the capture
capabilities of scanning to photocopy for a standard test target the
ieee standard a  test chart  in all instances scanning proved
superior to photocopy though only slightly more so in one

conceding the simplistic nature of her review of the quality of scanning
to photocopy kenney described it as one representation of the kinds of
settings that could be used with scanning capabilities on the equipment
cxp uses  kenney also pointed out that cxp investigated the quality
achieved with binary scanning only and noted the great promise in gray
scale and color scanning whose advantages and disadvantages need to be
examined  she argued further that scanning resolutions and file formats
can represent a complex tradeoff between the time it takes to capture
material file size fidelity to the original and onscreen display and
printing and equipment availability  all these factors must be taken
into consideration

cxp placed primary emphasis on the production in a timely and
costeffective manner of printed facsimiles that consisted largely of
blackandwhite text  with binary scanning large files may be
compressed efficiently and in a lossless manner ie no data is lost in
the process of compressing and decompressing an imagethe exact
bitrepresentation is maintained using group  ccitt ie the french
acronym for international consultative committee for telegraph and
telephone compression  cxp was getting compression ratios of about
forty to one  grayscale compression which primarily uses jpeg is much
less economical and can represent a lossy compression ie not
lossless so that as one compresses and decompresses the illustration
is subtly changed  while binary files produce a highquality printed
version it appears  that other combinations of spatial resolution with
gray andor color hold great promise as well and  that gray scale can
represent a tremendous advantage for onscreen viewing  the quality
associated with binary and gray scale also depends on the equipment used 
for instance binary scanning produces a much better copy on a binary
printer

among cxps findings concerning the production of microfilm from digital
files kenney reported that the digital files for the same reed lecture
were used to produce sample film using an electron beam recorder  the
resulting film was faithful to the image capture of the digital files
and while cxp felt that the text and image pages represented in the reed
lecture were superior to that of the lightlens film the resolution
readings for the  dpi were not as high as standard microfilming 
kenney argued that the standards defined for lightlens technology are
not totally transferable to a digital environment  moreover they are
based on definition of quality for a preservation copy  although making
this case will prove to be a long uphill struggle cxp plans to continue
to investigate the issue over the course of the next year

kenney concluded this portion of her talk with a discussion of the
advantages of creating film  it can serve as a primary backup and as a
preservation master to the digital file it could then become the print
or production master and service copies could be paper film optical
disks magnetic media or onscreen display

finally kenney presented details re production

      development and testing of a moderatelyhigh resolution production
     scanning workstation represented a third goal of cxp to date 
     volumes have been scanned or about  images

      the resulting digital files are stored and used to produce
     hardcopy replacements for the originals and additional prints on
     demand although the initial costs are high scanning technology
     offers an affordable means for reformatting brittle material

      a technician in production mode can scan  pages per hour when
     performing singlesheet scanning which is a necessity when working
     with truly brittle paper this figure is expected to increase
     significantly with subsequent iterations of the software from xerox
     a threemonth timeandcost study of scanning found that the average
     page book would take about an hour and forty minutes to scan
     this figure included the time for setup which involves keying in
     primary bibliographic data going into quality control mode to
     define page size establishing fronttoback registration and
     scanning sample pages to identify a default range of settings for
     the entire bookfunctions not dissimilar to those performed by
     filmers or those preparing a book for photocopy

      the final step in the scanning process involved rescans which
     happily were few and far between representing well under  percent
     of the total pages scanned

in addition to technician time cxp costed out equipment amortized over
four years the cost of storing and refreshing the digital files every
four years and the cost of printing and binding bookcloth binding a
paper reproduction  the total amounted to a little under  per single
page volume with  percent overhead includeda figure competitive
with the prices currently charged by photocopy vendors

of course with scanning in addition to the paper facsimile one is left
with a digital file from which subsequent copies of the book can be
produced for a fraction of the cost of photocopy with readers afforded
choices in the form of these copies

kenney concluded that digital technology offers an electronic means for a
library preservation effort to pay for itself  if a brittlebook program
included the means of disseminating reprints of books that are in demand
by libraries and researchers alike the initial investment in capture
could be recovered and used to preserve additional but less popular
books  she disclosed that an economic model for a selfsustaining
program could be developed for cxps report to the commission on
preservation and access cpa

kenney stressed that the focus of cxp has been on obtaining high quality
in a production environment  the use of digital technology is viewed as
an affordable alternative to other reformatting options

                                 


andre  overview and history of natdp  various agricultural cdrom
products created inhouse and by service bureaus  pilot project on
internet transmission  additional products in progress 


pamela andre associate director for automation national agricultural
text digitizing program natdp national agricultural library nal
presented an overview of natdp which has been underway at nal the last
four years before judith zidar discussed the technical details  andre
defined agricultural information as a broad range of material going from
basic and applied research in the hard sciences to the onepage pamphlets
that are distributed by the cooperative state extension services on such
things as how to grow blueberries

natdp began in late  with a meeting of representatives from the
landgrant library community to deal with the issue of electronic
information  nal and fortyfive of these libraries banded together to
establish this projectto evaluate the technology for converting what
were then source documents in paper form into electronic form to provide
access to that digital information and then to distribute it 
distributing that material to the communitythe university community as
well as the extension service community potentially down to the county
levelconstituted the groups chief concern

since january  when the microcomputerbased scanning system was
installed at nal natdp has done a variety of things concerning which
zidar would provide further details  for example the first technology
considered in the projects discussion phase was digital videodisc which
indicates how long ago it was conceived

over the four years of this project four separate cdrom products on
four different agricultural topics were created two at a
scanningandocr station installed at nal and two by service bureaus 
thus natdp has gained comparative information in terms of those relative
costs  each of these products contained the full ascii text as well as
page images of the material or between  and  pages of material
on these disks  topics included aquaculture food agriculture and
science ie international agriculture and research acid rain and
agent orange which was the final product distributed approximately
eighteen months before the workshop

the third phase of natdp focused on delivery mechanisms other than
cdrom  at the suggestion of clifford lynch who was a technical
consultant to the project at this point natdp became involved with the
internet and initiated a project with the help of north carolina state
university in which fourteen of the landgrant university libraries are
transmitting digital images over the internet in response to interlibrary
loan requestsa topic for another meeting  at this point the pilot
project had been completed for about a year and the final report would be
available shortly after the workshop  in the meantime the projects
success had led to its extension  andre noted that one of the first
things done under the program title was to select a retrieval package to
use with subsequent products windows personal librarian was the package
of choice after a lengthy evaluation
  
three additional products had been planned and were in progress

      an arrangement with the american society of agronomya
     professional society that has published the agronomy journal since
     about to scan and create bitmapped images of its journal 
     asa granted permission first to put and then to distribute this
     material in electronic form to hold it at nal and to use these
     electronic images as a mechanism to deliver documents or print out
     material for patrons among other uses  effectively nal has the
     right to use this material in support of its program 
     significantly this arrangement offers a potential cooperative
     model for working with other professional societies in agriculture
     to try to do the same thingput the journals of particular interest
     to agriculture research into electronic form

      an extension of the earlier product on aquaculture

      the george washington carver papersa joint project with
     tuskegee university to scan and convert from microfilm some 
     images of carvers papers letters and drawings

it was anticipated that all of these products would appear no more than
six months after the workshop

                                 


zidar  a separate arena for scanning  steps in creating a database 
image capture with and without performing ocr  keying in tracking data
 scanning with electronic and manual tracking  adjustments during
scanning process  scanning resolutions  compression  deskewing and
filtering  image capture from microform  the papers and letters of
george washington carver  equipment used for a scanning system  


judith zidar coordinator national agricultural text digitizing program
natdp national agricultural library nal illustrated the technical
details of natdp including her primary responsibility scanning and
creating databases on a topic and putting them on cdrom

zidar remarked a separate arena from the cdrom projects although the
processing of the material is nearly identical in which natdp is also
scanning material and loading it on a next microcomputer which in turn
is linked to nals integrated library system  thus searches in nals
bibliographic database will enable people to pull up actual page images
and text for any documents that have been entered

in accordance with the sessions topic zidar focused her illustrated
talk on image capture offering a primer on the three main steps in the
process   assemble the printed publications  design the database
database design occurs in the process of preparing the material for
scanning this step entails reviewing and organizing the material
defining the contentswhat will constitute a record what kinds of
fields will be captured in terms of author title etc  perform a
certain amount of markup on the paper publications  nal performs this
task record by record preparing work sheets or some other sort of
tracking material and designing descriptors and other enhancements to be
added to the data that will not be captured from the printed publication 
part of this process also involves determining natdps file and directory
structure  natdp attempts to avoid putting more than approximately 
images in a directory because placing more than that on a cdrom would
reduce the access speed

this upfront process takes approximately two weeks for a
page database  the next step is to capture the page images 
how long this process takes is determined by the decision whether or not
to perform ocr  not performing ocr speeds the process whereas text
capture requires greater care because of the quality of the image  it
has to be straighter and allowance must be made for text on a page not
just for the capture of photographs

natdp keys in tracking data that is a standard bibliographic record
including the title of the book and the title of the chapter which will
later either become the access information or will be attached to the
front of a fulltext record so that it is searchable

images are scanned from a bound or unbound publication chiefly from
bound publications in the case of natdp however because often they are
the only copies and the publications are returned to the shelves  natdp
usually scans one record at a time because its database tracking system
tracks the document in that way and does not require further logical
separating of the images  after performing optical character
recognition natdp moves the images off the hard disk and maintains a
volume sheet  though the system tracks electronically all the
processing steps are also tracked manually with a log sheet

zidar next illustrated the kinds of adjustments that one can make when
scanning from paper and microfilm for example redoing images that need
special handling setting for dithering or gray scale and adjusting for
brightness or for the whole book at one time

natdp is scanning at  dots per inch a standard scanning resolution 
though adequate for capturing text that is all of a standard size 
dpi is unsuitable for any kind of photographic material or for very small
text  many scanners allow for different image formats tiff of course
being a de facto standard  but if one intends to exchange images with
other people the ability to scan other image formats even if they are
less common becomes highly desirable

ccitt group  is the standard compression for normal blackandwhite
images jpeg for gray scale or color   zidar recommended  using the
standard compressions particularly if one attempts to make material
available and to allow users to download images and reuse them from
cdroms and  maintaining the ability to output an uncompressed image
because in image exchange uncompressed images are more likely to be able
to cross platforms

zidar emphasized the importance of deskewing and filtering as
requirements on natdps upgraded system  for instance scanning bound
books particularly books published by the federal government whose pages
are skewed and trying to scan them straight if ocr is to be performed
is extremely timeconsuming  the same holds for filtering of
poorquality or older materials

zidar described image capture from microform using as an example three
reels from a sixtysevenreel set of the papers and letters of george
washington carver that had been produced by tuskegee university  these
resulted in approximately  images which natdp had had scanned by
its service contractor science applications international corporation
saic  natdp also created bibliographic records for access  natdp did
not have such specialized equipment as a microfilm scanner

unfortunately the process of scanning from microfilm was not an
unqualified success zidar reported  because microfilm frame sizes vary
occasionally some frames were missed which without spending much time
and money could not be recaptured

ocr could not be performed from the scanned images of the frames  the
bleeding in the text simply output text when ocr was run that could not
even be edited  natdp tested for negative versus positive images
landscape versus portrait orientation and single versus dualpage
microfilm none of which seemed to affect the quality of the image but
also on none of them could ocr be performed

in selecting the microfilm they would use therefore natdp had other
factors in mind  zidar noted two factors that influenced the quality of
the images   the inherent quality of the original and  the amount of
size reduction on the pages

the carver papers were selected because they are informative and visually
interesting treat a single subject and are valuable in their own right 
the images were scanned and divided into logical records by saic then
delivered and loaded onto natdps system where bibliographic
information taken directly from the images was added  scanning was
completed in summer  and by the end of summer  the disk was
scheduled to be published

problems encountered during processing included the following  because
the microfilm scanning had to be done in a batch adjustment for
individual page variations was not possible  the frame size varied on
account of the nature of the material and therefore some of the frames
were missed while others were just partial frames  the only way to go
back and capture this material was to print out the page with the
microfilm reader from the missing frame and then scan it in from the
page which was extremely timeconsuming  the quality of the images
scanned from the printout of the microfilm compared unfavorably with that
of the original images captured directly from the microfilm  the
inability to perform ocr also was a major disappointment  at the time
computer output microfilm was unavailable to test

the equipment used for a scanning system was the last topic addressed by
zidar  the type of equipment that one would purchase for a scanning
system included  a microcomputer at least a  but preferably a 
a large hard disk  megabyte at minimum a multitasking operating
system that allows one to run some things in batch in the background
while scanning or doing text editing for example unix or os and
theoretically windows a highspeed scanner and scanning software that
allows one to make the various adjustments mentioned earlier a
highresolution monitor  dpi  ocr software and hardware to perform
text recognition an optical disk subsystem on which to archive all the
images as the processing is done file management and tracking software

zidar opined that the software one purchases was more important than the
hardware and might also cost more than the hardware but it was likely to
prove critical to the success or failure of ones system  in addition to
a standalone scanning workstation for image capture then text capture
requires one or two editing stations networked to this scanning station
to perform editing  editing the text takes two or three times as long as
capturing the images

finally zidar stressed the importance of buying an open system that allows
for more than one vendor complies with standards and can be upgraded

                                 


waters yale university librarys master plan to convert microfilm to
digital imagery pob  the place of electronic tools in the library of
the future  the uses of images and an image library  primary input from
preservation microfilm  features distinguishing pob from cxp and key
hypotheses guiding pob  use of vendor selection process to facilitate
organizational work  criteria for selecting vendor  finalists and
results of process for yale  key factor distinguishing vendors 
components design principles and some estimated costs of pob  role of
preservation materials in developing imaging market  factors affecting
quality and cost  factors affecting the usability of complex documents
in image form  


donald waters head of the systems office yale university library
reported on the progress of a master plan for a project at yale to
convert microfilm to digital imagery project open book pob  stating
that pob was in an advanced stage of planning waters detailed in
particular the process of selecting a vendor partner and several key
issues under discussion as yale prepares to move into the project itself 
he commented first on the vision that serves as the context of pob and
then described its purpose and scope

waters sees the library of the future not necessarily as an electronic
library but as a place that generates preserves and improves for its
clients ready access to both intellectual and physical recorded
knowledge  electronic tools must find a place in the library in the
context of this vision  several roles for electronic tools include
serving as  indirect sources of electronic knowledge or as finding
aids the online catalogues the articlelevel indices registers for
documents and archives direct sources of recorded knowledge fulltext
images and various kinds of compound sources of recorded knowledge the
socalled compound documents of hypertext mixed text and image
mixedtext image format and multimedia

pob is looking particularly at images and an image library the uses to
which images will be put eg storage printing browsing and then use
as input for other processes ocr as a subsequent process to image
capture or creating an image library and also possibly generating
microfilm

while input will come from a variety of sources pob is considering
especially input from preservation microfilm  a possible outcome is that
the film and paper which provide the input for the image library
eventually may go off into remote storage and that the image library may
be the primary access tool

the purpose and scope of pob focus on imaging  though related to cxp
pob has two features which distinguish it   scaleconversion of
 volumes into digital image form and  sourceconversion from
microfilm  given these features several key working hypotheses guide
pob including   since pob is using microfilm it is not concerned with
the image library as a preservation medium   digital imagery can improve
access to recorded knowledge through printing and network distribution at
a modest incremental cost of microfilm   capturing and storing documents
in a digital image form is necessary to further improvements in access
pob distinguishes between the imaging digitizing process and ocr
which at this stage it does not plan to perform

currently in its first or organizational phase pob found that it could
use a vendor selection process to facilitate a good deal of the
organizational work eg creating a project team and advisory board
confirming the validity of the plan establishing the cost of the project
and a budget selecting the materials to convert and then raising the
necessary funds

pob developed numerous selection criteria including  a firm committed
to imagedocument management the ability to serve as systems integrator
in a largescale project over several years interest in developing the
requisite software as a standard rather than a custom product and a
willingness to invest substantial resources in the project itself

two vendors dec and xerox were selected as finalists in october 
and with the support of the commission on preservation and access each
was commissioned to generate a detailed requirements analysis for the
project and then to submit a formal proposal for the completion of the
project which included a budget and costs the terms were that pob would
pay the loser  the results for yale of involving a vendor included 
broad involvement of yale staff across the board at a relatively low
cost which may have longterm significance in carrying out the project
twentyfive to thirty university people are engaged in pob better
understanding of the factors that affect corporate response to markets
for imaging products a competitive proposal and a more sophisticated
view of the imaging markets

the most important factor that distinguished the vendors under
consideration was their identification with the customer  the size and
internal complexity of the company also was an important factor  pob was
looking at large companies that had substantial resources  in the end
the process generated for yale two competitive proposals with xeroxs
the clear winner  waters then described the components of the proposal
the design principles and some of the costs estimated for the process

components are essentially four  a conversion subsystem a
networkaccessible storage subsystem for  books and pob expects
 to  dpi storage browsing stations distributed on the campus
network and network access to the image printers

among the design principles pob wanted conversion at the highest
possible resolution  assuming tiff files tiff files with group 
compression tcpip and ethernet network on campus pob wanted a
clientserver approach with image documents distributed to the
workstations and made accessible through native workstation interfaces
such as windows  pob also insisted on a phased approach to
implementation   a standalone singleuser lowcost entry into the
business with a workstation focused on conversion and allowing pob to
explore user access  movement into a highervolume conversion with
networkaccessible storage and multiple access stations and  a
highvolume conversion fullcapacity storage and multiple browsing
stations distributed throughout the campus

the costs proposed for startup assumed the existence of the yale network
and its two docutech image printers  other startup costs are estimated
at  million over the three phases  at the end of the project the annual
operating costs estimated primarily for the software and hardware proposed
come to about  but these exclude costs for labor needed in the
conversion process network and printer usage and facilities management

finally the selection process produced for yale a more sophisticated
view of the imaging markets  the management of complex documents in
image form is not a preservation problem not a library problem but a
general problem in a broad general industry  preservation materials are
useful for developing that market because of the qualities of the
material  for example much of it is out of copyright  the resolution
of key issues such as the quality of scanning and image browsing also
will affect development of that market

the technology is readily available but changing rapidly  in this
context of rapid change several factors affect quality and cost to
which pob intends to pay particular attention for example the various
levels of resolution that can be achieved  pob believes it can bring
resolution up to  dpi but an interpolation process from  to  is
more likely  the variation quality in microfilm will prove to be a
highly important factor  pob may reexamine the standards used to film in
the first place by looking at this process as a followon to microfilming

other important factors include  the techniques available to the
operator for handling material the ways of integrating quality control
into the digitizing work flow and a work flow that includes indexing and
storage  pobs requirement was to be able to deal with quality control
at the point of scanning  thus thanks to xerox pob anticipates having
a mechanism which will allow it not only to scan in batch form but to
review the material as it goes through the scanner and control quality
from the outset

the standards for measuring quality and costs depend greatly on the uses
of the material including subsequent ocr storage printing and
browsing  but especially at issue for pob is the facility for browsing 
this facility waters said is perhaps the weakest aspect of imaging
technology and the most in need of development

a variety of factors affect the usability of complex documents in image
form among them   the ability of the system to handle the full range
of document types not just monographs but serials multipart
monographs and manuscripts  the location of the database of record
for bibliographic information about the image document which pob wants
to enter once and in the most useful place the online catalog  a
document identifier for referencing the bibliographic information in one
place and the images in another  the technique for making the basic
internal structure of the document accessible to the reader and finally
 the physical presentation on the crt of those documents  pob is ready
to complete this phase now  one last decision involves deciding which
material to scan

                                 


discussion  tiff files constitute de facto standard  naras experience
with image conversion software and text conversion  rfc  
considerable flux concerning available hardware and software solutions 
nal throughput rate during scanning  window management questions 


in the questionandanswer period that followed waterss presentation
the following points emerged

      zidars statement about using tiff files as a standard meant de
     facto standard  this is what most people use and typically exchange
     with other groups across platforms or even occasionally across
     display software

      holmes commented on the unsuccessful experience of nara in
     attempting to run imageconversion software or to exchange between
     applications  what are supposedly tiff files go into other software
     that is supposed to be able to accept tiff but cannot recognize the
     format and cannot deal with it and thus renders the exchange
     useless  re text conversion he noted the different recognition
     rates obtained by substituting the make and model of scanners in
     naras recent test of an intelligent characterrecognition product
     for a new company  in the selection of hardware and software
     holmes argued software no longer constitutes the overriding factor
     it did until about a year ago rather it is perhaps important to
     look at both now

      danny cohen and alan katz of the university of southern california
     information sciences institute began circulating as an internet rfc
     rfc  about a month ago a standard for a tiff interchange
     format for internet distribution of monochrome bitmapped images
     which lynch said he believed would be used as a de facto standard

      fleischhauers impression from hearing these reports and thinking
     about ams experience was that there is considerable flux concerning
     available hardware and software solutions  hooton agreed and
     commented at the same time on zidars statement that the equipment
     employed affects the results produced  one cannot draw a complete
     conclusion by saying it is difficult or impossible to perform ocr
     from scanning microfilm for example with that device  that set of
     parameters and system requirements because numerous other people
     are accomplishing just that using other components perhaps 
     hooton opined that both the hardware and the software were highly
     important  most of the problems discussed today have been solved in
     numerous different ways by other people  though it is good to be
     cognizant of various experiences this is not to say that it will
     always be thus

      at nal the throughput rate of the scanning process for paper
     page by page performing ocr ranges from  to  pages per day
     not performing ocr is considerably faster although how much faster
     is not known  this is for scanning from bound books which is much
     slower

      waters commented on window management questions  dec proposed an
     xwindows solution which was problematical for two reasons  one was
     pobs requirement to be able to manipulate images on the workstation
     and bring them down to the workstation itself and the other was
     network usage

                                 


thoma  illustration of deficiencies in scanning and storage process 
image quality in this process  different costs entailed by better image
quality  techniques for overcoming various deficiencies  fixed
thresholding dynamic thresholding dithering image merge  page edge
effects    


george thoma chief communications engineering branch national library
of medicine nlm illustrated several of the deficiencies discussed by
the previous speakers  he introduced the topic of special problems by
noting the advantages of electronic imaging  for example it is regenerable
because it is a coded file and realtime quality control is possible with
electronic capture whereas in photographic capture it is not

one of the difficulties discussed in the scanning and storage process was
image quality which without belaboring the obvious means different
things for maps medical xrays or broadcast television  in the case of
documents thoma said image quality boils down to legibility of the
textual parts and fidelity in the case of gray or color photo printtype
material  legibility boils down to scan density the standard in most
cases being  dpi  increasing the resolution with scanners that
perform  or  dpi however comes at a cost

better image quality entails at least four different kinds of costs  
equipment costs because the ccd ie chargecouple device with
greater number of elements costs more   time costs that translate to
the actual capture costs because manual labor is involved the time is
also dependent on the fact that more data has to be moved around in the
machine in the scanning or network devices that perform the scanning as
well as the storage   media costs because at high resolutions larger
files have to be stored and  transmission costs because there is just
more data to be transmitted

but while resolution takes care of the issue of legibility in image
quality other deficiencies have to do with contrast and elements on the
page scanned or the image that needed to be removed or clarified  thus
thoma proceeded to illustrate various deficiencies how they are
manifested and several techniques to overcome them

fixed thresholding was the first technique described suitable for
blackandwhite text when the contrast does not vary over the page  one
can have many different threshold levels in scanning devices  thus
thoma offered an example of extremely poor contrast which resulted from
the fact that the stock was a heavy red  this is the sort of image that
when microfilmed fails to provide any legibility whatsoever  fixed
thresholding is the way to change the blacktored contrast to the
desired blacktowhite contrast

other examples included material that had been browned or yellowed by
age  this was also a case of contrast deficiency and correction was
done by fixed thresholding  a final example boils down to the same
thing slight variability but it is not significant  fixed thresholding
solves this problem as well  the microfilm equivalent is certainly legible
but it comes with dark areas  though thoma did not have a slide of the
microfilm in this case he did show the reproduced electronic image

when one has variable contrast over a page or the lighting over the page
area varies especially in the case where a bound volume has light
shining on it the image must be processed by a dynamic thresholding
scheme  one scheme dynamic averaging allows the threshold level not to
be fixed but to be recomputed for every pixel from the neighboring
characteristics  the neighbors of a pixel determine where the threshold
should be set for that pixel

thoma showed an example of a page that had been made deficient by a
variety of techniques including a burn mark coffee stains and a yellow
marker  application of a fixedthresholding scheme thoma argued might
take care of several deficiencies on the page but not all of them 
performing the calculation for a dynamic threshold setting however
removes most of the deficiencies so that at least the text is legible

another problem is representing a gray level with blackandwhite pixels
by a process known as dithering or electronic screening  but dithering
does not provide good image quality for pure blackandwhite textual
material  thoma illustrated this point with examples although its
suitability for photoprint is the reason for electronic screening or
dithering it cannot be used for every compound image  in the document
that was distributed by cxp thoma noticed that the dithered image of the
ieee test chart evinced some deterioration in the text  he presented an
extreme example of deterioration in the text in which compounded
documents had to be set right by other techniques  the technique
illustrated by the present example was an image merge in which the page
is scanned twice and the settings go from fixed threshold to the
dithering matrix the resulting images are merged to give the best
results with each technique

thoma illustrated how dithering is also used in nonphotographic or
nonprint materials with an example of a grayish page from a medical text
which was reproduced to show all of the gray that appeared in the
original  dithering provided a reproduction of all the gray in the
original of another example from the same text

thoma finally illustrated the problem of bordering or pageedge
effects  books and bound volumes that are placed on a photocopy machine
or a scanner produce pageedge effects that are undesirable for two
reasons   the aesthetics of the image after all if the image is to
be preserved one does not necessarily want to keep all of its
deficiencies  compression with the bordering problem thoma
illustrated the compression ratio deteriorated tremendously  one way
to eliminate this more serious problem is to have the operator at the
point of scanning window the part of the image that is desirable and
automatically turn all of the pixels out of that picture to white 

                                 


fleischhauer  ams experience with scanning bound materials  dithering



carl fleischhauer coordinator american memory library of congress
reported ams experience with scanning bound materials which he likened
to the problems involved in using photocopying machines  very few
devices in the industry offer bookedge scanning let alone book cradles 
the problem may be unsolvable fleischhauer said because a large enough
market does not exist for a preservationquality scanner  am is using a
kurzweil scanner which is a bookedge scanner now sold by xerox

devoting the remainder of his brief presentation to dithering
fleischhauer related ams experience with a contractor who was using
unsophisticated equipment and software to reduce moire patterns from
printed halftones  am took the same image and used the dithering
algorithm that forms part of the same kurzweil xerox scanner it
disguised moire patterns much more effectively

fleischhauer also observed that dithering produces a binary file which is
useful for numerous purposes for example printing it on a laser printer
without having to rehalftone it  but it tends to defeat efficient
compression because the very thing that dithers to reduce moire patterns
also tends to work against compression schemes  am thought the
difference in image quality was worth it

                                 


discussion  relative use as a criterion for pobs selection of books to
be converted into digital form 


during the discussion period waters noted that one of the criteria for
selecting books among the  to be converted into digital image form
would be how much relative use they would receivea subject still
requiring evaluation  the challenge will be to understand whether
coherent bodies of material will increase usage or whether pob should
seek material that is being used scan that and make it more accessible 
pob might decide to digitize materials that are already heavily used in
order to make them more accessible and decrease wear on them  another
approach would be to provide a large body of intellectually coherent
material that may be used more in digital form than it is currently used
in microfilm  pob would seek material that was out of copyright

                                 


baronas  origin and scope of aiim  types of documents produced in
aiims standards program  domain of aiims standardization work  aiims
structure  tc  and ms  electronic image management standards 
categories of eim standardization where aiim standards are being
developed   


jean baronas senior manager department of standards and technology
association for information and image management aiim described the
notforprofit association and the national and international programs
for standardization in which aiim is active

accredited for twentyfive years as the nations standards development
organization for document image management aiim began life in a library
community developing microfilm standards  today the association
maintains both its library and businessimage management standardization
activitiesand has moved into electronic imagemanagement
standardization eim

baronas defined the programs scope  aiim deals with   the
terminology of standards and of the technology it uses  methods of
measurement for the systems as well as quality  methodologies for
users to evaluate and measure quality  the features of apparatus used
to manage and edit images and  the procedures used to manage images

baronas noted that three types of documents are produced in the aiim
standards program  the first two accredited by the american national
standards institute ansi are standards and standard recommended
practices  recommended practices differ from standards in that they
contain more tutorial information  a technical report is not an ansi
standard  because aiims policies and procedures for developing
standards are approved by ansi its standards are labeled ansiaiim
followed by the number and title of the standard

baronas then illustrated the domain of aiims standardization work  for
example aiim is the administrator of the us technical advisory group
tag to the international standards organizations iso technical
committee tc ll micrographics and optical memories for document and
image recording storage and use  aiim officially works through ansi in
the international standardization process

baronas described aiims structure including its board of directors its
standards board of twelve individuals active in the imagemanagement
industry its strategic planning and legal admissibility task forces and
its national standards council which is comprised of the members of a
number of organizations who vote on every aiim standard before it is
published  baronas pointed out that aiims liaisons deal with numerous
other standards developers including the optical disk community office
and publishing systems imagecodesandcharacter set committees and the
national information standards organization niso

baronas illustrated the procedures of tc ll which covers all aspects of
image management  when aiims national program has conceptualized a new
project it is usually submitted to the international level so that the
member countries of tc ll can simultaneously work on the development of
the standard or the technical report  baronas also illustrated a classic
microfilm standard ms which deals with numerous imaging concepts that
apply to electronic imaging  originally developed in the ls revised
in the ls and revised again in l this standard is scheduled for
another revision  ms is an active standard whereby users may propose
new density ranges and new methods of evaluating film images in the
standards revision

baronas detailed several electronic imagemanagement standards for
instance ansiaiim ms a qualitycontrol guideline for scanning 
by  blackandwhite office documents  this standard is used with the
ieee fax imagea continuous tone photographic image with gray scales
text and several continuous tone picturesand aiim test target number
 a representative document used in office document management

baronas next outlined the four categories of eim standardization in which
aiim standards are being developed  transfer and retrieval evaluation
optical disc and document scanning applications and design and
conversion of documents  she detailed several of the main projects of
each   in the category of image transfer and retrieval a bilevel
image transfer format ansiaiim ms which is a proposed standard that
describes a file header for image transfer between unlike systems when
the images are compressed using g and g compression  the category of
image evaluation which includes the aiimproposed tr tutorial on image
resolution this technical report will treat the differences and
similarities between classical or photographic and electronic imaging
 design and conversion which includes a proposed technical report
called forms design optimization for eim this report considers how
generalpurpose business forms can be best designed so that scanning is
optimized reprographic characteristics such as type rules background
tint and color will likewise be treated in the technical report 
disk and document scanning applications includes a project a on planning
platters and disk management b on generating an application profile for
eim when images are stored and distributed on cdrom and c on
evaluating scsi and how a common command set can be generated for scsi
so that document scanners are more easily integrated  ansiaiim ms
will also apply to compressed images

                                 


battin  the implications of standards for preservation  a major
obstacle to successful cooperation  a hindrance to access in the digital
environment  standards a doubleedged sword for those concerned with the
preservation of the human record  nearterm prognosis for reliable
archival standards  preservation concerns for electronic media  need
for reconceptualizing our preservation principles  standards in the real
world and the politics of reproduction  need to redefine the concept of
archival and to begin to think in terms of life cycles  cooperation and
the la guardia eight  concerns generated by discussions on the problems
of preserving text and image  general principles to be adopted in a
world without standards 


patricia battin president the commission on preservation and access
cpa addressed the implications of standards for preservation  she
listed several areas where the library profession and the analog world of
the printed book had made enormous contributions over the past hundred
yearsfor example in bibliographic formats binding standards and most
important in determining what constitutes longevity or archival quality

although standards have lightened the preservation burden through the
development of national and international collaborative programs
nevertheless a pervasive mistrust of other peoples standards remains a
major obstacle to successful cooperation battin said

the zeal to achieve perfection regardless of the cost has hindered
rather than facilitated access in some instances and in the digital
environment where no real standards exist has brought an ironically
just reward

battin argued that standards are a doubleedged sword for those concerned
with the preservation of the human record that is the provision of
access to recorded knowledge in a multitude of media as far into the
future as possible  standards are essential to facilitate
interconnectivity and access but battin said as lynch pointed out
yesterday if set too soon they can hinder creativity expansion of
capability and the broadening of access  the characteristics of
standards for digital imagery differ radically from those for analog
imagery  and the nature of digital technology implies continuing
volatility and change  to reiterate precipitous standardsetting can
inhibit creativity but delayed standardsetting results in chaos

since in battins opinion the nearterm prognosis for reliable archival
standards as defined by librarians in the analog world is poor two
alternatives remain  standing pat with the old technology or
reconceptualizing

preservation concerns for electronic media fall into two general domains 
one is the continuing assurance of access to knowledge originally
generated stored disseminated and used in electronic form  this
domain contains several subdivisions including  the closed
proprietary systems discussed the previous day bundled information such
as electronic journals and government agency records and electronically
produced or captured raw data and  the application of digital
technologies to the reformatting of materials originally published on a
deteriorating analog medium such as acid paper or videotape

the preservation of electronic media requires a reconceptualizing of our
preservation principles during a volatile standardless transition which
may last far longer than any of us envision today  battin urged the
necessity of shifting focus from assessing measuring and setting
standards for the permanence of the medium to the concept of managing
continuing access to information stored on a variety of media and
requiring a variety of everchanging hardware and software for accessa
fundamental shift for the library profession

battin offered a primer on how to move forward with reasonable confidence
in a world without standards  her comments fell roughly into two sections
 standards in the real world and  the politics of reproduction

in regard to realworld standards battin argued the need to redefine the
concept of archive and to begin to think in terms of life cycles  in
the past the naive assumption that paper would last forever produced a
cavalier attitude toward life cycles  the transient nature of the
electronic media has compelled people to recognize and accept upfront the
concept of life cycles in place of permanency

digital standards have to be developed and set in a cooperative context
to ensure efficient exchange of information  moreover during this
transition period greater flexibility concerning how concepts such as
backup copies and archival copies in the cxp are defined is necessary
or the opportunity to move forward will be lost

in terms of cooperation particularly in the university setting battin
also argued the need to avoid going off in a hundred different
directions  the cpa has catalyzed a small group of universities called
the la guardia eightbecause la guardia airport is where meetings take
placeharvard yale cornell princeton penn state tennessee
stanford and usc to develop a digital preservation consortium to look
at all these issues and develop de facto standards as we move along
instead of waiting for something that is officially blessed  continuing
to apply analog values and definitions of standards to the digital
environment battin said will effectively lead to forfeiture of the
benefits of digital technology to research and scholarship

under the second rubric the politics of reproduction battin reiterated
an oftmade argument concerning the electronic library namely that it
is more difficult to transform than to create and nowhere is that belief
expressed more dramatically than in the conversion of brittle books to
new media  preserving information published in electronic media involves
making sure the information remains accessible and that digital
information is not lost through reproduction  in the analog world of
photocopies and microfilm the issue of fidelity to the original becomes
paramount as do issues of whose fidelity and whose original

battin elaborated these arguments with a few examples from a recent study
conducted by the cpa on the problems of preserving text and image 
discussions with scholars librarians and curators in a variety of
disciplines dependent on text and image generated a variety of concerns
for example   copy what is not what the technology is capable of 
this is very important for the history of ideas  scholars wish to know
what the author saw and worked from  and make available at the
workstation the opportunity to erase all the defects and enhance the
presentation   the fidelity of reproductionwhat is good enough what
can we afford and the difference it makesissues of subjective versus
objective resolution   the differences between primary and secondary
users  restricting the definition of primary user to the one in whose
discipline the material has been published runs one headlong into the
reality that these printed books have had a host of other users from a
host of other disciplines who not only were looking for very different
things but who also shared values very different from those of the
primary user   the relationship of the standard of reproduction to new
capabilities of scholarshipthe browsing standard versus an archival
standard  how good must the archival standard be  can a distinction be
drawn between potential users in setting standards for reproduction 
archival storage use copies browsing copiesought an attempt to set
standards even be made   finally costs  how much are we prepared to
pay to capture absolute fidelity  what are the tradeoffs between vastly
enhanced access degrees of fidelity and costs

these standards battin concluded serve to complicate further the
reproduction process and add to the long list of technical standards
that are necessary to ensure widespread access  ways to articulate and
analyze the costs that are attached to the different levels of standards
must be found

given the chaos concerning standards which promises to linger for the
foreseeable future battin urged adoption of the following general
principles

      strive to understand the changing information requirements of
     scholarly disciplines as more and more technology is integrated into
     the process of research and scholarly communication in order to meet
     future scholarly needs not to build for the past  capture
     deteriorating information at the highest affordable resolution even
     though the dissemination and display technologies will lag

      develop cooperative mechanisms to foster agreement on protocols
     for document structure and other interchange mechanisms necessary
     for widespread dissemination and use before official standards are
     set

      accept that in a transition period de facto standards will have
     to be developed

      capture information in a way that keeps all options open and
     provides for total convertibility  ocr scanning of microfilm
     producing microfilm from scanned documents etc

      work closely with the generators of information and the builders
     of networks and databases to ensure that continuing accessibility is
     a primary concern from the beginning

      piggyback on standards under development for the broad market and
     avoid libraryspecific standards work with the vendors in order to
     take advantage of that which is being standardized for the rest of
     the world

      concentrate efforts on managing permanence in the digital world
     rather than perfecting the longevity of a particular medium

                                 


discussion  additional comments on tiff 


during the brief discussion period that followed battins presentation
baronas explained that tiff was not developed in collaboration with or
under the auspices of aiim  tiff is a company product not a standard
is owned by two corporations and is always changing  baronas also
observed that ansiaiim ms a bilevel image file transfer format that
allows unlike systems to exchange images is compatible with tiff as well
as with decs architecture and ibms modcaioca

                                 


hooton  several questions to be considered in discussing text conversion



hooton introduced the final topic text conversion by noting that it is
becoming an increasingly important part of the imaging business  many
people now realize that it enhances their system to be able to have more
and more character data as part of their imaging system  re the issue of
ocr versus rekeying hooton posed several questions  how does one get
text into computerreadable form  does one use automated processes 
does one attempt to eliminate the use of operators where possible 
standards for accuracy he said are extremely important  it makes a
major difference in cost and time whether one sets as a standard 
percent acceptance or  percent  he mentioned outsourcing as a
possibility for converting text  finally what one does with the image
to prepare it for the recognition process is also important he said
because such preparation changes how recognition is viewed as well as
facilitates recognition itself

                                 


lesk  roles of participants in core  data flow  the scanning process 
the image interface  results of experiments involving the use of
electronic resources and traditional paper copies  testing the issue of
serendipity  conclusions 


michael lesk executive director computer science research bell
communications research inc bellcore discussed the chemical online
retrieval experiment core a cooperative project involving cornell
university oclc bellcore and the american chemical society acs

lesk spoke on  how the scanning was performed including the unusual
feature of page segmentation and  the use made of the text and the
image in experiments

working with the chemistry journals because acs has been saving its
typesetting tapes since the mids and thus has a significant backrun
of the most important chemistry journals in the united states core is
attempting to create an automated chemical library  approximately a
quarter of the pages by square inch are made up of images of
quasipictorial material dealing with the graphic components of the
pages is extremely important  lesk described the roles of participants
in core   acs provides copyright permission journals on paper
journals on microfilm and some of the definitions of the files  at
bellcore lesk chiefly performs the data preparation while dennis egan
performs experiments on the users of chemical abstracts and supplies the
indexing and numerous magnetic tapes   cornell provides the site of the
experiment  oclc develops retrieval software and other user interfaces
various manufacturers and publishers have furnished other help

concerning data flow bellcore receives microfilm and paper from acs the
microfilm is scanned by outside vendors while the paper is scanned
inhouse on an improvision scanner twenty pages per minute at  dpi
which provides sufficient quality for all practical uses  lesk would
prefer to have more gray level because one of the acs journals prints on
some colored pages which creates a problem

bellcore performs all this scanning creates a pageimage file and also
selects from the pages the graphics to mix with the text file which is
discussed later in the workshop  the user is always searching the ascii
file but she or he may see a display based on the ascii or a display
based on the images

lesk illustrated how the program performs page analysis and the image
interface  the user types several words is presented with a list
usually of the titles of articles contained in an issuethat derives
from the ascii clicks on an icon and receives an image that mirrors an
acs page  lesk also illustrated an alternative interface based on text
on the ascii the socalled superbook interface from bellcore

lesk next presented the results of an experiment conducted by dennis egan
and involving thirtysix students at cornell one third of them
undergraduate chemistry majors one third senior undergraduate chemistry
majors and one third graduate chemistry students  a third of them
received the paper journals the traditional paper copies and chemical
abstracts on paper  a third received image displays of the pictures of
the pages and a third received the text display with popup graphics

the students were given several questions made up by some chemistry
professors  the questions fell into five classes ranging from very easy
to very difficult and included questions designed to simulate browsing
as well as a traditional information retrievaltype task

lesk furnished the following results  in the straightforward question
searchthe question being what is the phosphorus oxygen bond distance
and hydroxy phosphatethe students were told that they could take
fifteen minutes and then if they wished give up  the students with
paper took more than fifteen minutes on average and yet most of them
gave up  the students with either electronic format text or image
received good scores in reasonable time hardly ever had to give up and
usually found the right answer

in the browsing study the students were given a list of eight topics
told to imagine that an issue of the journal of the american chemical
society had just appeared on their desks and were also told to flip
through it and to find topics mentioned in the issue  the average scores
were about the same  the students were told to answer yes or no about
whether or not particular topics appeared  the errors however were
quite different  the students with paper rarely said that something
appeared when it had not  but they often failed to find something
actually mentioned in the issue  the computer people found numerous
things but they also frequently said that a topic was mentioned when it
was not  the reason of course was that they were performing word
searches  they were finding that words were mentioned and they were
concluding that they had accomplished their task

this question also contained a trick to test the issue of serendipity 
the students were given another list of eight topics and instructed
without taking a second look at the journal to recall how many of this
new list of eight topics were in this particular issue  this was an
attempt to see if they performed better at remembering what they were not
looking for  they all performed about the same paper or electronics
about  percent accurate  in short lesk said people were not very
good when it came to serendipity but they were no worse at it with
computers than they were with paper

lesk gave a parenthetical illustration of the learning curve of students
who used superbook

the students using the electronic systems started off worse than the ones
using print but by the third of the three sessions in the series had
caught up to print  as one might expect electronics provide a much
better means of finding what one wants to read reading speeds once the
object of the search has been found are about the same

almost none of the students could perform the hard taskthe analogous
transformation  it would require the expertise of organic chemists to
complete  but an interesting result was that the students using the text
search performed terribly while those using the image system did best
that the text search system is driven by text offers the explanation
everything is focused on the text to see the pictures one must press
on an icon  many students found the right article containing the answer
to the question but they did not click on the icon to bring up the right
figure and see it  they did not know that they had found the right place
and thus got it wrong

the short answer demonstrated by this experiment was that in the event
one does not know what to read one needs the electronic systems the
electronic systems hold no advantage at the moment if one knows what to
read but neither do they impose a penalty

lesk concluded by commenting that on one hand the image system was easy
to use  on the other hand the text display system which represented
twenty manyears of work in programming and polishing was not winning
because the text was not being read just searched  the much easier
system is highly competitive as well as remarkably effective for the
actual chemists

                                 


erway  most challenging aspect of working on am  assumptions guiding
ams approach  testing different types of service bureaus  ams
requirement for  percent accuracy  requirements for textcoding 
additional factors influencing ams approach to coding  results of ams
experience with rekeying  other problems in dealing with service bureaus
 quality control the most timeconsuming aspect of contracting out
conversion  longterm outlook uncertain 


to ricky erway associate coordinator american memory library of
congress the constant variety of conversion projects taking place
simultaneously represented perhaps the most challenging aspect of working
on am  thus the challenge was not to find a solution for text
conversion but a tool kit of solutions to apply to lcs varied
collections that need to be converted  erway limited her remarks to the
process of converting text to machinereadable form and the variety of
lcs text collections for example bound volumes microfilm and
handwritten manuscripts

two assumptions have guided ams approach erway said   a desire not
to perform the conversion inhouse  because of the variety of formats and
types of texts to capitalize the equipment and have the talents and
skills to operate them at lc would be extremely expensive  further the
natural inclination to upgrade to newer and better equipment each year
made it reasonable for am to focus on what it did best and seek external
conversion services  using service bureaus also allowed am to have
several types of operations take place at the same time   am was not a
technology project but an effort to improve access to library
collections  hence whether text was converted using ocr or rekeying
mattered little to am  what mattered were cost and accuracy of results

am considered different types of service bureaus and selected three to
perform several small tests in order to acquire a sense of the field 
the sample collections with which they worked included handwritten
correspondence typewritten manuscripts from the s and
eighteenthcentury printed broadsides on microfilm  on none of these
samples was ocr performed they were all rekeyed  am had several special
requirements for the three service bureaus it had engaged  for instance
any errors in the original text were to be retained  working from bound
volumes or anything that could not be sheetfed also constituted a factor
eliminating companies that would have performed ocr

am requires  percent accuracy which though it sounds high often
means one or two errors per page  the initial batch of test samples
contained several handwritten materials for which am did not require
textcoding  the results erway reported were in all cases fairly
comparable  for the most part all three service bureaus achieved 
percent accuracy  am was satisfied with the work but surprised at the cost

as am began converting whole collections it retained the requirement for
 percent accuracy and added requirements for textcoding  am needed
to begin performing work more than three years ago before lc requirements
for sgml applications had been established  since ams goal was simply
to retain any of the intellectual content represented by the formatting
of the document which would be lost if one performed a straight ascii
conversion am used sgmllike codes  these codes resembled sgml tags
but were used without the benefit of documenttype definitions  am found
that many service bureaus were not yet sgmlproficient

additional factors influencing the approach am took with respect to
coding included   the inability of any known microcomputerbased
userretrieval software to take advantage of sgml coding and  the
multiple inconsistencies in format of the older documents which
confirmed am in its desire not to attempt to force the different formats
to conform to a single documenttype definition dtd and thus create the
need for a separate dtd for each document 

the five text collections that am has converted or is in the process of
converting include a collection of eighteenthcentury broadsides a
collection of pamphlets two typescript document collections and a
collection of  books

erway next reviewed the results of ams experience with rekeying noting
again that because the bulk of ams materials are historical the quality
of the text often does not lend itself to ocr  while nonenglish
speakers are less likely to guess or elaborate or correct typos in the
original text they are also less able to infer what we would they also
are nearly incapable of converting handwritten text  another
disadvantage of working with overseas keyers is that they are much less
likely to telephone with questions especially on the coding with the
result that they develop their own rules as they encounter new
situations

government contracting procedures and time frames posed a major challenge
to performing the conversion  many service bureaus are not accustomed to
retaining the image even if they perform ocr  thus questions of image
format and storage media were somewhat novel to many of them  erway also
remarked other problems in dealing with service bureaus for example
their inability to perform text conversion from the kind of microfilm
that lc uses for preservation purposes

but quality control in erways experience was the most timeconsuming
aspect of contracting out conversion  am has been attempting to perform
a percent quality review looking at either every tenth document or
every tenth page to make certain that the service bureaus are maintaining
 percent accuracy  but even if they are complying with the
requirement for accuracy finding errors produces a desire to correct
them and in turn to clean up the whole collection which defeats the
purpose to some extent  even a double entry requires a
characterbycharacter comparison to the original to meet the accuracy
requirement  lc is not accustomed to publish imperfect texts which
makes attempting to deal with the industry standard an emotionally
fraught issue for am  as was mentioned in the previous days discussion
going from  to  percent accuracy usually doubles costs and
means a third keying or another complete runthrough of the text

although am has learned much from its experiences with various collections
and various service bureaus erway concluded pessimistically that no
breakthrough has been achieved   incremental improvements have occurred
in some of the ocr technology some of the processes and some of the
standards acceptances which though they may lead to somewhat lower costs
do not offer much encouragement to many people who are anxiously awaiting
the day that the entire contents of lc are available online

                                 


zidar  several answers to why one attempts to perform fulltext
conversion  per page cost of performing ocr  typical problems
encountered during editing  editing poor copy ocr vs rekeying 


judith zidar coordinator national agricultural text digitizing program
natdp national agricultural library nal offered several answers to
the question of why one attempts to perform fulltext conversion  
text in an image can be read by a human but not by a computer so of
course it is not searchable and there is not much one can do with it  
some material simply requires wordlevel access  for instance the legal
profession insists on fulltext access to its material with taxonomic or
geographic material which entails numerous names one virtually requires
wordlevel access   full text permits rapid browsing and searching
something that cannot be achieved in an image with todays technology 
 text stored as ascii and delivered in ascii is standardized and highly
portable   people just want fulltext searching even those who do not
know how to do it  nal for the most part is performing ocr at an
actual cost per averagesize page of approximately   nal scans the
page to create the electronic image and passes it through the ocr device

zidar next rehearsed several typical problems encountered during editing 
praising the celerity of her student workers zidar observed that editing
requires approximately five to ten minutes per page assuming that there
are no large tables to audit  confusion among the three characters i  
and l constitutes perhaps the most common problem encountered  zeroes
and  os also are  frequently confused  double ms create a particular
problem even on clean pages  they are so wide in most fonts that they
touch and the system simply cannot tell where one letter ends and the
other begins  complex page formats occasionally fail to columnate
properly which entails rescanning as though one were working with a
single column entering the ascii and decolumnating for better
searching  with proportionally spaced text ocr can have difficulty
discerning what is a space and what are merely spaces between letters as
opposed to spaces between words and therefore will merge text or break
up words where it should not

zidar said that it can often take longer to edit a poorcopy ocr than to
key it from scratch  nal has also experimented with partial editing of
text whereby project workers go into and clean up the format removing
stray characters but not running a spellcheck  nal corrects typos in
the title and authors names which provides a foothold for searching and
browsing  even extremely poorquality ocr eg percent accuracy
can still be searched because numerous words are correct while the
important words are probably repeated often enough that they are likely
to be found correct somewhere  librarians however cannot tolerate this
situation though end users seem more willing to use this text for
searching provided that nal indicates that it is unedited  zidar
concluded that rekeying of text may be the best route to take in spite
of numerous problems with quality control and cost

                                 


discussion  modifying an image before performing ocr  nals costs per
page ams costs per page and experience with federal prison industries 
elements comprising natdps costs per page  ocr and structured markup 
distinction between the structure of a document and its representation
when put on the screen or printed 


hooton prefaced the lengthy discussion that followed with several
comments about modifying an image before one reaches the point of
performing ocr  for example in regard to an application containing a
significant amount of redundant data such as formtype data numerous
companies today are working on various kinds of form renewal prior to
going through a recognition process by using dropout colors  thus
acquiring access to form design or using electronic means are worth
considering  hooton also noted that conversion usually makes or breaks
ones imaging system  it is extremely important extremely costly in
terms of either capital investment or service and determines the quality
of the remainder of ones system because it determines the character of
the raw material used by the system

concerning the four projects undertaken by nal two inside and two
performed by outside contractors zidar revealed that an inhouse service
bureau executed the first at a cost between  and  per page for
everything including building of the database  the project undertaken
by the consultative group on international agricultural research cgiar
cost approximately  per page for the conversion plus some expenses
for the software and building of the database  the acid rain projecta
twodisk set produced by the university of vermont consisting of
canadian publications on acid raincost  per page for everything
including keying of the text which was double keyed scanning of the
images and building of the database  the inhouse project offered
considerable ease of convenience and greater control of the process  on
the other hand the service bureaus know their job and perform it
expeditiously because they have more people

as a useful comparison erway revealed ams costs as follows  
cents to  cents per thousand characters with an average page
containing  characters  requirements for coding and imaging
increase the costs  thus conversion of the text including the coding
costs approximately  per page  this figure does not include the
imaging and databasebuilding included in the nal costs  am also
enjoyed a happy experience with federal prison industries which
precluded the necessity of going through the requestforproposal process
to award a contract because it is another government agency  the
prisoners performed ams rekeying just as well as other service bureaus
and proved handy as well  am shipped them the books which they would
photocopy on a bookedge scanner  they would perform the markup on
photocopies return the books as soon as they were done with them
perform the keying and return the material to am on worm disks

zidar detailed the elements that constitute the previously noted cost of
approximately  per page  most significant is the editing correction
of errors and spellcheckings which though they may sound easy to
perform require in fact a great deal of time  reformatting text also
takes a while but a significant amount of nals expenses are for equipment
which was extremely expensive when purchased because it was one of the few
systems on the market  the costs of equipment are being amortized over
five years but are still quite high nearly  per month

hockey raised a general question concerning ocr and the amount of editing
required substantial in her experience to generate the kind of
structured markup necessary for manipulating the text on the computer or
loading it into any retrieval system  she wondered if the speakers could
extend the previous question about the costbenefit of adding or exerting
structured markup  erway noted that several ocr systems retain italics
bolding and other spatial formatting  while the material may not be in
the format desired these systems possess the ability to remove the
original materials quickly from the hands of the people performing the
conversion as well as to retain that information so that users can work
with it  hockey rejoined that the current thinking on markup is that one
should not say that something is italic or bold so much as why it is that
way  to be sure one needs to know that something was italicized but
how can one get from one to the other  one can map from the structure to
the typographic representation

fleischhauer suggested that given the  million items the library
holds it may not be possible for lc to do more than report that a thing
was in italics as opposed to why it was italics although that may be
desirable in some contexts  promising to talk a bit during the afternoon
session about several experiments oclc performed on automatic recognition
of document elements and which they hoped to extend weibel said that in
fact one can recognize the major elements of a document with a fairly
high degree of reliability at least as good as ocr  stevens drew a
useful distinction between standard generalized markup ie defining
for a documenttype definition the structure of the document and what
he termed a style sheet which had to do with italics bolding and other
forms of emphasis  thus two different components are at work one being
the structure of the document itself its logic and the other being its
representation when it is put on the screen or printed

                                 

session v  approaches to preparing electronic texts


hockey  text in ascii and the representation of electronic text versus
an image  the need to look at ways of using markup to assist retrieval 
the need for an encoding format that will be reusable and multifunctional


susan hockey director center for electronic texts in the humanities
ceth rutgers and princeton universities announced that one talk
weibels was moved into this session from the morning and that david
packard was unable to attend  the session would attempt to focus more on
what one can do with a text in ascii and the representation of electronic
text rather than just an image what one can do with a computer that
cannot be done with a book or an image  it would be argued that one can
do much more than just read a text and from that starting point one can
use markup and methods of preparing the text to take full advantage of
the capability of the computer  that would lead to a discussion of what
the european community calls reusability what may better be termed
durability that is how to prepare or make a text that will last a long
time and that can be used for as many applications as possible which
would lead to issues of improving intellectual access

hockey urged the need to look at ways of using markup to facilitate retrieval
not just for referencing or to help locate an item that is retrieved but also to put markup tags in
a text to help retrieve the thing sought either with linguistic tagging or
interpretation  hockey also argued that little advancement had occurred in
the software tools currently available for retrieving and searching text
she pressed the desideratum of going beyond boolean searches and performing
more sophisticated searching which the insertion of more markup in the text
would facilitate  thinking about electronic texts as opposed to images means
considering material that will never appear in print form or print will not
be its primary form that is material which only appears in electronic form
hockey alluded to the history and the need for markup and tagging and
electronic text which was developed through the use of computers in the
humanities as michelson had observed father busa had started in 
to prepare the firstever text on the computer

hockey remarked several large projects particularly in europe for the
compilation of dictionaries language studies and language analysis in
which people have built up archives of text and have begun to recognize
the need for an encoding format that will be reusable and multifunctional
that can be used not just to print the text which may be assumed to be a
byproduct of what one wants to do but to structure it inside the computer
so that it can be searched built into a hypertext system etc

                                 


weibel  oclcs approach to preparing electronic text  retroconversion
keying of texts more automated ways of developing data  project adapt
and the core project  intelligent character recognition does not exist 
advantages of sgml  data should be free of procedural markup
descriptive markup strongly advocated  oclcs interface illustrated 
storage requirements and costs for putting a lot of information on line 


stuart weibel senior research scientist online computer library center
inc oclc described oclcs approach to preparing electronic text  he
argued that the electronic world into which we are moving must
accommodate not only the future but the past as well and to some degree
even the present  thus starting out at one end with retroconversion and
keying of texts one would like to move toward much more automated ways
of developing data

for example project adapt had to do with automatically converting
document images into a structured document database with ocr text as
indexing and also a little bit of automatic formatting and tagging of
that text  the core project hosted by cornell university bellcore
oclc the american chemical society and chemical abstracts constitutes
weibels principal concern at the moment  this project is an example of
converting text for which one already has a machinereadable version into
a format more suitable for electronic delivery and database searching 
since michael lesk had previously described core weibel would say
little concerning it  borrowing a chemical phrase de novo synthesis
weibel cited the online journal of current clinical trials as an example
of de novo electronic publishing that is a form in which the primary
form of the information is electronic

project adapt then which oclc completed a couple of years ago and in
fact is about to resume is a model in which one takes page images either
in paper or microfilm and converts them automatically to a searchable
electronic database either online or local  the operating assumption
is that accepting some blemishes in the data especially for
retroconversion of materials will make it possible to accomplish more 
not enough money is available to support perfect conversion

weibel related several steps taken to perform image preprocessing
processing on the image before performing optical character
recognition as well as image postprocessing  he denied the existence
of intelligent character recognition and asserted that what is wanted is
page recognition which is a long way off  oclc has experimented with
merging of multiple optical character recognition systems that will
reduce errors from an unacceptable rate of  characters out of every
l to an unacceptable rate of  characters out of every l but it
is not good enough  it will never be perfect

concerning the core project weibel observed that bellcore is taking the
topography files extracting the page images and converting those
topography files to sgml markup  lesk hands that data off to oclc which
builds that data into a newton database the same system that underlies
the online system in virtually all of the reference products at oclc 
the longterm goal is to make the systems interoperable so that not just
bellcores system and oclcs system can access this data but other
systems can as well and the key to that is the z common command
language and the fulltext extension  z is fine for marc records
but is not enough to do it for full text that is make full texts
interoperable

weibel next outlined the critical role of sgml for a variety of purposes
for example as noted by hockey in the world of extremely large
databases using highly structured data to perform field searches 
weibel argued that by building the structure of the data in ie the
structure of the data originally on a printed page it becomes easy to
look at a journal article even if one cannot read the characters and know
where the title or author is or what the sections of that document would be
oclc wants to make that structure explicit in the database because it will
be important for retrieval purposes

the second big advantage of sgml is that it gives one the ability to
build structure into the database that can be used for display purposes
without contaminating the data with instructions about how to format
things  the distinction lies between procedural markup which tells one
where to put dots on the page and descriptive markup which describes
the elements of a document

weibel believes that there should be no procedural markup in the data at
all that the data should be completely unsullied by information about
italics or boldness  that should be left up to the display device
whether that display device is a page printer or a screen display device 
by keeping ones database free of that kind of contamination one can
make decisions down the road for example reorganize the data in ways
that are not cramped by builtin notions of what should be italic and
what should be bold  weibel strongly advocated descriptive markup  as
an example he illustrated the index structure in the core data  with
subsequent illustrated examples of markup weibel acknowledged the common
complaint that sgml is hard to read in its native form although markup
decreases considerably once one gets into the body  without the markup
however one would not have the structure in the data  one can pass
markup through a latex processor and convert it relatively easily to a
printed version of the document

weibel next illustrated an extremely cluttered screen dump of oclcs
system in order to show as much as possible the inherent capability on
the screen  he noted parenthetically that he had become a supporter of
xwindows as a result of the progress of the core project  weibel also
illustrated the two major parts of the interface  l a control box that
allows one to generate lists of items which resembles a small table of
contents based on key words one wishes to search and  a document
viewer which is a separate process in and of itself  he demonstrated
how to follow links through the electronic database simply by selecting
the appropriate button and bringing them up  he also noted problems that
remain to be accommodated in the interface eg as pointed out by lesk
what happens when users do not click on the icon for the figure

given the constraints of time weibel omitted a large number of ancillary
items in order to say a few words concerning storage requirements and
what will be required to put a lot of things on line  since it is
extremely expensive to reconvert all of this data especially if it is
just in paper form and even if it is in electronic form in typesetting
tapes he advocated building journals electronically from the start  in
that case if one only has text graphics and indexing which is all that
one needs with de novo electronic publishing because there is no need to
go back and look at bitmaps of pages one can get  journals of
full text or almost  million pages per year  these pages can be put in
approximately  gigabytes of storage which is not all that much
weibel said  for twenty years something less than three terabytes would
be required  weibel calculated the costs of storing this information as
follows  if a gigabyte costs approximately  then a terabyte costs
approximately  million to buy in terms of hardware  one also needs a
building to put it in and a staff like oclc to handle that information 
so to support a terabyte multiply by five which gives  million per
year for a supported terabyte of data

                                 


discussion  tapes saved by acs are the typography files originally
supporting publication of the journal  cost of building tagged text into
the database 


during the questionandanswer period that followed weibels
presentation these clarifications emerged  the tapes saved by the
american chemical society are the typography files that originally
supported the publication of the journal  although they are not tagged
in sgml they are tagged in very fine detail  every single sentence is
marked all the registry numbers all the publications issues dates and
volumes  no cost figures on tagging material on a permegabyte basis
were available  because acss typesetting system runs from tagged text
there is no extra cost per article  it was unknown what it costs acs to
keyboard the tagged text rather than just keyboard the text in the
cheapest process  in other words since one intends to publish things
and will need to build tagged text into a typography system in any case
if one does that in such a way that it can drive not only typography but
an electronic system which is what acs intends to domove to sgml
publishing the marginal cost is zero  the marginal cost represents the
cost of building tagged text into the database which is small

                                 


sperbergmcqueen  distinction between texts and computers  implications
of recognizing that all representation is encoding  dealing with
complicated representations of text entails the need for a grammar of
documents  variety of forms of formal grammars  text as a bitmapped
image does not represent a serious attempt to represent text in
electronic form  sgml the tei documenttype declarations and the
reusability and longevity of data  tei conformance explicitly allows
extension or modification of the tei tag set  administrative background
of the tei  several design goals for the tei tag set  an absolutely
fixed requirement of the tei guidelines  challenges the tei has
attempted to face  good texts not beyond economic feasibility  the
issue of reproducibility or processability  the issue of mages as
simulacra for the text redux  ones model of text determines what ones
software can do with a text and has economic consequences 


prior to speaking about sgml and markup michael sperbergmcqueen editor
text encoding initiative tei university of illinoischicago first drew
a distinction between texts and computers  texts are abstract cultural
and linguistic objects while computers are complicated physical devices
he said  abstract objects cannot be placed inside physical devices with
computers one can only represent text and act upon those representations

the recognition that all representation is encoding sperbergmcqueen
argued leads to the recognition of two things   the topic description
for this session is slightly misleading because there can be no discussion
of pros and cons of textcoding unless what one means is pros and cons of
working with text with computers   no text can be represented in a
computer without some sort of encoding images are one way of encoding text
ascii is another sgml yet another  there is no encoding without some
information loss that is there is no perfect reproduction of a text that
allows one to do away with the original  thus the question becomes
what is the most useful representation of text for a serious work
this depends on what kind of serious work one is talking about

the projects demonstrated the previous day all involved highly complex
information and fairly complex manipulation of the textual material
in order to use that complicated information one has to calculate it
slowly or manually and store the result  it needs to be stored therefore
as part of ones representation of the text  thus one needs to store the
structure in the text  to deal with complicated representations of text
one needs somehow to control the complexity of the representation of a text
that means one needs a way of finding out whether a document and an
electronic representation of a document is legal or not and that
means one needs a grammar of documents

sperbergmcqueen discussed the variety of forms of formal grammars
implicit and explicit as applied to text and their capabilities  he
argued that these grammars correspond to different models of text that
different developers have  for example one implicit model of the text
is that there is no internal structure but just one thing after another
a few characters and then perhaps a starttitle command and then a few
more characters and an endtitle command  sperbergmcqueen also
distinguished several kinds of text that have a sort of hierarchical
structure that is not very well defined which typically corresponds
to grammars that are not very well defined as well as hierarchies that
are very well defined eg the thesaurus linguae graecae and extremely
complicated things such as sgml which handle strictly hierarchical data
very nicely

sperbergmcqueen conceded that one other model not illustrated on his two
displays was the model of text as a bitmapped image an image of a page
and confessed to having been converted to a limited extent by the
workshop to the view that electronic images constitute a promising
probably superior alternative to microfilming  but he was not convinced
that electronic images represent a serious attempt to represent text in
electronic form  many of their problems stem from the fact that they are
not direct attempts to represent the text but attempts to represent the
page thus making them representations of representations

in this situation of increasingly complicated textual information and the
need to control that complexity in a useful way which begs the question
of the need for good textual grammars one has the introduction of sgml 
with sgml one can develop specific documenttype declarations
for specific text types or as with the tei attempts to generate
general documenttype declarations that can handle all sorts of text
the tei is an attempt to develop formats for text representation that
will ensure the kind of reusability and longevity of data discussed earlier
it offers a way to stay alive in the state of permanent technological
revolution

it has been a continuing challenge in the tei to create document grammars
that do some work in controlling the complexity of the textual object but
also allowing one to represent the real text that one will find 
fundamental to the notion of the tei is that tei conformance allows one
the ability to extend or modify the tei tag set so that it fits the text
that one is attempting to represent

sperbergmcqueen next outlined the administrative background of the tei 
the tei is an international project to develop and disseminate guidelines
for the encoding and interchange of machinereadable text  it is
sponsored by the association for computers in the humanities the
association for computational linguistics and the association for
literary and linguistic computing  representatives of numerous other
professional societies sit on its advisory board  the tei has a number
of affiliated projects that have provided assistance by testing drafts of
the guidelines

among the design goals for the tei tag set the scheme first of all must
meet the needs of research because the tei came out of the research
community which did not feel adequately served by existing tag sets 
the tag set must be extensive as well as compatible with existing and
emerging standards  in  version  of the guidelines was released
sperbergmcqueen illustrated their contents

sperbergmcqueen noted that one problem besetting electronic text has
been the lack of adequate internal or external documentation for many
existing electronic texts  the tei guidelines as currently formulated
contain few fixed requirements but one of them is this  there must
always be a document header an infile sgml tag that provides
 a bibliographic description of the electronic object one is talking
about that is who included it when what for and under which title
and  the copy text from which it was derived if any  if there was
no copy text or if the copy text is unknown then one states as much
version  of the guidelines was scheduled to be completed in fall 
and a revised third version is to be presented to the tei advisory board
for its endorsement this coming winter  the tei itself exists to provide
a markup language not a markedup text

among the challenges the tei has attempted to face is the need for a
markup language that will work for existing projects that is handle the
level of markup that people are using now to tag only chapter section
and paragraph divisions and not much else  at the same time such a
language also will be able to scale up gracefully to handle the highly
detailed markup which many people foresee as the future destination of
much electronic text and which is not the future destination but the
present home of numerous electronic texts in specialized areas

sperbergmcqueen dismissed the lowestcommondenominator approach as
unable to support the kind of applications that draw people who have
never been in the public library regularly before and make them come
back  he advocated more interesting text and more intelligent text 
asserting that it is not beyond economic feasibility to have good texts
sperbergmcqueen noted that the tei guidelines listing odd tags
contains tags that one is expected to enter every time the relevant
textual feature occurs  it contains all the tags that people need now
and it is not expected that everyone will tag things in the same way

the question of how people will tag the text is in large part a function
of their reaction to what sperbergmcqueen termed the issue of
reproducibility  what one needs to be able to reproduce are the things
one wants to work with  perhaps a more useful concept than that of
reproducibility or recoverability is that of processability that is
what can one get from an electronic text without reading it again
in the original  he illustrated this contention with a page from
jan comeniuss bilingual introduction to latin

sperbergmcqueen returned at length to the issue of images as simulacra
for the text in order to reiterate his belief that in the long run more
than images of pages of particular editions of the text are needed
because just as secondgeneration photocopies and secondgeneration
microfilm degenerate so secondgeneration representations tend to
degenerate and one tends to overstress some relatively trivial aspects
of the text such as its layout on the page which is not always
significant despite what the text critics might say and slight other
pieces of information such as the very important lexical ties between the
english and latin versions of comeniuss bilingual text for example 
moreover in many crucial respects it is easy to fool oneself concerning
what a scanned image of the text will accomplish  for example in order
to study the transmission of texts information concerning the text
carrier is necessary which scanned images simply do not always handle 
further even the highquality materials being produced at cornell use
much of the information that one would need if studying those books as
physical objects  it is a choice that has been made  it is an arguably
justifiable choice but one does not know what color those pen strokes in
the margin are or whether there was a stain on the page because it has
been filtered out  one does not know whether there were rips in the page
because they do not show up and on a couple of the marginal marks one
loses half of the mark because the pen is very light and the scanner
failed to pick it up and so what is clearly a checkmark in the margin of
the original becomes a little scoop in the margin of the facsimile 
standard problems for facsimile editions not new to electronics but
also true of lightlens photography and are remarked here because it is
important that we not fool ourselves that even if we produce a very nice
image of this page with good contrast we are not replacing the
manuscript any more than microfilm has replaced the manuscript

the tei comes from the research community where its first allegiance
lies but it is not just an academic exercise  it has relevance far
beyond those who spend all of their time studying text because ones
model of text determines what ones software can do with a text  good
models lead to good software  bad models lead to bad software  that has
economic consequences and it is these economic consequences that have
led the european community to help support the tei and that will lead
sperbergmcqueen hoped some software vendors to realize that if they
provide software with a better model of the text they can make a killing

                                 


discussion  implications of different dtds and tag sets  oda versus sgml 


during the discussion that followed several additional points were made 
neither aap ie association of american publishers nor cals ie
computeraided acquisition and logistics support has a documenttype
definition for ancient greek drama although the tei will be able to
handle that  given this state of affairs and assuming that the
technicaljournal producers and the commercial vendors decide to use the
other two types then an institution like the library of congress which
might receive all of their publications would have to be able to handle
three different types of document definitions and tag sets and be able to
distinguish among them

office document architecture oda has some advantages that flow from its
tight focus on office documents and clear directions for implementation 
much of the oda standard is easier to read and clearer at first reading
than the sgml standard which is extremely general  what that means is
that if one wants to use graphics in tiff and oda one is stuck because
oda defines graphics formats while tiff does not whereas sgml says the
world is not waiting for this work group to create another graphics format
what is needed is an ability to use whatever graphics format one wants

the tei provides a socket that allows one to connect the sgml document to
the graphics  the notation that the graphics are in is clearly a choice
that one needs to make based on her or his environment and that is one
advantage  sgml is less megalomaniacal in attempting to define formats
for all kinds of information though more megalomaniacal in attempting to
cover all sorts of documents  the other advantage is that the model of
text represented by sgml is simply an order of magnitude richer and more
flexible than the model of text offered by oda  both offer hierarchical
structures but sgml recognizes that the hierarchical model of the text
that one is looking at may not have been in the minds of the designers
whereas oda does not

oda is not really aiming for the kind of document that the tei wants to
encompass  the tei can handle the kind of material oda has as well as a
significantly broader range of material  oda seems to be very much
focused on office documents which is what it started out being called
office document architecture

                                 


calaluca  textencoding from a publishers perspective 
responsibilities of a publisher  reproduction of mignes latin series
whole and complete with sgml tags based on perceived need and expected
use  particular decisions arising from the general decision to produce
and publish pld 


the final speaker in this session eric calaluca vice president
chadwyckhealey inc spoke from the perspective of a publisher re
textencoding rather than as one qualified to discuss methods of
encoding data and observed that the presenters sitting in the room
whether they had chosen to or not were acting as publishers  making
choices gathering data gathering information and making assessments 
calaluca offered the hardwon conviction that in publishing very large
text files such as pld one cannot avoid making personal judgments of
appropriateness and structure

in calalucas view encoding decisions stem from prior judgments  two
notions have become axioms for him in the consideration of future sources
for electronic publication   electronic text publishing is as personal
as any other kind of publishing and questions of if and how to encode
the data are simply a consequence of that prior decision   all
personal decisions are open to criticism which is unavoidable

calaluca rehearsed his role as a publisher or better as an intermediary
between what is viewed as a sound idea and the people who would make use
of it  finding the specialist to advise in this process is the core of
that function  the publisher must monitor and hug the fine line between
giving users what they want and suggesting what they might need  one
responsibility of a publisher is to represent the desires of scholars and
research librarians as opposed to bullheadedly forcing them into areas
they would not choose to enter

calaluca likened the questions being raised today about data structure
and standards to the decisions faced by the abbe migne himself during
production of the patrologia series in the midnineteenth century 
chadwyckhealeys decision to reproduce mignes latin series whole and
complete with sgml tags was also based upon a perceived need and an
expected use  in the same way that mignes work came to be far more than
a simple handbook for clerics pld is already far more than a database
for theologians  it is a bedrock source for the study of western
civilization calaluca asserted

in regard to the decision to produce and publish pld the editorial board
offered direct judgments on the question of appropriateness of these
texts for conversion their encoding and their distribution and
concluded that the best possible project was one that avoided overt
intrusions or exclusions in so important a resource  thus the general
decision to transmit the original collection as clearly as possible with
the widest possible avenues for use led to other decisions   to encode
the data or not sgml or not tei or not  again the expected user
community asserted the need for normative tagging structures of important
humanities texts and the tei seemed the most appropriate structure for
that purpose  research librarians who are trained to view the larger
impact of electronic text sources on  or  or  doctoral
disciplines loudly approved the decision to include tagging  they see
what is coming better than the specialist who is completely focused on
one edition of ambroses de anima and they also understand that the
potential uses exceed present expectations   what will be tagged and
what will not  once again the board realized that one must tag the
obvious  but in no way should one attempt to identify through encoding
schemes every single discrete area of a text that might someday be
searched  that was another decision  searching by a column number an
author a word a volume permitting combination searches and tagging
notations seemed logical choices as core elements   how does one make
the data available  tieing it to a cdrom edition creates limitations
but a magnetic tape file that is very large is accompanied by the
encoding specifications and that allows one to make local modifications
also allows one to incorporate any changes one may desire within the
bounds of private research though exporting tag files from a cdrom
could serve just as well  since no one on the board could possibly
anticipate each and every way in which a scholar might choose to mine
this data bank it was decided to satisfy the basics and make some
provisions for what might come   not to encode the database would rob
it of the interchangeability and portability these important texts should
accommodate  for calaluca the extensive options presented by fulltext
searching require care in text selection and strongly support encoding of
data to facilitate the widest possible search strategies  better
software can always be created but summoning the resources the people
and the energy to reconvert the text is another matter

pld is being encoded captured and distributed because to
chadwyckhealey and the board it offers the widest possible array of
future research applications that can be seen today  calaluca concluded
by urging the encoding of all important text sources in whatever way
seems most appropriate and durable at the time without blanching at the
thought that ones work may require emendation in the future  thus
chadwyckhealey produced a very large humanities text database before the
final release of the tei guidelines

                                 


discussion  creating texts with markup advocated  trends in encoding 
the tei and the issue of interchangeability of standards  a
misconception concerning the tei  implications for an institution like
lc in the event that a multiplicity of dtds develops  producing images
as a first step towards possible conversion to full text through
character recognition  the aap tag sets as a common starting point and
the need for caution 


hockey prefaced the discussion that followed with several comments in
favor of creating texts with markup and on trends in encoding  in the
future when many more texts are available for online searching real
problems in finding what is wanted will develop if one is faced with
millions of words of data  it therefore becomes important to consider
putting markup in texts to help searchers home in on the actual things
they wish to retrieve  various approaches to refining retrieval methods
toward this end include building on a computer version of a dictionary
and letting the computer look up words in it to obtain more information
about the semantic structure or semantic field of a word its grammatical
structure and syntactic structure

hockey commented on the present keen interest in the encoding world
in creating   machinereadable versions of dictionaries that can be
initially tagged in sgml which gives a structure to the dictionary entry
these entries can then be converted into a more rigid or otherwise
different database structure inside the computer which can be treated as
a dynamic tool for searching mechanisms  large bodies of text to study
the language  in order to incorporate more sophisticated mechanisms
more about how words behave needs to be known which can be learned in
part from information in dictionaries  however the last ten years have
seen much interest in studying the structure of printed dictionaries
converted into computerreadable form  the information one derives about
many words from those is only partial one or two definitions of the
common or the usual meaning of a word and then numerous definitions of
unusual usages  if the computer is using a dictionary to help retrieve
words in a text it needs much more information about the common usages
because those are the ones that occur over and over again  hence the
current interest in developing large bodies of text in computerreadable
form in order to study the language  several projects are engaged in
compiling for example  million words hockey described one with
which she was associated briefly at oxford university involving
compilation of  million words of british english  about  percent of
that will contain detailed linguistic tagging encoded in sgml it will
have word class taggings with words identified as nouns verbs
adjectives or other parts of speech  this tagging can then be used by
programs which will begin to learn a bit more about the structure of the
language and then can go to tag more text

hockey said that the more that is tagged accurately the more one can
refine the tagging process and thus the bigger body of text one can build
up with linguistic tagging incorporated into it  hence the more tagging
or annotation there is in the text the more one may begin to learn about
language and the more it will help accomplish more intelligent ocr  she
recommended the development of software tools that will help one begin to
understand more about a text which can then be applied to scanning
images of that text in that format and to using more intelligence to help
one interpret or understand the text

hockey posited the need to think about common methods of textencoding
for a long time to come because building these large bodies of text is
extremely expensive and will only be done once

in the more general discussion on approaches to encoding that followed
these points were made

besser identified the underlying problem with standards that all have to
struggle with in adopting a standard namely the tension between a very
highly defined standard that is very interchangeable but does not work
for everyone because something is lacking and a standard that is less
defined more open more adaptable but less interchangeable  contending
that the way in which people use sgml is not sufficiently defined besser
wondered  if people resist the tei because they think it is too defined
in certain things they do not fit into and  how progress with
interchangeability can be made without frightening people away

sperbergmcqueen replied that the published drafts of the tei had met
with surprisingly little objection on the grounds that they do not allow
one to handle x or y or z  particular concerns of the affiliated
projects have led in practice to discussions of how extensions are to
be made the primary concern of any project has to be how it can be
represented locally thus making interchange secondary  the tei has
received much criticism based on the notion that everything in it is
required or even recommended which as it happens is a misconception
from the beginning   because none of it is required and very little is
actually actively recommended for all cases except that one document
ones source

sperbergmcqueen agreed with besser about this tradeoff  all the
projects in a set of twenty teiconformant projects will not necessarily
tag the material in the same way  one result of the tei will be that the
easiest problems will be solvedthose dealing with the external form of
the information but the problem that is hardest in interchange is that
one is not encoding what another wants and vice versa  thus after
the adoption of a common notation the differences in the underlying
conceptions of what is interesting about texts become more visible
the success of a standard like the tei will lie in the ability of
the recipient of interchanged texts to use some of what it contains
and to add the information that was not encoded that one wants in a
layered way so that texts can be gradually enriched and one does not
have to put in everything all at once  hence having a wellbehaved
markup scheme is important

stevens followed up on the paradoxical analogy that besser alluded to in
the example of the marc records namely the formats that are the same
except that they are different  stevens drew a parallel between
documenttype definitions and marc records for books and serials and maps
where one has a tagging structure and there is a textinterchange 
stevens opined that the producers of the information will set the terms
for the standard ie develop documenttype definitions for the users
of their products creating a situation that will be problematical for
an institution like the library of congress which will have to deal with
the dtds in the event that a multiplicity of them develops  thus
numerous people are seeking a standard but cannot find the tag set that
will be acceptable to them and their clients  sperbergmcqueen agreed
with this view and said that the situation was in a way worse  attempting
to unify arbitrary dtds resembled attempting to unify a marc record with a
bibliographic record done according to the prussian instructions 
according to stevens this situation occurred very early in the process

waters recalled from early discussions on project open book the concern
of many people that merely by producing images pob was not really
enhancing intellectual access to the material  nevertheless not wishing
to overemphasize the opposition between imaging and full text waters
stated that pob views getting the images as a first step toward possibly
converting to full text through character recognition if the technology
is appropriate  waters also emphasized that encoding is involved even
with a set of images

sperbergmcqueen agreed with waters that one can create an sgml document
consisting wholly of images  at first sight organizing graphic images
with an sgml document may not seem to offer great advantages but the
advantages of the scheme waters described would be precisely that
ability to move into something that is more of a multimedia document
a combination of transcribed text and page images  weibel concurred in
this judgment offering evidence from project adapt where a page is
divided into text elements and graphic elements and in fact the text
elements are organized by columns and lines  these lines may be used as
the basis for distributing documents in a network environment  as one
develops software intelligent enough to recognize what those elements
are it makes sense to apply sgml to an image initially that may in
fact ultimately become more and more text either through ocr or edited
ocr or even just through keying  for waters the labor of composing the
document and saying this set of documents or this set of images belongs
to this document constitutes a significant investment

weibel also made the point that the aap tag sets while not excessively
prescriptive offer a common starting point they do not define the
structure of the documents though  they have some recommendations about
dtds one could use as examples but they do just suggest tag sets   for
example the core project attempts to use the aap markup as much as
possible but there are clearly areas where structure must be added 
that in no way contradicts the use of aap tag sets

sperbergmcqueen noted that the tei prepared a long working paper early
on about the aap tag set and what it lacked that the tei thought it
needed and a fairly long critique of the naming conventions which has
led to a very different style of naming in the tei  he stressed the
importance of the opposition between prescriptive markup the kind that a
publisher or anybody can do when producing documents de novo and
descriptive markup in which one has to take what the text carrier
provides  in these particular tag sets it is easy to overemphasize this
opposition because the aap tag set is extremely flexible  even if one
just used the dtds they allow almost anything to appear almost anywhere

                                 

session vi  copyright issues


peters  several cautions concerning copyright in an electronic
environment  review of copyright law in the united states  the notion
of the public good and the desirability of incentives to promote it 
what copyright protects  works not protected by copyright  the rights
of copyright holders  publishers concerns in todays electronic
environment  compulsory licenses  the price of copyright in a digital
medium and the need for cooperation  additional clarifications   rough
justice oftentimes the outcome in numerous copyright matters  copyright
in an electronic society  copyright law always only sets up the
boundaries anything can be changed by contract 


marybeth peters policy planning adviser to the register of copyrights
library of congress   made several general comments and then opened the
floor to discussion of subjects of interest to the audience

having attended several sessions in an effort to gain a sense of what
people did and where copyright would affect their lives peters expressed
the following cautions

      if one takes and converts materials and puts them in new forms
     then from a copyright point of view one is creating something and
     will receive some rights

      however if what one is converting already exists a question
     immediately arises about the status of the materials in question

      putting something in the public domain in the united states offers
     some freedom from anxiety but distributing it throughout the world
     on a network is another matter even if one has put it in the public
     domain in the united states  re foreign laws very frequently a
     work can be in the public domain in the united states but protected
     in other countries  thus one must consider all of the places a
     work may reach lest one unwittingly become liable to being faced
     with a suit for copyright infringement or at least a letter
     demanding discussion of what one is doing

peters reviewed copyright law in the united states  the us
constitution effectively states that congress has the power to enact
copyright laws for two purposes   to encourage the creation and
dissemination of intellectual works for the good of society as a whole
and significantly  to give creators and those who package and
disseminate materials the economic rewards that are due them

congress strives to strike a balance which at times can become an
emotional issue  the united states has never accepted the notion of the
natural right of an author so much as it has accepted the notion of the
public good and the desirability of incentives to promote it  this state
of affairs however has created strains on the international level and
is the reason for several of the differences in the laws that we have 
today the united states protects almost every kind of work that can be
called an expression of an author  the standard for gaining copyright
protection is simply originality  this is a low standard and means that
a work is not copied from something else as well as shows a certain
minimal amount of authorship  one can also acquire copyright protection
for making a new version of preexisting material provided it manifests
some spark of creativity

however copyright does not protect ideas methods systemsonly the way
that one expresses those things  nor does copyright protect anything
that is mechanical anything that does not involve choice or criteria
concerning whether or not one should do a thing  for example the
results of a process called declicking in which one mechanically removes
impure sounds from old recordings are not copyrightable  on the other
hand the choice to record a song digitally and to increase the sound of
violins or to bring up the tympani constitutes the results of conversion
that are copyrightable  moreover if a work is protected by copyright in
the united states one generally needs the permission of the copyright
owner to convert it  normally who will own the newthat is converted
material is a matter of contract  in the absence of a contract the
person who creates the new material is the author and owner  but people
do not generally think about the copyright implications until after the
fact  peters stressed the need when dealing with copyrighted works to
think about copyright in advance  ones bargaining power is much greater
up front than it is down the road

peters next discussed works not protected by copyright for example any
work done by a federal employee as part of his or her official duties is
in the public domain in the united states  the issue is not wholly free
of doubt concerning whether or not the work is in the public domain
outside the united states  other materials in the public domain include 
any works published more than seventyfive years ago and any work
published in the united states more than twentyeight years ago whose
copyright was not renewed  in talking about the new technology and
putting material in a digital form to send all over the world peters
cautioned one must keep in mind that while the rights may not be an
issue in the united states they may be in different parts of the world
where most countries previously employed a copyright term of the life of
the author plus fifty years

peters next reviewed the economics of copyright holding  simply
economic rights are the rights to control the reproduction of a work in
any form  they belong to the author or in the case of a work made for
hire the employer  the second right which is critical to conversion
is the right to change a work  the right to make new versions is perhaps
one of the most significant rights of authors particularly in an
electronic world  the third right is the right to publish the work and
the right to disseminate it something that everyone who deals in an
electronic medium needs to know  the basic rule is if a copy is sold
all rights of distribution are extinguished with the sale of that copy 
the key is that it must be sold  a number of companies overcome this
obstacle by leasing or renting their product  these companies argue that
if the material is rented or leased and not sold they control the uses
of a work  the fourth right and one very important in a digital world
is a right of public performance which means the right to show the work
sequentially  for example copyright owners control the showing of a
cdrom product in a public place such as a public library  the reverse
side of public performance is something called the right of public
display  moral rights also exist which at the federal level apply only
to very limited visual works of art but in theory may apply under
contract and other principles  moral rights may include the right of an
author to have his or her name on a work the right of attribution and
the right to object to distortion or mutilationthe right of integrity

the way copyright law is worded gives much latitude to activities such as
preservation to use of material for scholarly and research purposes when
the user does not make multiple copies and to the generation of
facsimile copies of unpublished works by libraries for themselves and
other libraries  but the law does not allow anyone to become the
distributor of the product for the entire world  in todays electronic
environment publishers are extremely concerned that the entire world is
networked and can obtain the information desired from a single copy in a
single library  hence if there is to be only one sale which publishers
may choose to live with they will obtain their money in other ways for
example from access and use  hence the development of site licenses
and other kinds of agreements to cover what publishers believe they
should be compensated for  any solution that the united states takes
today has to consider the international arena

noting that the united states is a member of the berne convention and
subscribes to its provisions peters described the permissions process 
she also defined compulsory licenses  a compulsory license of which the
united states has had a few builds into the law the right to use a work
subject to certain terms and conditions  in the international arena
however the ability to use compulsory licenses is extremely limited 
thus clearinghouses and other collectives comprise one option that has
succeeded in providing for use of a work  often overlooked when one
begins to use copyrighted material and put products together is how
expensive the permissions process and managing it is  according to
peters the price of copyright in a digital medium whatever solution is
worked out will include managing and assembling the database  she
strongly recommended that publishers and librarians or people with
various backgrounds cooperate to work out administratively feasible
systems in order to produce better results

in the lengthy questionandanswer period that followed peterss
presentation the following points emerged

      the copyright office maintains that anything mechanical and
     totally exhaustive probably is not protected  in the event that
     what an individual did in developing potentially copyrightable
     material is not understood the copyright office will ask about the
     creative choices the applicant chose to make or not to make  as a
     practical matter if one believes she or he has made enough of those
     choices that person has a right to assert a copyright and someone
     else must assert that the work is not copyrightable  the more
     mechanical the more automatic a thing is the less likely it is to
     be copyrightable

      nearly all photographs are deemed to be copyrightable but no one
     worries about them much because everyone is free to take the same
     image  thus a photographic copyright represents what is called a
     thin copyright  the photograph itself must be duplicated in
     order for copyright to be violated

      the copyright office takes the position that xrays are not
     copyrightable because they are mechanical  it  can be argued
     whether or not image enhancement in scanning can be protected  one
     must exercise care with material created with public funds and
     generally in the public domain  an article written by a federal
     employee if written as part of official duties is not
     copyrightable  however control over a scientific article written
     by a national institutes of health grantee ie someone who
     receives money from the us government depends on nih policy  if
     the government agency has no policy and that policy can be
     contained in its regulations the contract or the grant the
     author retains copyright  if a provision of the contract grant or
     regulation states that there will be no copyright then it does not
     exist  when a work is created copyright automatically comes into
     existence unless something exists that says it does not

      an enhanced electronic copy of a print copy of an older reference
     work in the public domain that does not contain copyrightable new
     material is a purely mechanical rendition of the original work and
     is not copyrightable

      usually when a work enters the public domain nothing can remove
     it  for example congress recently passed into law the concept of
     automatic renewal which means that copyright on any work published
     between l and l does not have to be renewed in order to
     receive a seventyfiveyear term  but any work not renewed before
      is in the public domain

      concerning whether or not the united states keeps track of when
     authors die nothing was ever done nor is anything being done at
     the moment by the copyright office

      software that drives a mechanical process is itself copyrightable 
     if one changes platforms the software itself has a copyright  the
     world intellectual property organization will hold a symposium 
     march through  april l at harvard university on digital
     technology and will study this entire issue  if one purchases a
     computer software package such as macpaint and creates something
     new one receives protection only for that which has been added

peters added that often in copyright matters rough justice is the
outcome for example in collective licensing ascap ie american
society of composers authors and publishers and bmi ie broadcast
music inc where it may seem that the big guys receive more than their
due  of course people ought not to copy a creative product without
paying for it there should be some compensation  but the truth of the
world and it is not a great truth is that the big guy gets played on
the radio more frequently than the little guy who has to do much more
until he becomes a big guy  that is true of every author every
composer everyone and unfortunately is part of life

copyright always originates with the author except in cases of works
made for hire  most software falls into this category  when an author
sends his article to a journal he has not relinquished copyright though
he retains the right to relinquish it  the author receives absolutely
everything  the less prominent the author the more leverage the
publisher will have in contract negotiations  in order to transfer the
rights the author must sign an agreement giving them away

in an electronic society it is important to be able to license a writer
and work out deals  with regard to use of a work it usually is much
easier when a publisher holds the rights  in an electronic era a real
problem arises when one is digitizing and making information available 
peters referred again to electronic licensing clearinghouses  copyright
ought to remain with the author but as one moves forward globally in the
electronic arena a middleman who can handle the various rights becomes
increasingly necessary

the notion of copyright law is that it resides with the individual but
in an online environment where a work can be adapted and tinkered with
by many individuals there is concern  if changes are authorized and
there is no agreement to the contrary the person who changes a work owns
the changes  to put it another way the person who acquires permission
to change a work technically will become the author and the owner unless
some agreement to the contrary has been made  it is typical for the
original publisher to try to control all of the versions and all of the
uses  copyright law always only sets up the boundaries  anything can be
changed by contract

                                 

session vii  conclusion


general discussion  two questions for discussion  different emphases in
the workshop  bringing the text and image partisans together 
desiderata in planning the longterm development of something  questions
surrounding the issue of electronic deposit  discussion of electronic
deposit as an allusion to the issue of standards  need for a directory
of preservation projects in digital form and for access to their
digitized files  ceths catalogue of machinereadable texts in the
humanities  what constitutes a publication in the electronic world 
need for lc to deal with the concept of online publishing  lcs network
development office  exploring the limits of marc as a standard in terms
of handling electronic information  magnitude of the problem and the
need for distributed responsibility in order to maintain and store
electronic information  workshop participants to be viewed as a starting
point  development of a network version of am urged  a step toward ams
construction of some sort of apparatus for network access  a delicate
and agonizing policy question for lc  re the issue of electronic
deposit lc urged to initiate a catalytic process in terms of distributed
responsibility  suggestions for cooperative ventures  commercial
publishers fears  strategic questions for getting the image and text
people to think through longterm cooperation  clarification of the
driving force behind both the perseus and the cornell xerox projects 


in his role as moderator of the concluding session gifford raised two
questions he believed would benefit from discussion   are there enough
commonalities among those of us that have been here for two days so that
we can see courses of action that should be taken in the future  and if
so what are they and who might take them   partly derivative from
that but obviously very dangerous to lc as host do you see a role for
the library of congress in all this  of course the library of congress
holds a rather special status in a number of these matters because it is
not perceived as a player with an economic stake in them but are there
roles that lc can play that can help advance us toward where we are heading

describing himself as an uninformed observer of the technicalities of the
last two days gifford detected three different emphases in the workshop 
 people who are very deeply committed to text  people who are almost
passionate about images and  a few people who are very committed to
what happens to the networks  in other words the new networking
dimension the accessibility of the processability the portability of
all this across the networks  how do we pull those three together

adding a question that reflected hockeys comment that this was the
fourth workshop she had attended in the previous thirty days fleischhauer
wondered to what extent this meeting had reinvented the wheel or if it
had contributed anything in the way of bringing together a different group
of people from those who normally appear on the workshop circuit

hockey confessed to being struck at this meeting and the one the
electronic pierce consortium organized the previous week that this was a
coming together of people working on texts and not images  attempting to
bring the two together is something we ought to be thinking about for the
future  how one can think about working with image material to begin
with but structuring it and digitizing it in such a way that at a later
stage it can be interpreted into text and find a common way of building
text and images together so that they can be used jointly in the future
with the network support to begin there because that is how people will
want to access it

in planning the longterm development of something which is what is
being done in electronic text hockey stressed the importance not only
of discussing the technical aspects of how one does it but particularly
of thinking about what the people who use the stuff will want to do
but conversely there are numerous things that people start to do with
electronic text or material that nobody ever thought of in the beginning

lesk in response to the question concerning the role of the library of
congress remarked the often suggested desideratum of having electronic
deposit  since everything is now computertypeset an entire decade of
material that was machinereadable exists but the publishers frequently
did not save it has lc taken any action to have its copyright deposit
operation start collecting these machinereadable versions  in the
absence of peters gifford replied that the question was being
actively considered but that that was only one dimension of the problem
another dimension is the whole question of the integrity of the original
electronic document  it becomes highly important in science to prove
authorship  how will that be done

erway explained that under the old policy to make a claim for a
copyright for works that were published in electronic form including
software one had to submit a paper copy of the first and last twenty
pages of codesomething that represented the work but did not include
the entire work itself and had little value to anyone  as a temporary
measure lc has claimed the right to demand electronic versions of
electronic publications  this measure entails a proactive role for the
library to say that it wants a particular electronic version  publishers
then have perhaps a year to submit it  but the real problem for lc is
what to do with all this material in all these different formats  will
the library mount it  how will it give people access to it  how does lc
keep track of the appropriate computers software and media  the situation
is so hard to control erway said that it makes sense for each publishing
house to maintain its own archive  but lc cannot enforce that either

gifford acknowledged lesks suggestion that establishing a priority
offered the solution albeit a fairly complicated one  but who maintains
that register he asked  graber noted that lc does attempt to collect a
macintosh version and the ibmcompatible version of software  it does
not collect other versions  but while true for software byrum observed
this reply does not speak to materials that is all the materials that
were published that were on somebodys microcomputer or driver tapes
at a publishing office across the country  lc does well to acquire
specific machinereadable products selectively that were intended to be
machinereadable  materials that were in machinereadable form at one time
byrum said would be beyond lcs capability at the moment insofar as
attempting to acquire organize and preserve them are concernedand
preservation would be the most important consideration  in this
connection gifford reiterated the need to work out some sense of
distributive responsibility for a number of these issues which
inevitably will require significant cooperation and discussion
nobody can do it all

lesk suggested that some publishers may look with favor on lc beginning
to serve as a depository of tapes in an electronic manuscript standard 
publishers may view this as a service that they did not have to perform
and they might send in tapes  however sperbergmcqueen countered
although publishers have had equivalent services available to them for a
long time the electronic text archive has never turned away or been
flooded with tapes and is forever sending feedback to the depositor 
some publishers do send in tapes

andre viewed this discussion as an allusion to the issue of standards 
she recommended that the aap standard and the tei which has already been
somewhat harmonized internationally and which also shares several
compatibilities with the aap be harmonized to ensure sufficient
compatibility in the software  she drew the line at saying lc ought to
be the locus or forum for such harmonization

taking the group in a slightly different direction but one where at
least in the near term lc might play a helpful role lynch remarked the
plans of a number of projects to carry out preservation by creating
digital images that will end up in online or nearline storage at some
institution   presumably lc will link this material somehow to its
online catalog in most cases  thus it is in a digital form  lynch had
the impression that many of these institutions would be willing to make
those files accessible to other people outside the institution provided
that there is no copyright problem  this desideratum will require
propagating the knowledge that those digitized files exist so that they
can end up in other online catalogs  although uncertain about the
mechanism for achieving this result lynch said that it warranted
scrutiny because it seemed to be connected to some of the basic issues of
cataloging and distribution of records  it would be  foolish given the
amount of work that all of us have to do and our meager resources to
discover multiple institutions digitizing the same work  re microforms
lynch said we are in pretty good shape

battin called this a big problem and noted that the cornell people who
had already departed were working on it  at issue from the beginning
was to learn how to catalog that information into rlin and then into
oclc so that it would be accessible  that issue remains to be resolved 
lynch rejoined that putting it into oclc or rlin was helpful insofar as
somebody who is thinking of performing preservation activity on that work
could learn about it  it is not necessarily helpful for institutions to
make that available  battin opined that the idea was that it not only be
for preservation purposes but for the convenience of people looking for
this material  she endorsed lynchs dictum that duplication of this
effort was to be avoided by every means

hockey informed the workshop about one major current activity of ceth
namely a catalogue of machinereadable texts in the humanities  held on
rlin at present the catalogue has been concentrated on ascii as opposed
to digitized images of text  she is exploring ways to improve the
catalogue and make it more widely available and welcomed suggestions
about these concerns  ceth owns the records which are not just
restricted to rlin and can distribute them however it wishes

taking up lesks earlier question battin inquired whether lc since it
is accepting electronic files and designing a mechanism for dealing with
that rather than putting books on shelves would become responsible for
the national copyright depository of electronic materials  of course
that could not be accomplished overnight but it would be something lc
could plan for  gifford acknowledged that much thought was being devoted
to that set of problems and returned the discussion to the issue raised
by lynchwhether or not putting the kind of records that both battin and
hockey have been talking about in rlin is not a satisfactory solution 
it seemed to him that rlin answered lynchs original point concerning
some kind of directory for these kinds of materials  in a situation
where somebody is attempting to decide whether or not to scan this or
film that or to learn whether or not someone has already done so lynch
suggested rlin is helpful but it is not helpful in the case of a local
online catalogue  further one would like to have her or his system be
aware that that exists in digital form so that one can present it to a
patron even though one did not digitize it if it is out of copyright 
the only way to make those linkages would be to perform a tremendous
amount of realtime lookup which would be awkward at best or
periodically to yank the whole file from rlin and match it against ones
own stuff which is a nuisance

but where erway inquired does one stop including things that are
available with internet for instance in ones local catalogue
it almost seems that that is lcs means to acquire access to them
that represents lcs new form of library loan  perhaps lcs new online
catalogue is an amalgamation of all these catalogues on line  lynch
conceded that perhaps that was true in the very long term but was not
applicable to scanning in the short term  in his view the totals cited
by yale  books over perhaps a fouryear period and 
books from cornell were not big numbers while searching all over
creation for relatively rare occurrences will prove to be less efficient 
as gifford wondered if this would not be a separable file on rlin and
could be requested from them battin interjected that it was easily
accessible to an institution  severtson pointed out that that file cum
enhancements was available with reference information on cdrom which
makes it a little more available

in hockeys view the real question facing the workshop is what to put in
this catalogue because that raises the question of what constitutes a
publication in the electronic world  weibel interjected that eric joule
in oclcs office of research is also wrestling with this particular
problem while gifford thought it sounded fairly generic  hockey
contended that a majority of texts in the humanities are in the hands
of either a small number of large research institutions or individuals
and are not generally available for anyone else to access at all
she wondered if these texts ought to be catalogued

after argument proceeded back and forth for several minutes over why
cataloguing might be a necessary service lebron suggested that this
issue involved the responsibility of a publisher  the fact that someone
has created something electronically and keeps it under his or her
control does not constitute publication  publication implies
dissemination  while it would be important for a scholar to let other
people know that this creation exists in many respects this is no
different from an unpublished manuscript  that is what is being accessed
in there except that now one is not looking at it in the hardcopy but
in the electronic environment

lebron expressed puzzlement at the variety of ways electronic publishing
has been viewed  much of what has been discussed throughout these two
days has concerned cdrom publishing whereas in the online environment
that she confronts the constraints and challenges are very different 
sooner or later lc will have to deal with the concept of online
publishing  taking up the comment erway made earlier about storing
copies lebron gave her own journal as an example  how would she deposit
ojcct for copyright she asked because the journal will exist in the
mainframe at oclc and people will be able to access it  here the
situation is different ownership versus access and is something that
arises with publication in the online environment faster than is
sometimes realized  lacking clear answers to all of these questions
herself lebron did not anticipate that lc would be able to take a role
in helping to define some of them for quite a while

greenfield observed that lcs network development office is attempting
among other things to explore the limits of marc as a standard in terms
of handling electronic information  greenfield also noted that rebecca
guenther from that office gave a paper to the american society for
information science asis summarizing several of the discussion papers
that were coming out of the network development office  greenfield said
he understood that that office had a listserver soliciting just the kind
of feedback received today concerning the difficulties of identifying and
cataloguing electronic information  greenfield hoped that everybody
would be aware of that and somehow contribute to that conversation

noting two of lcs roles first to act as a repository of record for
material that is copyrighted in this country and second to make
materials it holds available in some limited form to a clientele that
goes beyond congress besser suggested that it was incumbent on lc to
extend those responsibilities to all the things being published in
electronic form  this would mean eventually accepting electronic
formats  lc could require that at some point they be in a certain
limited set of formats and then develop mechanisms for allowing people
to access those in the same way that other things are accessed  this
does not imply that they are on the network and available to everyone 
lc does that with most of its bibliographic records besser said which
end up migrating to the utility eg oclc or somewhere else  but just
as most of lcs books are available in some form through interlibrary
loan or some other mechanism so in the same way electronic formats ought
to be available to others in some format though with some copyright
considerations  besser was not suggesting that these mechanisms be
established tomorrow only that they seemed to fall within lcs purview
and that there should be longrange plans to establish them

acknowledging that those from lc in the room agreed with besser
concerning the need to confront difficult questions gifford underscored
the magnitude of the problem of what to keep and what to select  gifford
noted that lc currently receives some  items per day not counting
electronic materials and argued for much more distributed responsibility
in order to maintain and store electronic information

besser responded that the assembled group could be viewed as a starting
point whose initial operating premise could be helping to move in this
direction and defining how lc could do so for example in areas of
standardization or distribution of responsibility

fleischhauer added that am was fully engaged wrestling with some of the
questions that pertain to the conversion of older historical materials
which would be one thing that the library of congress might do  several
points mentioned by besser and several others on this question have a
much greater impact on those who are concerned with cataloguing and the
networking of bibliographic information as well as preservation itself

speaking directly to am which he considered was a largely uncopyrighted
database lynch urged development of a network version of am or
consideration of making the data in it available to people interested in
doing network multimedia  on account of the current great shortage of
digital data that is both appealing and unencumbered by complex rights
problems this course of action could have a significant effect on making
network multimedia a reality

in this connection fleischhauer reported on a fragmentary prototype in
lcs office of information technology services that attempts to associate
digital images of photographs with cataloguing information in ways that
work within a local area networka step so to say toward ams
construction of some sort of apparatus for access  further am has
attempted to use standard data forms in order to help make that
distinction between the access tools and the underlying data and thus
believes that the database is networkable

a delicate and agonizing policy question for lc however which comes
back to resources and unfortunately has an impact on this is to find
some appropriate honorable and legal costrecovery possibilities  a
certain skittishness concerning costrecovery has made people unsure
exactly what to do  am would be highly receptive to discussing further
lynchs offer to test or demonstrate its database in a network
environment fleischhauer said

returning the discussion to what she viewed as the vital issue of
electronic deposit battin recommended that lc initiate a catalytic
process in terms of distributed responsibility that is bring together
the distributed organizations and set up a study group to look at all
these issues and see where we as a nation should move  the broader
issues of how we deal with the management of electronic information will
not disappear but only grow worse

lesk took up this theme and suggested that lc attempt to persuade one
major library in each state to deal with its state equivalent publisher
which might produce a cooperative project that would be equitably
distributed around the country and one in which lc would be dealing with
a minimal number of publishers and minimal copyright problems

graber remarked the recent development in the scientific community of a
willingness to use sgml and either deposit or interchange on a fairly
standardized format  he wondered if a similar movement was taking place
in the humanities  although the national library of medicine found only
a few publishers to cooperate in a like venture two or three years ago a
new effort might generate a much larger number willing to cooperate

kimball recounted his units machinereadable collections reading room
troubles with the commercial publishers of electronic media in acquiring
materials for lcs collections in particular the publishers fear that
they would not be able to cover their costs and would lose control of
their products that lc would give them away or sell them and make
profits from them  he doubted that the publishing industry was prepared
to move into this area at the moment given its resistance to allowing lc
to use its machinereadable materials as the library would like

the copyright law now addresses compact disk as a medium and lc can
request one copy of that or two copies if it is the only version and
can request copies of software but that fails to address magazines or
books or anything like that which is in machinereadable form

gifford acknowledged the thorny nature of this issue which he illustrated
with the example of the cumbersome process involved in putting a copy of a
scientific database on a lan in lcs science reading room  he also
acknowledged that lc needs help and could enlist the energies and talents
of workshop participants in thinking through a number of these problems

gifford returned the discussion to getting the image and text people to
think through together where they want to go in the long term  mylonas
conceded that her experience at the pierce symposium the previous week at
georgetown university and this week at lc had forced her to reevaluate
her perspective on the usefulness of text as images  mylonas framed the
issues in a series of questions  how do we acquire machinereadable
text  do we take pictures of it and perform ocr on it later  is it
important to obtain very highquality images and text etc 
fleischhauer agreed with mylonass framing of strategic questions adding
that a large institution such as lc probably has to do all of those
things at different times  thus the trick is to exercise judgment  the
workshop had added to his and ams considerations in making those
judgments  concerning future meetings or discussions mylonas suggested
that screening priorities would be helpful

weibel opined that the diversity reflected in this group was a sign both
of the health and of the immaturity of the field and more time would
have to pass before we convince one another concerning standards

an exchange between mylonas and battin clarified the point that the
driving force behind both the perseus and the cornell xerox projects was
the preservation of knowledge for the future not simply for particular
research use  in the case of perseus mylonas said the assumption was
that the texts would not be entered again into electronically readable
form  sperbergmcqueen added that a scanned image would not serve as an
archival copy for purposes of preservation in the case of say the bill
of rights in the sense that the scanned images are effectively the
archival copies for the cornell mathematics books


                                 


                          appendix i  program



                                workshop
                                   on
                               electronic
                                  texts



                              june 

                           library of congress
                            washington dc



    supported by a grant from the david and lucile packard foundation


tuesday  june 

national demonstration lab atrium library madison

 am   coffee and danish registration

 am   welcome

          prosser gifford director for scholarly programs and carl
             fleischhauer coordinator american memory library of
             congress

l am   session i  content in a new form  who will use it and what
          will they do

          broad description of the range of electronic information 
          characterization of who uses it and how it is or may be used 
          in addition to a look at scholarly uses this session will
          include a presentation on use by students k and college
          and the general public

          moderator  james daly
          avra michelson archival research and evaluation staff
             national archives and records administration overview
          susan h veccia team leader american memory user evaluation
             and
          joanne freeman associate coordinator american memory library
             of congress beyond the scholar


 am  break

 am  session ii  show and tell

          each presentation to consist of a fifteenminute
          statementshow group discussion will follow lunch

          moderator  jacqueline hess director national demonstration
             lab

              a classics project stressing texts and text retrieval
                more than multimedia  perseus project harvard
                university
                elli mylonas managing editor

              other humanities projects employing the emerging norms of
                the text encoding initiative tei  chadwyckhealeys
                the english poetry full text database andor patrologia
                latina database
                eric m calaluca vice president chadwyckhealey inc

              american memory
                carl fleischhauer coordinator and
                ricky erway associate coordinator library of congress

              founding fathers example from packard humanities
                institute  the papers of george washington university
                of virginia
                dorothy twohig managing editor andor
                david woodley packard

              an electronic medical journal offering graphics and
                fulltext searchability  the online journal of current
                clinical trials american association for the advancement
                of science
                maria l lebron managing editor

              a project that offers facsimile images of pages but omits
                searchable text  cornell math books
                lynne k personius assistant director cornell
                   information technologies for scholarly information
                   sources cornell university

 pm  lunch  dining room a library madison   exhibits
          available

 pm   session ii  show and tell contd


 pm   break


 pm   session iii  distribution networks and networking  options
          for dissemination

          published disks  university presses and publicsector
             publishers privatesector publishers
          computer networks

          moderator  robert g zich special assistant to the associate
             librarian for special projects library of congress
          clifford a lynch director library automation university of
             california
          howard besser school of library and information science
             university of pittsburgh
          ronald l larsen associate director of libraries for
             information technology university of maryland at college
             park
          edwin b brownrigg executive director memex research
             institute

 pm   reception  montpelier room library madison 

                                 

wednesday  june 

dining room a library madison 

 am   coffee and danish

 am   session iv  image capture text capture overview of text and
          image storage formats

          moderator  william l hooton vice president of operations
             inet

          a principal methods for image capture of text
             direct scanning
             use of microform

          anne r kenney assistant director department of preservation
             and conservation cornell university
          pamela qj andre associate director automation and
          judith a zidar coordinator national agricultural text
             digitizing program natdp national agricultural library
             nal
          donald j waters head systems office yale university library

          b special problems
             bound volumes
             conservation
             reproducing printed halftones

          carl fleischhauer coordinator american memory library of
             congress
          george thoma chief communications engineering branch
             national library of medicine nlm


 am  break

 am  session iv  image capture text capture overview of text and
          image storage formats contd

          c image standards and implications for preservation

          jean baronas senior manager department of standards and
             technology association for information and image management
             aiim
          patricia battin president the commission on preservation and
             access cpa

          d text conversion
             ocr vs rekeying
             standards of accuracy and use of imperfect texts
             service bureaus

          stuart weibel senior research specialist online computer
             library center inc oclc
          michael lesk executive director computer science research
             bellcore
          ricky erway associate coordinator american memory library of
             congress
          pamela qj andre associate director automation and
          judith a zidar coordinator national agricultural text
             digitizing program natdp national agricultural library
             nal


 pm   lunch

 pm   session v  approaches to preparing electronic texts

          discussion of approaches to structuring text for the computer
          pros and cons of text coding description of methods in
          practice and comparison of textcoding methods

          moderator  susan hockey director center for electronic texts
             in the humanities ceth rutgers and princeton universities
          david woodley packard
          cm sperbergmcqueen editor text encoding initiative tei
             university of illinoischicago
          eric m calaluca vice president chadwyckhealey inc


 pm   break

 pm   session vi  copyright issues

          marybeth peters policy planning adviser to the register of
             copyrights library of congress

 pm   session vii conclusion

          general discussion
          what topics were omitted or given short shrift that anyone
             would like to talk about now
          is there a group here  what should the group do next if
             anything  what should the library of congress do next if
             anything
          moderator  prosser gifford director for scholarly programs
             library of congress

 pm   adjourn


                                 


                         appendix ii  abstracts


session i

avra michelson           forecasting the use of electronic texts by
                         social sciences and humanities scholars

this presentation explores the ways in which electronic texts are likely
to be used by the nonscientific scholarly community  many of the
remarks are drawn from a report the speaker coauthored with jeff
rothenberg a computer scientist at the rand corporation

the speaker assesses  current scholarly use of information technology
and  the key trends in information technology most relevant to the
research process in order to predict how social sciences and humanities
scholars are apt to use electronic texts  in introducing the topic
current use of electronic texts is explored broadly within the context of
scholarly communication  from the perspective of scholarly
communication the work of humanities and social sciences scholars
involves five processes   identification of sources  communication
with colleagues  interpretation and analysis of data  dissemination
of research findings and  curriculum development and instruction  the
extent to which computation currently permeates aspects of scholarly
communication represents a viable indicator of the prospects for
electronic texts

the discussion of current practice is balanced by an analysis of key
trends in the scholarly use of information technology  these include the
trends toward enduser computing and connectivity which provide a
framework for forecasting the use of electronic texts through this
millennium  the presentation concludes with a summary of the ways in
which the nonscientific scholarly community can be expected to use
electronic texts and the implications of that use for information
providers

susan veccia and joanne freeman    electronic archives for the public 
                                   use of american memory in public and
                                   school libraries

this joint discussion focuses on nonscholarly applications of electronic
library materials specifically addressing use of the library of congress
american memory am program in a small number of public and school
libraries throughout the united states  am consists of selected library
of congress primary archival materials stored on optical media
cdromvideodisc and presented with little or no editing  many
collections are accompanied by electronic introductions and users guides
offering background information and historical context  collections
represent a variety of formats including photographs graphic arts
motion pictures recorded sound music broadsides and manuscripts
books and pamphlets

in  the library of congress began a nationwide evaluation of am in
different types of institutions  test sites include public libraries
elementary and secondary school libraries college and university
libraries state libraries and special libraries  susan veccia and
joanne freeman will discuss their observations on the use of am by the
nonscholarly community using evidence gleaned from this ongoing
evaluation effort

veccia will comment on the overall goals of the evaluation project and
the types of public and school libraries included in this study  her
comments on nonscholarly use of am will focus on the public library as a
cultural and community institution often bridging the gap between formal
and informal education  freeman will discuss the use of am in school
libraries  use by students and teachers has revealed some broad
questions about the use of electronic resources as well as definite
benefits gained by the nonscholar  topics will include the problem of
grasping content and context in an electronic environment the stumbling
blocks created by new technologies and the unique skills and interests
awakened through use of electronic resources

session ii

elli mylonas             the perseus project  interactive sources and
                         studies in classical greece

the perseus project  has just released perseus  the first publicly
available version of its hypertextual database of multimedia materials on
classical greece  perseus is designed to be used by a wide audience
comprised of readers at the student and scholar levels  as such it must
be able to locate information using different strategies and it must
contain enough detail to serve the different needs of its users  in
addition it must be delivered so that it is affordable to its target
audience  these problems and the solutions we chose are described in
mylonas an interface to classical greek civilization jasis 
march 

in order to achieve its objective the project staff decided to make a
conscious separation between selecting and converting textual database
and image data on the one hand and putting it into a delivery system on
the other  that way it is possible to create the electronic data
without thinking about the restrictions of the delivery system  we have
made a great effort to choose systemindependent formats for our data
and to put as much thought and work as possible into structuring it so
that the translation from paper to electronic form will enhance the value
of the data a discussion of these solutions as of two years ago is in
elli mylonas gregory crane kenneth morrell and d neel smith the
perseus project  data in the electronic age in accessing antiquity 
the computerization of classical databases j solomon and t worthen
eds  university of arizona press in press

much of the work on perseus is focused on collecting and converting the
data on which the project is based  at the same time it is necessary to
provide means of access to the information in order to make it usable
and them to investigate how it is used  as we learn more about what
students and scholars from different backgrounds do with perseus we can
adjust our data collection and also modify the system to accommodate
them  in creating a delivery system for general use we have tried to
avoid favoring any one type of use by allowing multiple forms of access
to and navigation through the system

the way text is handled exemplifies some of these principles  all text
in perseus is tagged using sgml following the guidelines of the text
encoding initiative tei  this markup is used to index the text and
process it so that it can be imported into hypercard  no sgml markup
remains in the text that reaches the user because currently it would be
too expensive to create a system that acts on sgml in real time 
however the regularity provided by sgml is essential for verifying the
content of the texts and greatly speeds all the processing performed on
them  the fact that the texts exist in sgml ensures that they will be
relatively easy to port to different hardware and software and so will
outlast the current delivery platform  finally the sgml markup
incorporates existing canonical reference systems chapter verse line
etc indexing and navigation are based on these features  this ensures
that the same canonical reference will always resolve to the same point
within a text and that all versions of our texts regardless of delivery
platform even paper printouts will function the same way

in order to provide tools for users the text is processed by a
morphological analyzer and the results are stored in a database 
together with the index the greekenglish lexicon and the index of all
the english words in the definitions of the lexicon the morphological
analyses comprise a set of linguistic tools that allow users of all
levels to work with the textual information and to accomplish different
tasks  for example students who read no greek may explore a concept as
it appears in greek texts by using the englishgreek index and then
looking up works in the texts and translations or scholars may do
detailed morphological studies of word use by using the morphological
analyses of the texts  because these tools were not designed for any one
use the same tools and the same data can be used by both students and
scholars

notes
       perseus is based at harvard university with collaborators at
     several other universities  the project has been funded primarily
     by the annenbergcpb project as well as by harvard university
     apple computer and others  it is published by yale university
     press  perseus runs on macintosh computers under the hypercard
     program

eric calaluca

chadwyckhealey embarked last year on two distinct yet related fulltext
humanities database projects

the english poetry fulltext database and the patrologia latina database
represent new approaches to linguistic research resources  the size and
complexity of the projects present problems for electronic publishers
but surmountable ones if they remain abreast of the latest possibilities
in data capture and retrieval software techniques

the issues which required address prior to the commencement of the
projects were legion

        editorial selection or exclusion of materials in each
          database

        deciding whether or not to incorporate a normative encoding
          structure into the databases
               a  if one is selected should it be sgml
               b  if sgml then the tei
     
        deliver as cdrom magnetic tape or both

        can one produce retrieval software advanced enough for the
          postdoctoral linguist yet accessible enough for unattended
          general use  should one try

        re fair and liberal networking policies what are the risks to
          an electronic publisher

        how does the emergence of national and international education
          networks affect the use and viability of research projects
          requiring high investment  do the new european community
          directives concerning database protection necessitate two
          distinct publishing projects one for north america and one for
          overseas

from new notions of scholarly fair use to the future of optical media
virtually every issue related to electronic publishing was aired  the
result is two projects which have been constructed to provide the quality
research resources with the fewest encumbrances to use by teachers and
private scholars

dorothy twohig

in spring  the editors of the papers of george washington john
adams thomas jefferson james madison and benjamin franklin were
approached by classics scholar david packard on behalf of the packard
humanities foundation with a proposal to produce a cdrom edition of the
complete papers of each of the founding fathers  this electronic edition
will supplement the published volumes making the documents widely
available to students and researchers at reasonable cost  we estimate
that our cdrom edition of washingtons papers will be substantially
completed within the next two years and ready for publication  within
the next ten years or so similar cdrom editions of the franklin adams
jefferson and madison papers also will be available  at the library of
congresss session on technology i would like to discuss not only the
experience of the washington papers in producing the cdrom edition but
the impact technology has had on these major editorial projects 
already we are editing our volumes with an eye to the material that will
be readily available in the cdrom edition  the completed electronic
edition will provide immense possibilities for the searching of documents
for information in a way never possible before  the kind of technical
innovations that are currently available and on the drawing board will
soon revolutionize historical research and the production of historical
documents  unfortunately much of this new technology is not being used
in the planning stages of historical projects simply because many
historians are aware only in the vaguest way of its existence  at least
two major new historical editing projects are considering microfilm
editions simply because they are not aware of the possibilities of
electronic alternatives and the advantages of the new technology in terms
of flexibility and research potential compared to microfilm  in fact
too many of us in history and literature are still at the stage of
struggling with our pcs  there are many historical editorial projects in
progress presently and an equal number of literary projects  while the
two fields have somewhat different approaches to textual editing there
are ways in which electronic technology can be of service to both

since few of the editors involved in the founding fathers cdrom editions
are technical experts in any sense i hope to point out in my discussion
of our experience how many of these electronic innovations can be used
successfully by scholars who are novices in the world of new technology 
one of the major concerns of the sponsors of the multitude of new
scholarly editions is the limited audience reached by the published
volumes  most of these editions are being published in small quantities
and the publishers price for them puts them out of the reach not only of
individual scholars but of most public libraries and all but the largest
educational institutions  however little attention is being given to
ways in which technology can bypass conventional publication to make
historical and literary documents more widely available

what attracted us most to the cdrom edition of the papers of george
washington was the fact that david packards aim was to make a complete
edition of all of the  documents we have collected available in an
inexpensive format that would be placed in public libraries small
colleges and even high schools  this would provide an audience far
beyond our present copy  published edition  since the cdrom
edition will carry none of the explanatory annotation that appears in the
published volumes we also feel that the use of the cdrom will lead many
researchers to seek out the published volumes

in addition to ignorance of new technical advances i have found that too
many editorsand historians and literary scholarsare resistant and
even hostile to suggestions that electronic technology may enhance their
work  i intend to discuss some of the arguments traditionalists are
advancing to resist technology ranging from distrust of the speed with
which it changes we are already wondering what is out there that is
better than cdrom to suspicion of the technical language used to
describe electronic developments

maria lebron

the online journal of current clinical trials a joint venture of the
american association for the advancement of science aaas and the online
computer library center inc oclc is the first peerreviewed journal
to provide full text tabular material and line illustrations on line 
this presentation will discuss the genesis and startup period of the
journal  topics of discussion will include historical overview
daytoday management of the editorial peer review and manuscript
tagging and publication  a demonstration of the journal and its features
will accompany the presentation

lynne personius

cornell university library cornell information technologies and xerox
corporation with the support of the commission on preservation and
access and sun microsystems inc have been collaborating in a project
to test a prototype system for recording brittle books as digital images
and producing on demand highquality archival paper replacements  the
project goes beyond that however to investigate some of the issues
surrounding scanning storing retrieving and providing access to
digital images in a network environment

the joint study in digital preservation began in january   xerox
provided the college library access and storage system class software
a prototype dotsperinch dpi scanner and the hardware necessary
to support network printing on the docutech printer housed in cornells
computing and communications center ccc

the cornell staff using the hardware and software became an integral part
of the development and testing process for enhancements to the class
software system  the collaborative nature of this relationship is
resulting in a system that is specifically tailored to the preservation
application

a digital library of  volumes or approximately  images has
been created and is stored on an optical jukebox that resides in ccc 
the library includes a collection of select mathematics monographs that
provides mathematics faculty with an opportunity to use the electronic
library  the remaining volumes were chosen for the library to test the
various capabilities of the scanning system

one project objective is to provide users of the cornell library and the
library staff with the ability to request facsimiles of digitized images
or to retrieve the actual electronic image for browsing  a prototype
viewing workstation has been created by xerox with input into the design
by a committee of cornell librarians and computer professionals  this
will allow us to experiment with patron access to the images that make up
the digital library  the viewing station provides search retrieval and
ultimately printing functions with enhancements to facilitate
navigation through multiple documents

cornell currently is working to extend access to the digital library to
readers using workstations from their offices  this year is devoted to
the development of a network resident image conversion and delivery
server and client software that will support readers who use apple
macintosh computers ibm windows platforms and sun workstations 
equipment for this development was provided by sun microsystems with
support from the commission on preservation and access

during the showandtell session of the workshop on electronic texts a
prototype view station will be demonstrated  in addition a display of
original library books that have been digitized will be available for
review with associated printed copies for comparison  the fifteenminute
overview of the project will include a slide presentation that
constitutes a tour of the preservation digitizing process

the final networkconnected version of the viewing station will provide
library users with another mechanism for accessing the digital library
and will also provide the capability of viewing images directly  this
will not require special software although a powerful computer with good
graphics will be needed

the joint study in digital preservation has generated a great deal of
interest in the library community  unfortunately or perhaps
fortunately this project serves to raise a vast number of other issues
surrounding the use of digital technology for the preservation and use of
deteriorating library materials which subsequent projects will need to
examine  much work remains

session iii

howard besser                      networking multimedia databases

what do we have to consider in building and distributing databases of
visual materials in a multiuser environment  this presentation examines
a variety of concerns that need to be addressed before a multimedia
database can be set up in a networked environment

in the past it has not been feasible to implement databases of visual
materials in shareduser environments because of technological barriers 
each of the two basic models for multiuser multimedia databases has
posed its own problem  the analog multimedia storage model represented
by project athenas parallel analog and digital networks has required an
incredibly complex and expensive infrastructure  the economies of
scale that make multiuser setups cheaper per user served do not operate
in an environment that requires a computer workstation videodisc player
and two display devices for each user

the digital multimedia storage model has required vast amounts of storage
space as much as one gigabyte per thirty still images  in the past the
cost of such a large amount of storage space made this model a
prohibitive choice as well  but plunging storage costs are finally
making this second alternative viable

if storage no longer poses such an impediment what do we need to
consider in building digitally stored multiuser databases of visual
materials  this presentation will examine the networking and
telecommunication constraints that must be overcome before such databases
can become commonplace and useful to a large number of people

the key problem is the vast size of multimedia documents and how this
affects not only storage but telecommunications transmission time 
anything slower than t speed is impractical for files of  megabyte or
larger which is likely to be small for a multimedia document  for
instance even on a  kb line it would take three minutes to transfer a
megabyte file  and these figures assume ideal circumstances and do
not take into consideration other users contending for network bandwidth
disk access time or the time needed for remote display  current common
telephone transmission rates would be completely impractical few users
would be willing to wait the hour necessary to transmit a single image at
 baud

this necessitates compression which itself raises a number of other
issues  in order to decrease file sizes significantly we must employ
lossy compression algorithms  but how much quality can we afford to
lose  to date there has been only one significant study done of
imagequality needs for a particular user group and this study did not
look at loss resulting from compression  only after identifying
imagequality needs can we begin to address storage and network bandwidth
needs

experience with xwindowsbased applications such as imagequery the
university of california at berkeley image database demonstrates the
utility of a clientserver topology but also points to the limitation of
current software for a distributed environment  for example
applications like imagequery can incorporate compression but current x
implementations do not permit decompression at the end users
workstation  such decompression at the host computer alleviates storage
capacity problems while doing nothing to address problems of
telecommunications bandwidth

we need to examine the effects on network throughput of moving
multimedia documents around on a network  we need to examine various
topologies that will help us avoid bottlenecks around servers and
gateways  experience with applications such as these raise still broader
questions how closely is the multimedia document tied to the software
for viewing it  can it be accessed and viewed from other applications 
experience with the marc format and more recently with the z
protocols shows how useful it can be to store documents in a form in
which they can be accessed by a variety of application software

finally from an intellectualaccess standpoint we need to address the
issue of providing access to these multimedia documents in
interdisciplinary environments  we need to examine terminology and
indexing strategies that will allow us to provide access to this material
in a crossdisciplinary way

ronald larsen            directions in highperformance networking for
                         libraries

the pace at which computing technology has advanced over the past forty
years shows no sign of abating  roughly speaking each fiveyear period
has yielded an orderofmagnitude improvement in price and performance of
computing equipment  no fundamental hurdles are likely to prevent this
pace from continuing for at least the next decade  it is only in the
past five years though that computing has become ubiquitous in
libraries affecting all staff and patrons directly or indirectly

during these same five years communications rates on the internet the
principal academic computing network have grown from  kbps to 
mbps and the nsfnet backbone is now running  mbps  over the next five
years communication rates on the backbone are expected to exceed  gbps 
growth in both the population of network users and the volume of network
traffic  has continued to grow geometrically at rates approaching 
percent per month  this flood of capacity and use likened by some to
drinking from a firehose  creates immense opportunities and challenges
for libraries  libraries must anticipate the future implications of this
technology participate in its development and deploy it to ensure
access to the worlds information resources

the infrastructure for the information age is being put in place 
libraries face strategic decisions about their role in the development
deployment and use of this infrastructure  the emerging infrastructure
is much more than computers and communication lines  it is more than the
ability to compute at a remote site send electronic mail to a peer
across the country or move a file from one library to another  the next
five years will witness substantial development of the information
infrastructure of the network

in order to provide appropriate leadership library professionals must
have a fundamental understanding of and appreciation for computer
networking from local area networks to the national research and
education network nren  this presentation addresses these
fundamentals and how they relate to libraries today and in the near
future

edwin brownrigg               electronic library visions and realities

the electronic library has been a vision desired by manyand rejected by
somesince vannevar bush coined the term memex to describe an automated
intelligent personal information system  variations on this vision have
included ted nelsons xanadau alan kays dynabook and lancasters
paperless library with the most recent incarnation being the
knowledge navigator described by john scully of apple  but the reality
of library service has been less visionary and the leap to the electronic
library has eluded universities publishers and information technology
files

the memex research institute memri an independent nonprofit research
and development organization has created an electronic library program
of shared research and development in order to make the collective vision
more concrete  the program is working toward the creation of large
indexed publicly available electronic image collections of published
documents in academic special and public libraries  this strategic
plan is the result of the first stage of the program which has been an
investigation of the information technologies available to support such
an effort the economic parameters of electronic service compared to
traditional library operations and the business and political factors
affecting the shift from print distribution to electronic networked
access

the strategic plan envisions a combination of publicly searchable access
databases image and text document collections stored on network file
servers local and remote network access and an intellectual property
managementcontrol system  this combination of technology and
information content is defined in this plan as an elibrary or elibrary
collection  some participating sponsors are already developing projects
based on memris recommended directions

the elibrary strategy projected in this plan is a visionary one that can
enable major changes and improvements in academic public and special
library service  this vision is though one that can be realized with
todays technology  at the same time it will challenge the political
and social structure within which libraries operate  in academic
libraries the traditional emphasis on local collections extending to
accreditation issues in public libraries the potential of electronic
branch and central libraries fully available to the public and for
special libraries new opportunities for shared collections and networks

the environment in which this strategic plan has been developed is at
the moment dominated by a sense of library limits  the continued
expansion and rapid growth of local academic library collections is now
clearly at an end  corporate libraries and even law libraries are
faced with operating within a difficult economic climate as well as with
very active competition from commercial information sources  for
example public libraries may be seen as a desirable but not critical
municipal service in a time when the budgets of safety and health
agencies are being cut back

further libraries in general have a very high labortocost ratio in
their budgets and labor costs are still increasing notwithstanding
automation investments  it is difficult for libraries to obtain capital
startup or seed funding for innovative activities and those
technologyintensive initiatives that offer the potential of decreased
labor costs can provoke the opposition of library staff

however libraries have achieved some considerable successes in the past
two decades by improving both their service and their credibility within
their organizationsand these positive changes have been accomplished
mostly with judicious use of information technologies  the advances in
computing and information technology have been wellchronicled  the
continuing precipitous drop in computing costs the growth of the
internet and private networks and the explosive increase in publicly
available information databases

for example oclc has become one of the largest computer network
organizations in the world by creating a cooperative cataloging network
of more than  libraries worldwide  online public access catalogs
now serve millions of users on more than  dedicated terminals in
the united states alone  the university of california melvyl online
catalog system has now expanded into an index database reference service
and supports more than six million searches a year  and libraries have
become the largest group of customers of cdrom publishing technology
more than  optical media publications such as those offered by
infotrac and silver platter are subscribed to by us libraries

this march of technology continues and in the next decade will result in
further innovations that are extremely difficult to predict  what is
clear is that libraries can now go beyond automation of their order files
and catalogs to automation of their collections themselvesand it is
possible to circumvent the fiscal limitations that appear to obtain
today

this electronic library strategic plan recommends a paradigm shift in
library service and demonstrates the steps necessary to provide improved
library services with limited capacities and operating investments

session iva

anne kenney

the cornellxerox joint study in digital preservation resulted in the
recording of  brittle books as dpi digital images and the
production on demand of highquality and archivally sound paper
replacements  the project which was supported by the commission on
preservation and access also investigated some of the issues surrounding
scanning storing retrieving and providing access to digital images in
a network environment

anne kenney will focus on some of the issues surrounding direct scanning
as identified in the cornell xerox project  among those to be discussed
are  image versus text capture indexing and access imagecapture
capabilities a comparison to photocopy and microfilm production and
cost analysis storage formats protocols and standards and the use of
this scanning technology for preservation purposes

the dpi digital images produced in the cornell xerox project proved
highly acceptable for creating paper replacements of deteriorating
originals  the  scanned volumes provided an array of imagecapture
challenges that are common to nineteenthcentury printing techniques and
embrittled material and that defy the use of textconversion processes 
these challenges include diminished contrast between text and background
fragile and deteriorated pages uneven printing elaborate type faces
faint and bold text adjacency handwritten text and annotations nonroman
languages and a proliferation of illustrated material embedded in text 
the latter category included highfrequency and lowfrequency halftones
continuous tone photographs intricate mathematical drawings maps
etchings reversepolarity drawings and engravings

the xerox prototype scanning system provided a number of important
features for capturing this diverse material  technicians used multiple
threshold settings filters line art and halftone definitions
autosegmentation windowing and softwareediting programs to optimize
image capture  at the same time this project focused on production 
the goal was to make scanning as affordable and acceptable as
photocopying and microfilming for preservation reformatting  a
timeandcost study conducted during the last three months of this
project confirmed the economic viability of digital scanning and these
findings will be discussed here

from the outset the cornell xerox project was predicated on the use of
nonproprietary standards and the use of common protocols when standards
did not exist  digital files were created as tiff images which were
compressed prior to storage using group  ccitt compression  the xerox
software is ms dos based and utilizes offthe shelf programs such as
microsoft windows and wang image wizard  the digital library is designed
to be hardwareindependent and to provide interchangeability with other
institutions through network connections  access to the digital files
themselves is twotiered  bibliographic records for the computer files
are created in rlin and cornells local system and access into the actual
digital images comprising a book is provided through a document control
structure and a networked image fileserver both of which will be
described

the presentation will conclude with a discussion of some of the issues
surrounding the use of this technology as a preservation tool storage
refreshing backup

pamela andre and judith zidar

the national agricultural library nal has had extensive experience with
raster scanning of printed materials  since  the library has
participated in the national agricultural text digitizing project natdp
a cooperative effort between nal and fortyfive land grant university
libraries  an overview of the project will be presented giving its
history and nals strategy for the future

an indepth discussion of natdp will follow including a description of
the scanning process from the gathering of the printed materials to the
archiving of the electronic pages  the type of equipment required for a
standalone scanning workstation and the importance of file management
software will be discussed  issues concerning the images themselves will
be addressed briefly such as image format black and white versus color
gray scale versus dithering and resolution

also described will be a study currently in progress by nal to evaluate
the usefulness of converting microfilm to electronic images in order to
improve access  with the cooperation of tuskegee university nal has
selected three reels of microfilm from a collection of sixtyseven reels
containing the papers letters and drawings of george washington carver 
the three reels were converted into  electronic images using a
specialized microfilm scanner  the selection filming and indexing of
this material will be discussed

donald waters

project open book the yale university librarys effort to convert 
 books from microfilm to digital imagery is currently in an advanced
state of planning and organization  the yale library has selected a
major vendor to serve as a partner in the project and as systems
integrator  in its proposal the successful vendor helped isolate areas
of risk and uncertainty as well as key issues to be addressed during the
life of the project  the yale library is now poised to decide what
material it will convert to digital image form and to seek funding
initially for the first phase and then for the entire project

the proposal that yale accepted for the implementation of project open
book will provide at the end of three phases a conversion subsystem
browsing stations distributed on the campus network within the yale
library a subsystem for storing  books at  and  dots per
inch and network access to the image printers  pricing for the system
implementation assumes the existence of yales campus ethernet network
and its highspeed image printers and includes other requisite hardware
and software as well as system integration services  proposed operating
costs include hardware and software maintenance but do not include
estimates for the facilities management of the storage devices and image
servers

yale selected its vendor partner in a formal process partly funded by
the commission for preservation and access  following a request for
proposal the yale library selected two vendors as finalists to work with
yale staff to generate a detailed analysis of requirements for project
open book  each vendor used the results of the requirements analysis to
generate and submit a formal proposal for the entire project  this
competitive process not only enabled the yale library to select its
primary vendor partner but also revealed much about the state of the
imaging industry about the varying corporate commitments to the markets
for imaging technology and about the varying organizational dynamics
through which major companies are responding to and seeking to develop
these markets

project open book is focused specifically on the conversion of images
from microfilm to digital form  the technology for scanning microfilm is
readily available but is changing rapidly  in its project requirements
the yale library emphasized features of the technology that affect the
technical quality of digital image production and the costs of creating
and storing the image library  what levels of digital resolution can be
achieved by scanning microfilm  how does variation in the quality of
microfilm particularly in film produced to preservation standards
affect the quality of the digital images  what technologies can an
operator effectively and economically apply when scanning film to
separate twoup images and to control for and correct image
imperfections  how can quality control best be integrated into
digitizing work flow that includes document indexing and storage

the actual and expected uses of digital imagesstorage browsing
printing and ocrhelp determine the standards for measuring their
quality  browsing is especially important but the facilities available
for readers to browse image documents is perhaps the weakest aspect of
imaging technology and most in need of development  as it defined its
requirements the yale library concentrated on some fundamental aspects
of usability for image documents  does the system have sufficient
flexibility to handle the full range of document types including
monographs multipart and multivolume sets and serials as well as
manuscript collections  what conventions are necessary to identify a
document uniquely for storage and retrieval  where is the database of
record for storing bibliographic information about the image document 
how are basic internal structures of documents such as pagination made
accessible to the reader  how are the image documents physically
presented on the screen to the reader

the yale library designed project open book on the assumption that
microfilm is more than adequate as a medium for preserving the content of
deteriorated library materials  as planning in the project has advanced
it is increasingly clear that the challenge of digital image technology
and the key to the success of efforts like project open book is to
provide a means of both preserving and improving access to those
deteriorated materials

session ivb

george thoma

in the use of electronic imaging for document preservation there are
several issues to consider such as  ensuring adequate image quality
maintaining substantial conversion rates throughput providing unique
identification for automated access and retrieval and accommodating
bound volumes and fragile material

to maintain high image quality image processing functions are required
to correct the deficiencies in the scanned image  some commercially
available systems include these functions while some do not  the
scanned raw image must be processed to correct contrast deficiencies
both poor overall contrast resulting from light print andor dark
background and variable contrast resulting from stains and
bleedthrough  furthermore the scan density must be adequate to allow
legibility of print and sufficient fidelity in the pseudohalftoned gray
material  borders or pageedge effects must be removed for both
compactibility and aesthetics  page skew must be corrected for aesthetic
reasons and to enable accurate character recognition if desired 
compound images consisting of both twotoned text and grayscale
illustrations must be processed appropriately to retain the quality of
each

session ivc

jean baronas

standards publications being developed by scientists engineers and
business managers in association for information and image management
aiim standards committees can be applied to electronic image management
eim processes including  document image transfer retrieval and
evaluation optical disk and document scanning and document design and
conversion  when combined with eim system planning and operations
standards can assist in generating image databases that are
interchangeable among a variety of systems  the applications of
different approaches for imagetagging indexing compression and
transfer often cause uncertainty concerning eim system compatibility
calibration performance and upward compatibility until standard
implementation parameters are established  the aiim standards that are
being developed for these applications can be used to decrease the
uncertainty successfully integrate imaging processes and promote open
systems  aiim is an accredited american national standards institute
ansi standards developer with more than twenty committees comprised of
 volunteers representing users vendors and manufacturers  the
standards publications that are developed in these committees have
national acceptance and provide the basis for international harmonization
in the development of new international organization for standardization
iso standards

this presentation describes the development of aiims eim standards and a
new effort at aiim a database on standards projects in a wide framework
of imaging industries including capture recording processing
duplication distribution display evaluation and preservation  the
aiim imagery database will cover imaging standards being developed by
many organizations in many different countries  it will contain
standards publications dates origins related national and
international projects status key words and abstracts  the ansi image
technology standards board requested that such a database be established
as did the isointernational electrotechnical commission joint task force
on imagery  aiim will take on the leadership role for the database and
coordinate its development with several standards developers

patricia battin

     characteristics of standards for digital imagery

           nature of digital technology implies continuing volatility

           precipitous standardsetting not possible and probably not
          desirable

           standards are a complex issue involving the medium the
          hardware the software and the technical capacity for
          reproductive fidelity and clarity

           the prognosis for reliable archival standards as defined by
          librarians in the foreseeable future is poor

     significant potential and attractiveness of digital technology as a
     preservation medium and access mechanism

     productive use of digital imagery for preservation requires a
     reconceptualizing of preservation principles in a volatile
     standardless world

     concept of managing continuing access in the digital environment
     rather than focusing on the permanence of the medium and longterm
     archival standards developed for the analog world

     transition period  how long and what to do

            redefine archival

            remove the burden of archival copy from paper artifacts

            use digital technology for storage develop management
          strategies for refreshing medium hardware and software

            create acidfree paper copies for transition period backup
          until we develop reliable procedures for ensuring continuing
          access to digital files

session ivd

stuart weibel            the role of sgml markup in the core project 

the emergence of highspeed telecommunications networks as a basic
feature of the scholarly workplace is driving the demand for electronic
document delivery  three distinct categories of electronic
publishingrepublishing are necessary to support access demands in this
emerging environment

       conversion of paper or microfilm archives to electronic format
       conversion of electronic files to formats tailored to
          electronic retrieval and display
       primary electronic publishing materials for which the
          electronic version is the primary format

oclc has experimental or product development activities in each of these
areas  among the challenges that lie ahead is the integration of these
three types of information stores in coherent distributed systems

the core chemistry online retrieval experiment project is a model for
the conversion of large text and graphics collections for which
electronic typesetting files are available category   the american
chemical society has made available computer typography files dating from
 for its twenty journals  this collection of some  journalyears
is being converted to an electronic format that will be accessible
through several enduser applications

the use of standard generalized markup language sgml offers the means
to capture the structural richness of the original articles in a way that
will support a variety of retrieval navigation and display options
necessary to navigate effectively in very large text databases

an sgml document consists of text that is marked up with descriptive tags
that specify the function of a given element within the document  as a
formal language construct an sgml document can be parsed against a
documenttype definition dtd that unambiguously defines what elements
are allowed and where in the document they can or must occur  this
formalized map of article structure allows the user interface design to
be uncoupled from the underlying database system an important step
toward interoperability  demonstration of this separability is a part of
the core project wherein user interface designs born of very different
philosophies will access the same database

notes
       the core project is a collaboration among cornell universitys
     mann library bell communications research bellcore the american
     chemical society acs the chemical abstracts service cas and
     oclc

michael lesk                  the core electronic chemistry library

a major online file of chemical journal literature complete with
graphics is being developed to test the usability of fully electronic
access to documents as a joint project of cornell university the
american chemical society the chemical abstracts service oclc and
bellcore with additional support from sun microsystems springerverlag
digitai equipment corporation sony corporation of america and apple
computers  our file contains the american chemical societys online
journals supplemented with the graphics from the paper publication  the
indexing of the articles from chemical abstracts documents is available
in both image and text format and several different interfaces can be
used  our goals are  to assess the effectiveness and acceptability of
electronic access to primary journals as compared with paper and  to
identify the most desirable functions of the user interface to an
electronic system of journals including in particular a comparison of
pageimage display with ascii display interfaces  early experiments with
chemistry students on a variety of tasks suggest that searching tasks are
completed much faster with any electronic system than with paper but
that for reading all versions of the articles are roughly equivalent

pamela andre and judith zidar

text conversion is far more expensive and timeconsuming than image
capture alone  nals experience with optical character recognition ocr
will be related and compared with the experience of having text rekeyed 
what factors affect ocr accuracy  how accurate does full text have to be
in order to be useful  how do different users react to imperfect text 
these are questions that will be explored  for many a service bureau
may be a better solution than performing the work inhouse this will also
be discussed

session vi

marybeth peters

copyright law protects creative works  protection granted by the law to
authors and disseminators of works includes the right to do or authorize
the following  reproduce the work prepare derivative works distribute
the work to the public and publicly perform or display the work  in
addition copyright owners of sound recordings and computer programs have
the right to control rental of their works  these rights are not
unlimited there are a number of exceptions and limitations

an electronic environment places strains on the copyright system 
copyright owners want to control uses of their work and be paid for any
use the public wants quick and easy access at little or no cost  the
marketplace is working in this area  contracts guidelines on electronic
use and collective licensing are in use and being refined

issues concerning the ability to change works without detection are more
difficult to deal with  questions concerning the integrity of the work
and the status of the changed version under the copyright law are to be
addressed  these are public policy issues which require informed
dialogue


                                 


                appendix iii  directory of participants
                         

presenters

     pamela qj andre
     associate director automation
     national agricultural library
      baltimore boulevard
     beltsville md 
     phone   
     fax   
     email  internet  pandreasrrarsusdagov

     jean baronas senior manager
     department of standards and technology
     association for information and image management aiim
      wayne avenue suite 
     silver spring md 
     phone   
     fax   
     
     patricia battin president
     the commission on preservation and access
      th street nw
     suite 
     washington dc 
     phone   
     fax   
     email  cpagwuvmbitnet

     howard besser
     centre canadien darchitecture
     canadian center for architecture
      rue baile
     montreal quebec hh s
     canada
     phone   
     fax   
     email  howardlispittedu

     edwin b brownrigg executive director
     memex research institute
      bonita avenue
     roseville ca 
     phone   
     fax   
     email  bitnet  memexcalstate

     eric m calaluca vice president
     chadwyckhealey inc
      king street
     alexandria va l
     phone   l
     fax   

     james daly
      deepwood road
     baltimore md 
     phone   

     ricky erway associate coordinator
     american memory
     library of congress
     phone   
     fax   

     carl fleischhauer coordinator
     american memory
     library of congress
     phone   
     fax   

     joanne freeman
      jefferson park avenue no 
     charlottesville va  
     
     prosser gifford
     director for scholarly programs
     library of congress
     phone   
     fax   
     email  pgifseqlocgov

     jacqueline hess director
     national demonstration laboratory
       for interactive information technologies
     library of congress
     phone   
     fax   
     
     susan hockey director
     center for electronic texts in the humanities ceth
     alexander library
     rutgers university
      college avenue
     new brunswick nj 
     phone   
     fax   
     email  hockeyzodiacrutgersedu

     william l hooton vice president
     business  technical development
       imaging  information systems group
     inet
      rockledge drive suite 
     bethesda md l
     phone   
     fax   

     anne r kenney associate director
     department of preservation and conservation
      olin library
     cornell university
     ithaca ny 
     phone   
     fax   
     email  lydycornellabitnet

     ronald l larsen
     associate director for information technology
     university of maryland at college park
     room b mckeldin library
     college park md 
     phone   
     fax   
     email  rlarsenlibrumdedu

     maria l lebron managing editor
     the online journal of current clinical trials
     l h street nw
     washington dc 
     phone   
     fax   
     email  pubsaaasgwuvmbitnet

     michael lesk executive director
     computer science research
     bell communications research inc
     rm a
      south street
     morristown nj ll     
     phone   
     fax   
     email  leskbellcorecom internet or bellcorelesk uucp

     clifford a lynch
     director library automation
     university of california
        office of the president
      lakeside drive th floor
     oakland ca 
     phone   
     fax   
     email  caluruccmvsa

     avra michelson
     national archives and records administration
     nsz rm n
     th  pennsylvania nw
     washington dc 
     phone   
     fax   
     email  tmicunihgov
     
     elli mylonas managing editor
     perseus project
     department of the classics
     harvard university
      boylston hall
     cambridge ma 
     phone      direct
     fax   
     email  elliikarosharvardedu or elliwjhharvardedu

     david woodley packard
     packard humanities institute
      second street suite 
     los altos ca 
     phone    phi
     fax   

     lynne k personius assistant director
     cornell information technologies for
      scholarly information sources
      olin library
     cornell university
     ithaca ny 
     phone   
     fax   
     email  jrncornellcbitnet

     marybeth peters
     policy planning adviser to the
       register of copyrights
     library of congress
     office lm 
     phone   
     fax   

     c michael sperbergmcqueen
     editor text encoding initiative
     computer center mc 
     university of illinois at chicago
     box 
     chicago il 
     phone   
     fax   
     email  uuicvmccuicedu or uuicvmbitnet

     george r thoma chief
     communications engineering branch
     national library of medicine
      rockville pike
     bethesda md 
     phone   
     fax   
     email  thomalhcnlmnihgov

     dorothy twohig editor
     the papers of george washington
      alderman library
     university of virginia
     charlottesville va 
     phone   
     fax   

     susan h veccia team leader
     american memory user evaluation
     library of congress
     american memory evaluation project
     phone   
     fax   
     email  svecseqlocgov

     donald j waters head
     systems office
     yale university library
     new haven ct 
     phone   
     fax   
     email  dwatersyalevmbitnet or dwatersyalevmyccyaleedu

     stuart weibel senior research scientist
     oclc
      frantz road
     dublin oh 
     phone   l
     fax   
     email  internet  sturschoclcorg

     robert g zich
     special assistant to the associate librarian
       for special projects
     library of congress
     phone   
     fax   
     email  rzicseqlocgov

     judith a zidar coordinator
     national agricultural text digitizing program
     information systems division
     national agricultural library
      baltimore boulevard
     beltsville md 
     phone    or 
     fax   
     email  internet  jzidarasrrarsusdagov


observers

     helen aguera program officer
     division of research
     room 
     national endowment for the humanities
      pennsylvania avenue nw
     washington dc 
     phone   
     fax   

     m ellyn blanton deputy director
     national demonstration laboratory
       for interactive information technologies
     library of congress
     phone   
     fax   

     charles m dollar
     national archives and records administration
     nsz rm n
     th  pennsylvania nw
     washington dc 
     phone   
     fax   

     jeffrey field deputy to the director
     division of preservation and access
     room 
     national endowment for the humanities
      pennsylvania avenue nw
     washington dc 
     phone   
     fax   

     lorrin garson
     american chemical society
     research and development department
      th street nw
     washington dc 
     phone   
     fax  email  internet  lrgacsorg

     william m holmes jr
     national archives and records administration
     nsz rm n
     th  pennsylvania nw
     washington dc 
     phone   
     fax   
     email  wholmesamericanedu

     sperling martin
     information resource management
      doolittle street
     gaithersburg md 
     phone   

     michael neuman director
     the center for text and technology
     academic computing center
      reiss science building
     georgetown university
     washington dc 
     phone   
     fax   
     email  neumanguvaxbitnet neumanguvaxgeorgetownedu

     barbara paulson program officer
     division of preservation and access
     room 
     national endowment for the humanities
      pennsylvania avenue nw
     washington dc 
     phone   
     fax   
     
     allen h renear
     senior academic planning analyst
     brown university computing and information services
      waterman street
     campus box 
     providence ri 
     phone   
     fax   
     email  bitnet  allenbrownvm or           
     internet  allenbrownvmbrownedu

     susan m severtson president
     chadwyckhealey inc
      king street
     alexandria va l
     phone   l
     fax        

     frank withrow
     us department of education
      new jersey avenue nw
     washington dc 
     phone   
     fax   


lc staff
     
     linda l arret
     machinereadable collections reading room lj 
      

     john d byrum jr
     descriptive cataloging division lm 
      

     mary jane cavallo
     science and technology division la 
      

     susan thea david
     congressional research service lm 
      

     robert dierker
     senior adviser for multimedia activities lm 
      

     william w ellis
     associate librarian for science and technology lm 
      

     ronald gephart
     manuscript division lm 
      

     james graber
     information technology services lm g
      

     rich greenfield
     american memory lm 
      

     rebecca guenther
     network development lm 
      

     kenneth e harris
     preservation lm g
      

     staley hitchcock
     manuscript division lm 
      

     bohdan kantor
     office of special projects lm 
      

     john w kimball jr
     machinereadable collections reading room lj 
      

     basil manns
     information technology services lm g
      

     sally hart mccallum
     network development lm 
      

     dana j pratt
     publishing office lm 
      

     jane riefenhauser
     american memory lm 
      

     william z schenck
     collections development lm 
      

     chandru j shahani
     preservation research and testing office rt lm g
      

     william j sittig
     collections development lm 
      

     paul smith
     manuscript division lm 
      

     james l stevens
     information technology services lm g
      

     karen stuart
     manuscript division lm 
      

     tamara swora
     preservation microfilming office lm g
      

     sarah thomas
     collections cataloging lm 
      


                                   end
      

note  this file has been edited for use on computer networks  this
editing required the removal of diacritics underlining and fonts such
as italics and bold  

kde 

a few of the italics when used for emphasis were replaced by caps mh

end of the project gutenberg etext of loc workshop on electronic etexts

